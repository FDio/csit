# Copyright (c) 2025 Cisco and/or its affiliates.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at:
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


Commands to convert RFC text from .md (so I do not need to search next time).

Hints:
+ https://www.rubydoc.info/gems/kramdown-rfc2629/
+ https://github.com/cabo/kramdown-rfc
+ https://www.rfc-editor.org/materials/FAQ-xml2rfcv3.html

Initial:
$ sudo aptitude install ruby-rubygems
$ sudo gem install kramdown-rfc
$ kdrfc --version

Main:
$ kdrfc draft-ietf-bmwg-mlrsearch-11.md

If that complains, do it manually at https://author-tools.ietf.org/

Finally:
https://author-tools.ietf.org/idnits


LLM prompts:

I attached the "work in progress" version of a document I am working on.
Currently I am focusing on defining the scope, and there are many open questions.
I feel I have some circular dependencies between the formal Scope (section of MLRsearch Specification chapter)
and informal introduction (sections BMWG Documents, Test Requirements and MLRsearch Position of Introduction chapter).
I guess my ideal structure would us the Introduction to divide BMWG "norms" into few types:
General RFC2544 guidelines (line testing configurations present in production by default,
not testing specialized configurations optimized for performance on a benchmark),
RFC2544 trial, other documents focusing on requirements (DUT config, traffic profile, wait times,
report format) specific to some networking protocol, RFC 2544 benchmarks (e.g. Throughput or Frame Loss Rate),
and RFC 2544 report format for benchmark results.
The formal Scope would then state general guidelines stay the same, Trial gets more freedom (only when used for MLRsearch),
other documents still apply, MLRsearch can work as addition, strengthening, or replacement for Throughput,
but report format requirements are quite different.
And I guess the Introduction should properly introduce the trade-off between configurability and comparability,
and as MLRsearch is quite configurable, it is not recommended for comparability purposes
unless specific Search Goals are agreed upon.
I am not even sure where to start editing to improve the current content.
Let me start with an easy question to you. Should we say MLRsearch introduces a benchmark,
or should we say it introduces a class of benchmarks (differing by search goals and trial deviations)?


I am thinking about some remains of historic language, mentioning "implementation",
"user", focusing on "same lab repeatability" and discussing optional attributes, their default values,
and recommending some search goals over other (avoid long trials, set goal width at loss ratio).
Should the formal scope mention the document contains some recommendations that may be useful
for this kind of repeatability concerns (even though they are not useful for inter-lab comparison purposes)?
I hope that Introduction and Specification can be written mostly without reference to
"implementation" and "user" (mention those only in some discussion paragraphs),
keeping Controller's search strategy out of scope, but recommending to keep any strategy
constant and deterministic for repeatability purposes?


Hmm, it is true that some pairs of Search Goal instances may lead to "interference" during search.
But I also feel it could be possible to state some reasonable assumptions on SUT behavior
and Controller strategy to prove that RFC2544 and TST009 goals do not interfere with each other this way
(TST009 is weaker, no combination of trial results can classify a load as an upper bound for TST009
and a lower bound for RFC2544 at the same time, so no reason to add a trial focusing on one goal
that could invalidate a lower bound for the other goal).
Proving such claims is out of scope (similar to proving general correctness),
but maybe we can prepare terminology, distinguishing single combined benchmark (multiple goals at once,
useful for "same lab" repeatability) from several independent benchmarks (one goal per search,
useful for inter-lab comparisons)?

Back to the formal Scope section. We can maybe say that Specification defines the class of benchmarks
in a way that supports single-goal benchmarks equally well as multiple-goal benchmarks,
but the discussion of "goal interference" is outside of the scope (and we do not need to mention motivations there).

