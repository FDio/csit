Trending Methodology
====================

Continuous Trending and Analysis
--------------------------------

With increasing number of features and code changes in the FD.io VPP
data plane codebase, it is increasingly difficult to measure and detect
VPP data plane performance changes. Similarly, once degradation is
detected, it is getting harder to bisect the source code in search of
the Bad code change or addition. The problem is further escalated by a
large combination of compute platforms that VPP is running and used on,
including Intel Xeon, Intel Atom, ARM Aarch64.

Existing FD.io CSIT continuous performance trending test jobs help, but
they rely on human factors for anomaly detection, and as such are error
prone and unreliable, as the volume of data generated by these jobs is
growing exponentially.

Proposed solution is to eliminate human factor and fully automate
performance trending, regression and progression detection, as well as
bisecting.

This document describes a high-level design of a system for continuous
measuring, trending and performance change detection for FD.io VPP SW
data plane. It builds upon the existing CSIT framework with extensions
to its throughput testing methodology, CSIT data analytics engine (PAL –
Presentation-and-Analytics-Layer) and associated Jenkins jobs
definitions.

Proposed design replaces existing CSIT performance trending jobs and
tests with new Performance Trending (PT) CSIT module and separate
Performance Analysis (PA) module ingesting results from PT and
analysing, detecting and reporting any performance anomalies using
historical trending data and statistical metrics. PA does also produce
trending graphs with summary and drill-down views across all specified
tests that can be reviewed and inspected regularly by FD.io developers
and users community.

Trend Analysis
--------------

All measured performance trend data is treated as time-series data that
can be modelled using normal distribution. After trimming the outliers,
the average and deviations from average are used for detecting
performance change anomalies following the three-sigma rule of thumb
(a.k.a. 68-95-99.7 rule).

Analysis Metrics
````````````````

Following statistical metrics are proposed as performance trend
indicators over the rolling window of last <N> sets of historical
measurement data:

- Q1, Q2, Q3, Quartiles, three points dividing a ranked set of data set
  into four equal parts, Q2 is the median of the data.
- IQR = Q3 Q1, Inter Quartile Range, measure of variability, used here
  to calculate and eliminate outliers.
- Outliers, extreme values that are at least (1.5 * IQR) below Q1.

  - Note: extreme values that are at least (1.5 * IQR) above Q3 are not
    considered outliers, and are likely to be classified as
    progressions.

- TMA, Trimmed Moving Average, average across the data set of the
  rolling window of <N> values without the outliers. Used here to
  calculate TMSD.
- TMSD, Trimmed Moving Standard Deviation, standard deviation over the
  data set of the rolling window of <N> values without the outliers,
  requires calculating TMA. Used for anomaly detection.
- TMM, Trimmed Moving Median, median across the data set of the rolling
  window of <N> values with all data points, excluding the outliers.
  Used as a trending value and as a reference for anomaly detection.

The relation between IQR and Standard Deviation (denoted by sigma) is
shown in figure below.

598px-Boxplot_vs_PDF.png

Outlier Detection
`````````````````

Outlier evaluation of test result with value <X> follows the definition
in previous section:

  Outlier Evaluation Criteria   Evaluation Result
  ====================================================
  X < (Q1 - 1.5 * IQR)            Outlier
  X >= (Q1 - 1.5 * IQR)           Valid (For Trending)

Anomaly Detection
`````````````````

To verify compliance of test result with value <X> against defined trend
metric and detect anomalies, three simple evaluation criteria are
proposed:

  Anomaly Evaluation Criteria                 Compliance Confidence Level     Evaluation Result
  =====================================================================================
  (TMM - 3 * TMSD) <= X <= (MM + 3 * TMSD)             99.73%                    Normal
  X < (MM - 3 * TMSD)                                 Anomaly                   Regression
  X > (MM + 3 * TMSD)                                 Anomaly                   Progression


TMM is used for the central trend reference point instead of TMA as it
is more robust to anomalies.

Trend Compliance
````````````````

Trend compliance metrics are targeted to provide an indication of trend
changes over a short-term (i.e. weekly) and a long-term (i.e.
quarterly), comparing the last trend value, TMM[last], to one from week
ago, TMM[last - 1week] and to the maximum of trend values over last
quarter except last week, max(TMM[(last - 3mths)..(last - 1week)]),
respectively. This results in following trend compliance calculations:

  Trend Compliance Metric     Change Formula         V(alue)                     R(eference)
  ==========================================================================================================
  Short-Term Change           ((V - R) / R)         TMM[last]       TMM[last - 1week]
  Long-Term Change            ((V - R) / R)         TMM[last]       max(TMM[(last - 3mths)..(last - 1week)])

Trend Dashboard
---------------

Dashboard tables list a summary of per test-case VPP MRR performance
trend and trend compliance metrics and detected number of anomalies.
Data samples come from the CSIT VPP trending MRR jobs executed twice a
day, every 12 hrs (02:00, 14:00 UTC). All trend and anomaly calculations
are defined in <ref to section 2. Trending Methodology> and use a
rolling window of <N> data samples, currently with N = 14 covering last
7 days. Short-Term trend change is using last week trend as a reference
value, Long-Term change is using maximum over a quarter period as a
reference value.

Separate tables are generated for tested VPP worker-thread-core
combinations (1t1c, 2t2c, 4t4c). Test case names are linked to
respective trending graphs for ease of navigation thru the test data.

Trending Graphs
---------------

**Content to be added**

Detailed Reporting
------------------

**Content to be updated**

Analysis results are reported per test case in text format, in graphical
format with trending graphs and as a cumulative Jenkins job result, as
follows:

  Test Result Evaluation        Text Result     Reported Reason     Trending Graph Markers
  ==========================================================================================
        Normal                      Pass          Normal            Part of plot line
        Regression                  Fail          Regression        Red circle
        Progression                 Pass          Progression       Green circle

Jenkins job cumulative results:

- Pass - if all detection results are Pass or Warning.
- Fail - if any detection result is Fail.

Performance Trending (PT) Jobs
------------------------------

**Content to be updated**

CSIT PT runs regular performance test jobs finding MRR, PDR and NDR per test cases. PT is designed as follows:

#. PT job triggers:

  #. Periodic e.g. daily.
  #. On-demand gerrit triggered.
  #. Other periodic TBD.

#. Measurements and calculations per test case:

  #. MRR Max Received Rate

    #. Measured: Unlimited tolerance of packet loss.
    #. Send packets at link rate, count total received packets, divide by test trial period.

  #. Optimized binary search bounds for PDR and NDR tests:

    #.  Calculated: High and low bounds for binary search based on MRR and pre-defined Packet Loss Ratio (PLR).
    #. HighBound=MRR, LowBound=to-be-determined.
    #. PLR – acceptable loss ratio for PDR tests, currently set to 0.5% for all performance tests.

  #. PDR and NDR:

    #.  Run binary search within the calculated bounds, find PDR and NDR.
    #. Measured: PDR Partial Drop Rate – limited non-zero tolerance of packet loss.
    #. Measured: NDR Non Drop Rate - zero packet loss.

#. Archive MRR, PDR and NDR per test case.
#. Archive counters collected at MRR, PDR and NDR.

Performance Analysis (PA) Jobs
------------------------------

**Content to be updated**

CSIT PA runs performance analysis, change detection and trending using specified trend analysis metrics over the rolling window of last <N> sets of historical measurement data. PA is defined as follows:

#. PA job triggers:

  #. By PT job at its completion.
  #. On-demand gerrit triggered.
  #. Other periodic TBD.

#. Download and parse archived historical data and the new data:

  #. New data from latest PT job is evaluated against the rolling window of <N> sets of historical data.
  #. Download RF output.xml files and compressed archived data.
  #. Parse out the data filtering test cases listed in PA specification (part of CSIT PAL specification file).

#. Calculate trend metrics for the rolling window of <N> sets of historical data:

  #. Calculate quartiles Q1, Q2, Q3.
  #. Trim outliers using IQR.
  #. Calculate TMA and TMSD.
  #. Calculate normal trending range per test case based on TMA and TMSD.

#. Evaluate new test data against trend metrics:

  #. If within the range of (TMA +/- 3*TMSD) => Result = Pass, Reason = Normal.
  #. If below the range => Result = Fail, Reason = Regression.
  #. If above the range => Result = Pass, Reason = Progression.

#. Generate and publish results

  #. Relay evaluation result to job result.
  #. Generate a new set of trend analysis summary graphs and drill-down graphs.

    #. Summary graphs to include measured values with Normal, Progression and Regression markers. MM shown in the background if possible.
    #. Drill-down graphs to include MM, TMA and TMSD.

  #. Publish trend analysis graphs in html format on https://docs.fd.io/.
