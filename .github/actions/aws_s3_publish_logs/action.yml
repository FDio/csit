---
name: "ðŸ› ï¸ AWS S3 Publish Logs"
description: |
  This GitHub Action uploads logs to AWS S3.

inputs:
  AWS_ACCESS_KEY_ID:
    description: "Unique, public identifier for an AWS IAM user."
    required: true
    type: string
  AWS_SECRET_ACCESS_KEY:
    description: "Long-term security credential for AWS account or an IAM user."
    required: true
    type: string
  AWS_REGION:
    description: "Long-term security credential for AWS account or an IAM user."
    required: true
    type: string
  S3_BUCKET:
    description: "Name of the Amazon S3 bucket."
    required: true
    default: "fdio-logs-s3-cloudfront-index"
    type: string
  S3_PATH:
    description: "Path within Amazon AWS S3 bucket."
    required: false
    default: "vex-yul-rot-jenkins-1/${{ github.job }}/${{ github.run_id }}"
    type: string
  ARCHIVES_PATH:
    description: "Source directory with logs artifact to archive."
    required: false
    default: "${{ github.workspace }}/archives"
    type: string

runs:
  using: "composite"
  steps:
    - name: Check if AWS CLI is pre-installed
      id: aws-binary-check
      shell: bash
      run: |
        if command -v aws >/dev/null 2>&1; then
            echo "AWS CLI is already installed. Skipping install."
            echo "AWS_CLI_PREINSTALLED=true" >> "$GITHUB_OUTPUT"
            aws --version
            exit 0
        fi
        echo "AWS_CLI_PREINSTALLED=false" >> "$GITHUB_OUTPUT"

    - name: Cache or Restore the zip
      if: ${{ steps.aws-binary-check.outputs.AWS_CLI_PREINSTALLED == 'false' }}
      uses: actions/cache@v4
      id: aws-cli-cache
      with:
        path: ${{ runner.temp }}/aws-cli-cache/*.zip
        key: ${{ runner.os }}-${{ runner.arch }}-aws-cli-v2-zip

    - name: Install AWS CLI
      shell: bash
      if: ${{ steps.aws-binary-check.outputs.AWS_CLI_PREINSTALLED == 'false' }}
      env:
        CACHE_HIT: ${{ steps.aws-cli-cache.outputs.cache-hit == 'true' }}
        CACHE_PATH: ${{ runner.temp }}/aws-cli-cache
      run: |
        AWS_PACKAGE="awscli-exe-linux-$(uname -m).zip"

        mkdir -p "$CACHE_PATH"
        cd "$CACHE_PATH"

        if [ "$CACHE_HIT" = "false" ]; then
            curl -fsSL \
                --retry 3 \
                --retry-delay 5 \
                --connect-timeout 15 \
                --max-time 60 \
                -o "$AWS_PACKAGE" "https://awscli.amazonaws.com/$AWS_PACKAGE"
        fi

        unzip -o -q "$AWS_PACKAGE"
        sudo ./aws/install --update
        rm -rf ./aws

    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v5.0.0
      with:
        aws-access-key-id: ${{ inputs.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ inputs.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ inputs.AWS_REGION }}

    - name: Upload archives directory content to S3
      shell: bash
      run: |
        get_content_type() {
          local file_ext="${1##*.}"
          case "$file_ext" in
            xml)  echo "application/xml" ;;
            html) echo "text/html" ;;
            txt)  echo "text/plain" ;;
            log)  echo "text/plain" ;;
            css)  echo "text/css" ;;
            md)  echo "text/markdown" ;;
            rst)  echo "text/x-rst" ;;
            csv)  echo "text/csv" ;;
            svg) echo "image/svg+xml" ;;
            jpg|jpeg) echo "image/jpeg" ;;
            png) echo "image/png" ;;
            gif) echo "image/gif" ;;
            js)   echo "application/javascript" ;;
            pdf) echo "application/pdf" ;;
            json) echo "application/json" ;;
            otf) echo "application/otf" ;;
            ttf) echo "application/ttf" ;;
            woff) echo "application/woff" ;;
            woff2) echo "application/woff2" ;;
            *)   echo "application/octet-stream" ;;
          esac
        }

        export -f get_content_type

        pushd ${{ inputs.ARCHIVES_PATH }}

        # Traverse and upload
        find . -type f | while read -r file; do
          rel_path="${file#./}"              # relative path
          tmp_file="$(mktemp).gz"            # temp gzip file
          gzip -c "${file}" > "${tmp_file}"  # compress

          content_type=$(get_content_type "${file}")

          S3_ARN="s3://${{ inputs.S3_BUCKET }}/${{ inputs.S3_PATH }}/${rel_path}.gz"

          echo "Uploading ${rel_path}.gz -> ${S3_ARN} (Content-Type: ${content_type}, gzip)"
          aws s3 cp "${tmp_file}" "${S3_ARN}" --content-type "${content_type}" --content-encoding "gzip"

          rm -f "${tmp_file}"
        done
