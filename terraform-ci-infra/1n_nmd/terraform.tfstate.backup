{
  "version": 4,
  "terraform_version": "0.14.4",
  "serial": 722,
  "lineage": "e4e7f30a-652d-7a31-e31c-5e3a3388c9b9",
  "outputs": {},
  "resources": [
    {
      "module": "module.elastic",
      "mode": "data",
      "type": "template_file",
      "name": "nomad_job_elastic",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "6fbaa136a3f2d99d2cebd028fdf6463b4f9b4f43cd4fd7c6fdac7211e5c46e76",
            "rendered": "job \"prod-elastic\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 1\n\n    health_check      = \"checks\"\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 1\n\n    # Specifies if the job should auto-promote to the canary version when all\n    # canaries become healthy during a deployment. Defaults to false which means\n    # canaries must be manually updated with the nomad deployment promote\n    # command.\n    auto_promote      = true\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = true\n\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-elastic-cluster\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count             = 1\n\n    constraint {\n      attribute       = \"${node.unique.name}\"\n      value           = \"s41-nomad-x86_64\"\n    }\n\n    ephemeral_disk {\n      size            = \"50000\"\n      sticky          = true\n      migrate         = false\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-elastic-cluster\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver          = \"docker\"\n\n      kill_timeout    = \"600s\"\n      kill_signal     = \"SIGTERM\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image         = \"docker.elastic.co/elasticsearch/elasticsearch:7.10.1\"\n        dns_servers   = [ \"${attr.unique.network.ip-address}\" ]\n        command       = \"elasticsearch\"\n        args          = [\n          \"-Enode.name=elastic${NOMAD_ALLOC_INDEX}\",\n          \"-Enetwork.host=0.0.0.0\",\n          \"-Ecluster.name=elastic\",\n          \"-Ehttp.port=${NOMAD_PORT_rest}\",\n          \"-Ehttp.publish_port=${NOMAD_HOST_PORT_rest}\",\n          \"-Ebootstrap.memory_lock=true\",\n          \"-Epath.logs=/alloc/logs/\",\n          \"-Ediscovery.type=single-node\",\n          \"-Etransport.publish_port=${NOMAD_HOST_PORT_transport}\",\n          \"-Etransport.port=${NOMAD_PORT_transport}\",\n          \"-Expack.license.self_generated.type=basic\",\n          \"-Expack.security.enabled=true\",\n          \"-Expack.security.http.ssl.enabled=true\",\n          \"-Expack.security.http.ssl.key=certs/elastic${NOMAD_ALLOC_INDEX}.key\",\n          \"-Expack.security.http.ssl.certificate=certs/elastic${NOMAD_ALLOC_INDEX}.crt\",\n          \"-Expack.security.http.ssl.certificate_authorities=certs/ca.crt\",\n          \"-Expack.security.transport.ssl.enabled=true\",\n          \"-Expack.security.transport.ssl.key=certs/elastic${NOMAD_ALLOC_INDEX}.key\",\n          \"-Expack.security.transport.ssl.certificate=certs/elastic${NOMAD_ALLOC_INDEX}.crt\",\n          \"-Expack.security.transport.ssl.certificate_authorities=certs/ca.crt\",\n          \"-Expack.security.transport.ssl.verification_mode=certificate\"\n        ]\n        volumes       = [\n          \"secrets/elastic${NOMAD_ALLOC_INDEX}.crt:/usr/share/elasticsearch/config/certs/elastic${NOMAD_ALLOC_INDEX}.crt\",\n          \"secrets/elastic${NOMAD_ALLOC_INDEX}.key:/usr/share/elasticsearch/config/certs/elastic${NOMAD_ALLOC_INDEX}.key\",\n          \"secrets/ca.crt:/usr/share/elasticsearch/config/certs/ca.crt\",\n          \"secrets/ca.key:/usr/share/elasticsearch/config/certs/ca.key\",\n          \"secrets/instances.yml:/usr/share/elasticsearch/config/instances.yaml\",\n          \"secrets/users_roles:/usr/share/elasticsearch/config/users_roles\",\n          \"secrets/users:/usr/share/elasticsearch/config/users\"\n        ]\n        ulimit {\n          memlock     = \"-1\"\n          nofile      = \"65536\"\n          nproc       = \"8192\"\n        }\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\ninstances:\n  - name: 'elastic0'\n    dns: [ 'elastic0.elastic.service.consul', 'elastic.service.consul']\n  - name: 'kibana'\n    dns: [ 'kibana.service.consul' ]\nEOF\n        destination  = \"secrets/instances.yml\"\n      }\n      template {\n        data         = \u003c\u003cEOF\nelasticuser:$2a$10$kgJDjo1/pBaBsnUHvsGOiOT3IAJhk2TBVNJeTv/1EPg//klcJCK4y\nEOF\n        destination  = \"secrets/users\"\n      }\n      template {\n        data         = \u003c\u003cEOF\nkibana_admin:elasticuser\nmonitoring_user:elasticuser\nsuperuser:elasticuser\nEOF\n        destination  = \"secrets/users_roles\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDaDCCAlCgAwIBAgIUG3esFYSWamMDpavP3zw/JTA5svswDQYJKoZIhvcNAQEL\nBQAwNDEyMDAGA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5l\ncmF0ZWQgQ0EwHhcNMjEwMTE0MTQwODU2WhcNMjQwMTE0MTQwODU2WjATMREwDwYD\nVQQDEwhlbGFzdGljMDCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALUU\nZYRGTcGz4NrP7vIzZo5kDBekffRYugqBzfDztNtfqBPEKB/rK9tJr8Qp2IfKJJ0l\nyuMTkYUcebjKIHfOJ4MAvNKtA1lSj3j0FJSOw9LXFuIYKCD661RmdWiwnz4pAVIs\noAriQmz/Z1hib38vmZ7dyYgS1VkUssHDxKM9q/9Q1ILmuOtuOJN6e5shnGNTm2Ft\nj3uNk8ZTHBOM4LiCBu4tQ3v1DIjdT4t2pG5grQu374Sinux3YlcIeuxQbpAVaOl+\ngfQnK7skdtXWMYVmZnuMJpRRIxzWyhsAlWaOletbjQ3xOqW/oXtfg9ve6qPhFJuC\ns5l3YqufKlXVoyrbXCkCAwEAAaOBkjCBjzAdBgNVHQ4EFgQUKLG4/6t+9LtjH18Q\n7PTFas+YXzAwHwYDVR0jBBgwFoAUYgsxZu+uL3YBbh6EoB7W49VgmCUwQgYDVR0R\nBDswOYIfZWxhc3RpYzAuZWxhc3RpYy5zZXJ2aWNlLmNvbnN1bIIWZWxhc3RpYy5z\nZXJ2aWNlLmNvbnN1bDAJBgNVHRMEAjAAMA0GCSqGSIb3DQEBCwUAA4IBAQAEepAs\nh7d+a2k6Qj7B3KyZnX0O50toeZW+tKnnfGin0H5LGgvVn40mRJEJKBzatp/LvHh+\nx//YM+x8IbZe7bDtf69EUqq6C3881Xsq1jj77GZ1buEP+W9nRNjM3o4mjcn3RfPw\nGRV6lHnpHvhqAUIFtlOHvaa0UuEbqQkomxr/e44btRdnDQ6SRRh4xwBKYT/O/a7O\nYpS514Q4vKNQ6XLSAdGpJjK6KdHO5xkzCi4zYastpuy8ct/qqiAklAtXaF+7F0OX\nriSs0AsAokMsF+hLv5d1kAH535uHs7Mr85gw7Y7/qlXmJovh4oWcys8XTPvlHhkQ\n98I9yFJ7HRAUgbah\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/elastic${NOMAD_ALLOC_INDEX}.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEpQIBAAKCAQEAtRRlhEZNwbPg2s/u8jNmjmQMF6R99Fi6CoHN8PO021+oE8Qo\nH+sr20mvxCnYh8oknSXK4xORhRx5uMogd84ngwC80q0DWVKPePQUlI7D0tcW4hgo\nIPrrVGZ1aLCfPikBUiygCuJCbP9nWGJvfy+Znt3JiBLVWRSywcPEoz2r/1DUgua4\n6244k3p7myGcY1ObYW2Pe42TxlMcE4zguIIG7i1De/UMiN1Pi3akbmCtC7fvhKKe\n7HdiVwh67FBukBVo6X6B9CcruyR21dYxhWZme4wmlFEjHNbKGwCVZo6V61uNDfE6\npb+he1+D297qo+EUm4KzmXdiq58qVdWjKttcKQIDAQABAoIBAQCMeL4n1sILOhd8\np0Gd8fHlFAetb5WmMA5iiD/SY7wxUgt5Cfp2iGEFRCxt6GhpLo8ouWCit1N0B5sF\nlweI6QwNvEy+wiiO6lUSZ4ZvmDChJupBiqvWqdBVMQZzqFBgUD8OGEAvMUaGd7sb\n/YCxEaQCcdsdDD8lU8E4Pz4TxIvhCukcFV/Jd60yh2K7UAnruG8ZcytmTsfnA7s7\nIIsnu20YllwflSjkb3oKtOsgKjMc6Oj01vjH90W+UXC7S+OJ/bXdCCBJn6QYfVRd\nIQ9l0bvtu+4yoRBmXRbIJ6zf8++FOBXa25I5U2VJbw44VszlZfeRahqoxF8jrSTh\n7uZZr3HhAoGBAPwM30AmGP0msTbJSm5KhSdwhxLtQ1xAd3z8HmgEQ99ncdeVqJ/b\nkHh6L3UW+0kLHHvJs0D4xlr6CJrZBRtwJLxuE7dtlBD7ouvVZjdQ09drKio7/aC3\nL3/spyRAw7yHlZFVkvNapZV9TUfcpwrza4FUiHdhATw7YhPQQ53x54t3AoGBALfq\n0WRksjZrMSxQWzPCBkVx1efc6WzednV8FJFldLbA1+sVi1g0fSW6y5EK/gHgezIp\nEdyG937lUnXyCYxMH+LuZUkq2ZyUjpH/pQxCBXpmwDA8shRD/UTBtpd/x/ORapCs\nPF/sC5eXATyQX7ADWQeeTZndWICNQuLNf0mKNv1fAoGBAOd8cvWZh83IYW2txTwy\nGMS2JngNjJYHZzZU3yAs+qENgpK7Eplur+rWXQuuxa66E7jk8Eq1sIcRqCF/O5+N\niU+90UHf0+MdGO57mVsoUsc/1wPfAPtAAtH8aS10hdB6vbUy4Lm8AOOgpv9e+dOm\n6I9pMcRiRR4qc9M6rT88UqnVAoGAblX0eusiMx2JqZEntdxf0MejUW+ppkOsA32G\nBVg9deopXwJUz3zl23295G0Yx915azVSXt+lmT5QgyvKaJ2+v3DP2N5ZIOPKyHH6\n/WiaSr1b7VRsbVYAmoAwX6EsPsZtjQ+XROCib7YK6t+eWEUZ40UoPveYwb59cv1f\nsKm3pbcCgYEAgyYc6DdrsAgT4S9g38hdqtuiSejhcGojHVeaMmPNxgoaXZZ66+g6\nKujDXHJtkUCBluXOhfsQcjAr2UzvxpGdvDd5Ym5HA6l/2qAtph+f8DUofESSz7vh\nT9/4PLw82sIU9/wFM40F1IV20W1TgUnJZln4HdpWPpwmXottoOX3y2s=\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/elastic${NOMAD_ALLOC_INDEX}.key\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDSTCCAjGgAwIBAgIUal1tFY90NA8IgvMVUs/jfyG3N00wDQYJKoZIhvcNAQEL\nBQAwNDEyMDAGA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5l\ncmF0ZWQgQ0EwHhcNMjEwMTE0MTQwODU2WhcNMjQwMTE0MTQwODU2WjA0MTIwMAYD\nVQQDEylFbGFzdGljIENlcnRpZmljYXRlIFRvb2wgQXV0b2dlbmVyYXRlZCBDQTCC\nASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKBXjJFIv4LL2DfqhUdPQGJl\nMmTgxIvqkDsoOxDYuTP27hY27wKUWFBO0sYkWxpddenGQxk66SzkU1XukvGw/Ygs\nezyuvRZq2UlOOt+2eIIjYmb6TIfhpDbMeVhYXfKiFtRHUYskd2VfSUktff8NEnC1\niWLPQaMdv81CmMOGkshjVYn4gaHD+b8Kv7sfnFn5WYMojWez1OOfWke+lJfw+sIa\ntOaZ+ufGZB50H63OZrUJJJa0QahlTakHJpXrk5x0mUq/E9P74FUFQ+tDqUPjLXQq\naPFbzwtSyiT0Rk8nMqu2TQm0kkz79wjR44MmXJo+qFAbMVY/fam+kLEpth0UL9UC\nAwEAAaNTMFEwHQYDVR0OBBYEFGILMWbvri92AW4ehKAe1uPVYJglMB8GA1UdIwQY\nMBaAFGILMWbvri92AW4ehKAe1uPVYJglMA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZI\nhvcNAQELBQADggEBAG4SEZ6beme+BG00ybv4YcFVEhUI24jgrpntyIOHC2ARkLXW\n1yl22Ue/eRg8fP9UhHH2YmUXt2Fy8b1HzihH38WAO/Wxx2K6u38C2ADuTppELWIU\nXUktUCOeYB2B5jbwFSExm6N2Rhj9YJsdlm/Lvph9s2VQThdKUZPOXqdi8u5C6L0k\ns2gkrKJdm4hF7NnVcgIzBPBY86sYzOMW1CXFP6o887KxKjPI7A1JAAkrPZz3ob0n\nB9pBayly5UAtixLJhQbkDfGAB1gRnDWaCDYmN+YT4LaQ/tsqueW8Ba1hQ1PqVLvw\n6J6ytch0M9sCIQe1PzLhEJRpfqtqeacr8yEUgkg=\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/ca.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEAoFeMkUi/gsvYN+qFR09AYmUyZODEi+qQOyg7ENi5M/buFjbv\nApRYUE7SxiRbGl116cZDGTrpLORTVe6S8bD9iCx7PK69FmrZSU4637Z4giNiZvpM\nh+GkNsx5WFhd8qIW1EdRiyR3ZV9JSS19/w0ScLWJYs9Box2/zUKYw4aSyGNVifiB\nocP5vwq/ux+cWflZgyiNZ7PU459aR76Ul/D6whq05pn658ZkHnQfrc5mtQkklrRB\nqGVNqQcmleuTnHSZSr8T0/vgVQVD60OpQ+MtdCpo8VvPC1LKJPRGTycyq7ZNCbSS\nTPv3CNHjgyZcmj6oUBsxVj99qb6QsSm2HRQv1QIDAQABAoIBADz9EBqy8SVvI+8g\n5VEadAL5OxHj7N7LedEGnHDr/oYlhqosev0gL/dcBBAaBA0jP5aMMzmFjuvkbU5i\nUMJd8BG72aRbUtEUE1Iuz3YIkg3uJ5/D1RhaW3v8iqtv8Uw5GzXjasDiPgfxFo8f\nHq3E6x6z7m4HJ5BD4JDSpAi7R1mw0VmklleDG/7FjCbWjyJIbv9I0337uqiFQJ6t\nERPf7+LK0mdaBvhPcWBSShXPi0p6kTe+/DcWmcf9ORBII9PRipd0bViSoot4xSWc\n6DKjY0lDK23C4YPNOFb5aI4BfsMxcXhEnf6oAx1+oKDgGSDrPmMtuUB3OzdWPeBv\nZUtAZAECgYEA+pHYIX+KQ+cDBgFxlzZwQBUVRZzie9twUCZrAXcrV4qTSbI3ML1f\nTrzSzDsdfVYuPUUCy5hbSJ5SqYchZ09oWhKXgfffsHwgMO2faN7iUYPvFKyxUrMS\n+Gj8VgkiQEV3ka820LxDbaVc34R6kkoOnRt1QSohoARZzmN/0K9/pe0CgYEAo9Ef\njdpS0HF7nlJ2lc7WJirKdg8ECCVuswD7Ay4SKz1/pIcSdTES2w1204d1v14jPt/S\ncoy5VT28CT4Zfn/D9yhj/NvIG9IKshAGClcVAkH+Uh9iXBIZvPzashZD6Xp0FfRj\n5QX6koiS8Jxnf2A+1s5SLNKPnWoPwWnKCvJJdIkCgYEA0x/X8EG6ioQ3c/P7deGU\nqyoYhlMuMhYviBkWyGFUz6ofeFUFU7f8eid3pkWZD2ZyB4YCWPHC2GkuVVFav+WU\nk3Be4E+u1tF/fjp5uq8yGmUEKXNo5bmlHlG3a/a+OVFO8h2kHjTCy7wtiNfjPyfP\nMGlWXtXVBzMjSFdl9rwo3fECgYA1tOPxb7hi2jG7EDIMn0kaLkE+P2IFAbCvQw0I\nV9xhDMKCQD5O6Y3S/zEL3Ic//C71+A9YusYwKhMxvIhDLsQijb1qMuwCIvSauCIi\n1bXvjY9BgUSQBuclTIiuhhoxu5G/eOYfObySue/iroRIAFfZuL68LzQiWZlcwcAZ\noqFucQKBgCFUFgFc9LebqwBC/3nEgeAkDRutipp8TQOw4ZFCVI43ShFNz4CJqaq9\nuFBgoI6tMoc5LDBTzs7rJvijcWN44pbo2COVPHQZDCNsI15G0t5JjY0cOz426D9r\nRVhLPYqpE6zQHy35PAnZnYQ8h/VfIsmGvkczjeADPIXR8jyf6FQ7\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/ca.key\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name              = \"elastic\"\n        port              = \"rest\"\n        tags              = [ \"elastic${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Cluster REST Check Live\"\n          port            = \"rest\"\n          type            = \"tcp\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n        check {\n          name            = \"Elastic Cluster HTTP Check Live\"\n          type            = \"http\"\n          port            = \"rest\"\n          protocol        = \"https\"\n          header {\n            Authorization = [\"Basic ZWxhc3RpY3VzZXI6RWxhc3RpYzEyMzQ=\"]\n          }\n          tls_skip_verify = true\n          path            = \"/_cat/health?pretty\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n      service {\n        name              = \"elastic-transport\"\n        port              = \"transport\"\n        tags              = [ \"elastic-transport${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Cluster Transport Check Live\"\n          type            = \"tcp\"\n          port            = \"transport\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu        = 40000\n        memory     = 40000\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        network {\n          port \"rest\" {\n            static = 9200\n          }\n          port \"transport\" {\n            static = 9300\n          }\n        }\n      }\n    }\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-elastic-kibana\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count              = 1\n\n    constraint {\n      attribute        = \"${node.unique.name}\"\n      value            = \"s41-nomad-x86_64\"\n    }\n\n    update {\n      max_parallel     = 1\n      health_check     = \"checks\"\n      min_healthy_time = \"10s\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-elastic-kibana\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver           = \"docker\"\n\n      kill_timeout     = \"60s\"\n      kill_signal      = \"SIGTERM\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image         = \"docker.elastic.co/kibana/kibana:7.10.1\"\n        dns_servers   = [ \"${attr.unique.network.ip-address}\" ]\n        command       = \"kibana\"\n        args          = [\n          \"--server.name=kibana\",\n          \"--server.host=0.0.0.0\",\n          \"--server.port=${NOMAD_PORT_http}\",\n          \"--server.ssl.enabled=true\",\n          \"--server.ssl.certificate=/etc/kibana/config/certs/kibana.crt\",\n          \"--server.ssl.key=/etc/kibana/config/certs/kibana.key\",\n          \"--elasticsearch.hosts=https://elastic0.elastic.service.consul:9200\",\n          \"--elasticsearch.username=elasticuser\",\n          \"--elasticsearch.password=Elastic1234\",\n          \"--elasticsearch.ssl.certificateAuthorities=/etc/kibana/config/certs/ca.crt\",\n          \"--xpack.apm.ui.enabled=false\",\n          \"--xpack.graph.enabled=false\",\n          \"--xpack.grokdebugger.enabled=false\",\n          \"--xpack.maps.enabled=false\",\n          \"--xpack.ml.enabled=false\",\n          \"--xpack.searchprofiler.enabled=false\"\n        ]\n        volumes       = [\n          \"secrets/kibana.crt:/etc/kibana/config/certs/kibana.crt\",\n          \"secrets/kibana.key:/etc/kibana/config/certs/kibana.key\",\n          \"secrets/ca.crt:/etc/kibana/config/certs/ca.crt\"\n        ]\n        ulimit {\n          memlock     = \"-1\"\n          nofile      = \"65536\"\n          nproc       = \"8192\"\n        }\n      }\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDQzCCAiugAwIBAgIVAOckTq6CgphZAuFRgi8dAgkk3mIIMA0GCSqGSIb3DQEB\nCwUAMDQxMjAwBgNVBAMTKUVsYXN0aWMgQ2VydGlmaWNhdGUgVG9vbCBBdXRvZ2Vu\nZXJhdGVkIENBMB4XDTIxMDExNDE0MDg1N1oXDTI0MDExNDE0MDg1N1owETEPMA0G\nA1UEAxMGa2liYW5hMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAsAbC\nTe1t0tZFDO9stsqpl1g16DPGuk4hqe5HrXBDRk1atswitJ5+hVdWnCiZ9zBHsXLO\nDN5m9AkqYe4r9e7+CIAW6v3TzvYa6qP9rQVt873l/zJnZp1mS2Cro+HdLctXCTW3\n3OC7VjIOmGidbNFPWe+Nh5Plp2u9eQSDLJ3OGx0Q9GlQAUp9dbHsauWpLBy0YRsQ\nX+0UHTm9Nx04cNZGilC12F/LuJtuqFiqCSQ/QAPmb+AJWsTZAPlhQl7raNh6Ghcj\nO11GZzlQQ1MWjNCaBtsRFJsOrbYbK5qbvZUbm9FPDjlKCZ7TCrVcjwwYrP/9HvfV\nCS9eQ+Q7JhxjKV6zowIDAQABo28wbTAdBgNVHQ4EFgQU39rJmu9XDHTDDyvKAIPr\nP6Fek5MwHwYDVR0jBBgwFoAUYgsxZu+uL3YBbh6EoB7W49VgmCUwIAYDVR0RBBkw\nF4IVa2liYW5hLnNlcnZpY2UuY29uc3VsMAkGA1UdEwQCMAAwDQYJKoZIhvcNAQEL\nBQADggEBAD8ySxda8bqehs4ZdmdFBe0n0Fqo6KK8rRCGqKu/qpzuSA9/T372NE/k\nWhx3QQPWcb3DhS+oEZ2s8KPrq6pSZtDcQqMWusxeNX7L/V0FLtKneksP8w/y0Wb4\nKeAss66DVrr6Jl1WNzPO0Ia9SugQa4gcXf6M6sH72NqgQZMqfkUoPw3OrKxgD7zD\nww/eKW82CRW1/SkHEbpgIhT4zl2MOnlIT1XGBl+OdFLlTo9QQJtZl4+p9VBvvuVC\nKwdU1h+0YRL7ktT3JcmVdvloxmQljymCx4AttPAZubXDVr0r02ne51bJw92gomHY\noG/i7diepgsKwG0txI8FIaF17URHdg0=\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/kibana.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEAsAbCTe1t0tZFDO9stsqpl1g16DPGuk4hqe5HrXBDRk1atswi\ntJ5+hVdWnCiZ9zBHsXLODN5m9AkqYe4r9e7+CIAW6v3TzvYa6qP9rQVt873l/zJn\nZp1mS2Cro+HdLctXCTW33OC7VjIOmGidbNFPWe+Nh5Plp2u9eQSDLJ3OGx0Q9GlQ\nAUp9dbHsauWpLBy0YRsQX+0UHTm9Nx04cNZGilC12F/LuJtuqFiqCSQ/QAPmb+AJ\nWsTZAPlhQl7raNh6GhcjO11GZzlQQ1MWjNCaBtsRFJsOrbYbK5qbvZUbm9FPDjlK\nCZ7TCrVcjwwYrP/9HvfVCS9eQ+Q7JhxjKV6zowIDAQABAoIBAQCQJQjKTaqIY6Rp\n4kpRKYZVBAwo2PVcrQyOHi0eDvdYQ5IMbP/ijoOm541qFSl3rVaYLh4jlaATKMpH\nJYVkQFBQX6vkxPTE3u3NxXq/S9ntJk2IfBsGgdA527DSY+v+Syw7w3yL6JAgFp+z\nGMAJUyG60RtBsc/3GJgw2IweZh9YPUdmgPy6Ci2MutXVJwiRlies9xSDJwgzQHCr\nCrJJmhc2kTxX16yBJ/KM1aJjME+/fVBvvr9+QHrK4sqqgMXCtzLCfBRocWcv71kq\ntnTVBrYrPH/Wwq//5B5qlshJ99d1V+k4btVpLDqL17iB2bWRYZ00QTluepf0C77n\nis/j/uR5AoGBAPJE8zOYePYb6pMnVdMjAtEeR7zPMyr+fqpkTtzaSydLor58cB8z\nhyLzW2AeD+2qGAIoojjfnkhZm56/kYMnyuS6+qW2wO6527ts5K37XEMHsPs+33fJ\nWN0sOYZM+kBw/3YFsq4o/rl0poj7WObe47GEQqp8cV+r5RgPMDy9AoBvAoGBALoA\nszcwYODvR0H8UMCIk0l3QWVAgP8HLDP0ZVJ2GYzek+0nx30LFJLkoMohoxg01FKd\n0t3HvsQnyR582hXGmTt5g87BjOTKiFJO1ivMgaNaVoYSF4G4Jiobf5R3Lq0tY5od\nAPu1uW0RfnX1vDaCEFaphFhb57n+7us0rhfSn7INAoGAT3a+LpY8Vr0hW9LzG6XI\nLr832H490kRXV5w/IcGYFPOCFejK/fDwyk34ErbJkrLP3SVm0DDIwgJiQNek6tgK\nfKu3utMOxT7BC+DTwR1JTdMgAcjFk4y/UQxIcfyduLVXlWaZDPb1Ve8lEJkgt9kz\n5e3zz+exaCgBpLqWn9V/FJECgYBFlpN2N2RXY04Okt6HWdF47+QIhJx+TWmtOmdZ\n9ZNTj8ZaOMK6tpWI6354gSMqoEE7c457qQpnCteEz4MsGHQluy2kAee7hUaBPLuG\nAWoS+m5alJQ01Pd6U3Vkzz4oTk3wT5+ZjICGHMBqU3iKEBkawysff6rvfEBYwQnN\nIeDbVQKBgFIz6Inou83siOUhFbDZnLmedRGINPw5NJiHpRS/x8rdWAN1brrvqj+l\nFKwTUHgHnazREeLFlQ4kRvdKK+zogJPfA9T0Zetf49YHYuG08+oo4pZSrtbabacZ\nnES7vAtMC5TCvwKTNXBk1nbxdX3jVFJl88wiTpBbnwSqnjt/Ta8t\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/kibana.key\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDSTCCAjGgAwIBAgIUal1tFY90NA8IgvMVUs/jfyG3N00wDQYJKoZIhvcNAQEL\nBQAwNDEyMDAGA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5l\ncmF0ZWQgQ0EwHhcNMjEwMTE0MTQwODU2WhcNMjQwMTE0MTQwODU2WjA0MTIwMAYD\nVQQDEylFbGFzdGljIENlcnRpZmljYXRlIFRvb2wgQXV0b2dlbmVyYXRlZCBDQTCC\nASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKBXjJFIv4LL2DfqhUdPQGJl\nMmTgxIvqkDsoOxDYuTP27hY27wKUWFBO0sYkWxpddenGQxk66SzkU1XukvGw/Ygs\nezyuvRZq2UlOOt+2eIIjYmb6TIfhpDbMeVhYXfKiFtRHUYskd2VfSUktff8NEnC1\niWLPQaMdv81CmMOGkshjVYn4gaHD+b8Kv7sfnFn5WYMojWez1OOfWke+lJfw+sIa\ntOaZ+ufGZB50H63OZrUJJJa0QahlTakHJpXrk5x0mUq/E9P74FUFQ+tDqUPjLXQq\naPFbzwtSyiT0Rk8nMqu2TQm0kkz79wjR44MmXJo+qFAbMVY/fam+kLEpth0UL9UC\nAwEAAaNTMFEwHQYDVR0OBBYEFGILMWbvri92AW4ehKAe1uPVYJglMB8GA1UdIwQY\nMBaAFGILMWbvri92AW4ehKAe1uPVYJglMA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZI\nhvcNAQELBQADggEBAG4SEZ6beme+BG00ybv4YcFVEhUI24jgrpntyIOHC2ARkLXW\n1yl22Ue/eRg8fP9UhHH2YmUXt2Fy8b1HzihH38WAO/Wxx2K6u38C2ADuTppELWIU\nXUktUCOeYB2B5jbwFSExm6N2Rhj9YJsdlm/Lvph9s2VQThdKUZPOXqdi8u5C6L0k\ns2gkrKJdm4hF7NnVcgIzBPBY86sYzOMW1CXFP6o887KxKjPI7A1JAAkrPZz3ob0n\nB9pBayly5UAtixLJhQbkDfGAB1gRnDWaCDYmN+YT4LaQ/tsqueW8Ba1hQ1PqVLvw\n6J6ytch0M9sCIQe1PzLhEJRpfqtqeacr8yEUgkg=\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/ca.crt\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name              = \"kibana\"\n        port              = \"http\"\n        tags              = [ \"kibana${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Kibana Transport Check Live\"\n          port            = \"http\"\n          type            = \"tcp\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n        check {\n          name            = \"Elastic Kibana HTTP Check Live\"\n          type            = \"http\"\n          port            = \"http\"\n          protocol        = \"https\"\n          method          = \"GET\"\n          tls_skip_verify = true\n          path            = \"/\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu           = 1000\n        memory        = 8192\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        network {\n          port \"http\" {\n            static    = 5601\n          }\n        }\n      }\n    }\n  }\n}\n",
            "template": "job \"${job_name}\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"${datacenters}\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 1\n\n    health_check      = \"checks\"\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n%{ if use_canary }\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 1\n\n    # Specifies if the job should auto-promote to the canary version when all\n    # canaries become healthy during a deployment. Defaults to false which means\n    # canaries must be manually updated with the nomad deployment promote\n    # command.\n    auto_promote      = true\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = true\n%{ endif }\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-elastic-cluster\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count             = ${group_count}\n\n    constraint {\n      attribute       = \"$${node.unique.name}\"\n      value           = \"s41-nomad-x86_64\"\n    }\n\n    ephemeral_disk {\n      size            = \"50000\"\n      sticky          = true\n      migrate         = false\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-elastic-cluster\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver          = \"docker\"\n\n      kill_timeout    = \"600s\"\n      kill_signal     = \"SIGTERM\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image         = \"docker.elastic.co/elasticsearch/elasticsearch:${version}\"\n        dns_servers   = [ \"$${attr.unique.network.ip-address}\" ]\n        command       = \"elasticsearch\"\n        args          = [\n          \"-Enode.name=${cluster_service_name}$${NOMAD_ALLOC_INDEX}\",\n          \"-Enetwork.host=0.0.0.0\",\n          \"-Ecluster.name=${cluster_service_name}\",\n          \"-Ehttp.port=$${NOMAD_PORT_rest}\",\n          \"-Ehttp.publish_port=$${NOMAD_HOST_PORT_rest}\",\n          \"-Ebootstrap.memory_lock=true\",\n          \"-Epath.logs=/alloc/logs/\",\n          \"-Ediscovery.type=single-node\",\n          \"-Etransport.publish_port=$${NOMAD_HOST_PORT_transport}\",\n          \"-Etransport.port=$${NOMAD_PORT_transport}\",\n          \"-Expack.license.self_generated.type=basic\",\n          \"-Expack.security.enabled=true\",\n          \"-Expack.security.http.ssl.enabled=true\",\n          \"-Expack.security.http.ssl.key=certs/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.key\",\n          \"-Expack.security.http.ssl.certificate=certs/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.crt\",\n          \"-Expack.security.http.ssl.certificate_authorities=certs/ca.crt\",\n          \"-Expack.security.transport.ssl.enabled=true\",\n          \"-Expack.security.transport.ssl.key=certs/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.key\",\n          \"-Expack.security.transport.ssl.certificate=certs/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.crt\",\n          \"-Expack.security.transport.ssl.certificate_authorities=certs/ca.crt\",\n          \"-Expack.security.transport.ssl.verification_mode=certificate\"\n        ]\n        volumes       = [\n          \"secrets/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.crt:/usr/share/elasticsearch/config/certs/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.crt\",\n          \"secrets/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.key:/usr/share/elasticsearch/config/certs/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.key\",\n          \"secrets/ca.crt:/usr/share/elasticsearch/config/certs/ca.crt\",\n          \"secrets/ca.key:/usr/share/elasticsearch/config/certs/ca.key\",\n          \"secrets/instances.yml:/usr/share/elasticsearch/config/instances.yaml\",\n          \"secrets/users_roles:/usr/share/elasticsearch/config/users_roles\",\n          \"secrets/users:/usr/share/elasticsearch/config/users\"\n        ]\n        ulimit {\n          memlock     = \"-1\"\n          nofile      = \"65536\"\n          nproc       = \"8192\"\n        }\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\ninstances:\n  - name: 'elastic0'\n    dns: [ 'elastic0.elastic.service.consul', 'elastic.service.consul']\n  - name: 'kibana'\n    dns: [ 'kibana.service.consul' ]\nEOF\n        destination  = \"secrets/instances.yml\"\n      }\n      template {\n        data         = \u003c\u003cEOF\nelasticuser:$2a$10$kgJDjo1/pBaBsnUHvsGOiOT3IAJhk2TBVNJeTv/1EPg//klcJCK4y\nEOF\n        destination  = \"secrets/users\"\n      }\n      template {\n        data         = \u003c\u003cEOF\nkibana_admin:elasticuser\nmonitoring_user:elasticuser\nsuperuser:elasticuser\nEOF\n        destination  = \"secrets/users_roles\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDaDCCAlCgAwIBAgIUG3esFYSWamMDpavP3zw/JTA5svswDQYJKoZIhvcNAQEL\nBQAwNDEyMDAGA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5l\ncmF0ZWQgQ0EwHhcNMjEwMTE0MTQwODU2WhcNMjQwMTE0MTQwODU2WjATMREwDwYD\nVQQDEwhlbGFzdGljMDCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALUU\nZYRGTcGz4NrP7vIzZo5kDBekffRYugqBzfDztNtfqBPEKB/rK9tJr8Qp2IfKJJ0l\nyuMTkYUcebjKIHfOJ4MAvNKtA1lSj3j0FJSOw9LXFuIYKCD661RmdWiwnz4pAVIs\noAriQmz/Z1hib38vmZ7dyYgS1VkUssHDxKM9q/9Q1ILmuOtuOJN6e5shnGNTm2Ft\nj3uNk8ZTHBOM4LiCBu4tQ3v1DIjdT4t2pG5grQu374Sinux3YlcIeuxQbpAVaOl+\ngfQnK7skdtXWMYVmZnuMJpRRIxzWyhsAlWaOletbjQ3xOqW/oXtfg9ve6qPhFJuC\ns5l3YqufKlXVoyrbXCkCAwEAAaOBkjCBjzAdBgNVHQ4EFgQUKLG4/6t+9LtjH18Q\n7PTFas+YXzAwHwYDVR0jBBgwFoAUYgsxZu+uL3YBbh6EoB7W49VgmCUwQgYDVR0R\nBDswOYIfZWxhc3RpYzAuZWxhc3RpYy5zZXJ2aWNlLmNvbnN1bIIWZWxhc3RpYy5z\nZXJ2aWNlLmNvbnN1bDAJBgNVHRMEAjAAMA0GCSqGSIb3DQEBCwUAA4IBAQAEepAs\nh7d+a2k6Qj7B3KyZnX0O50toeZW+tKnnfGin0H5LGgvVn40mRJEJKBzatp/LvHh+\nx//YM+x8IbZe7bDtf69EUqq6C3881Xsq1jj77GZ1buEP+W9nRNjM3o4mjcn3RfPw\nGRV6lHnpHvhqAUIFtlOHvaa0UuEbqQkomxr/e44btRdnDQ6SRRh4xwBKYT/O/a7O\nYpS514Q4vKNQ6XLSAdGpJjK6KdHO5xkzCi4zYastpuy8ct/qqiAklAtXaF+7F0OX\nriSs0AsAokMsF+hLv5d1kAH535uHs7Mr85gw7Y7/qlXmJovh4oWcys8XTPvlHhkQ\n98I9yFJ7HRAUgbah\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEpQIBAAKCAQEAtRRlhEZNwbPg2s/u8jNmjmQMF6R99Fi6CoHN8PO021+oE8Qo\nH+sr20mvxCnYh8oknSXK4xORhRx5uMogd84ngwC80q0DWVKPePQUlI7D0tcW4hgo\nIPrrVGZ1aLCfPikBUiygCuJCbP9nWGJvfy+Znt3JiBLVWRSywcPEoz2r/1DUgua4\n6244k3p7myGcY1ObYW2Pe42TxlMcE4zguIIG7i1De/UMiN1Pi3akbmCtC7fvhKKe\n7HdiVwh67FBukBVo6X6B9CcruyR21dYxhWZme4wmlFEjHNbKGwCVZo6V61uNDfE6\npb+he1+D297qo+EUm4KzmXdiq58qVdWjKttcKQIDAQABAoIBAQCMeL4n1sILOhd8\np0Gd8fHlFAetb5WmMA5iiD/SY7wxUgt5Cfp2iGEFRCxt6GhpLo8ouWCit1N0B5sF\nlweI6QwNvEy+wiiO6lUSZ4ZvmDChJupBiqvWqdBVMQZzqFBgUD8OGEAvMUaGd7sb\n/YCxEaQCcdsdDD8lU8E4Pz4TxIvhCukcFV/Jd60yh2K7UAnruG8ZcytmTsfnA7s7\nIIsnu20YllwflSjkb3oKtOsgKjMc6Oj01vjH90W+UXC7S+OJ/bXdCCBJn6QYfVRd\nIQ9l0bvtu+4yoRBmXRbIJ6zf8++FOBXa25I5U2VJbw44VszlZfeRahqoxF8jrSTh\n7uZZr3HhAoGBAPwM30AmGP0msTbJSm5KhSdwhxLtQ1xAd3z8HmgEQ99ncdeVqJ/b\nkHh6L3UW+0kLHHvJs0D4xlr6CJrZBRtwJLxuE7dtlBD7ouvVZjdQ09drKio7/aC3\nL3/spyRAw7yHlZFVkvNapZV9TUfcpwrza4FUiHdhATw7YhPQQ53x54t3AoGBALfq\n0WRksjZrMSxQWzPCBkVx1efc6WzednV8FJFldLbA1+sVi1g0fSW6y5EK/gHgezIp\nEdyG937lUnXyCYxMH+LuZUkq2ZyUjpH/pQxCBXpmwDA8shRD/UTBtpd/x/ORapCs\nPF/sC5eXATyQX7ADWQeeTZndWICNQuLNf0mKNv1fAoGBAOd8cvWZh83IYW2txTwy\nGMS2JngNjJYHZzZU3yAs+qENgpK7Eplur+rWXQuuxa66E7jk8Eq1sIcRqCF/O5+N\niU+90UHf0+MdGO57mVsoUsc/1wPfAPtAAtH8aS10hdB6vbUy4Lm8AOOgpv9e+dOm\n6I9pMcRiRR4qc9M6rT88UqnVAoGAblX0eusiMx2JqZEntdxf0MejUW+ppkOsA32G\nBVg9deopXwJUz3zl23295G0Yx915azVSXt+lmT5QgyvKaJ2+v3DP2N5ZIOPKyHH6\n/WiaSr1b7VRsbVYAmoAwX6EsPsZtjQ+XROCib7YK6t+eWEUZ40UoPveYwb59cv1f\nsKm3pbcCgYEAgyYc6DdrsAgT4S9g38hdqtuiSejhcGojHVeaMmPNxgoaXZZ66+g6\nKujDXHJtkUCBluXOhfsQcjAr2UzvxpGdvDd5Ym5HA6l/2qAtph+f8DUofESSz7vh\nT9/4PLw82sIU9/wFM40F1IV20W1TgUnJZln4HdpWPpwmXottoOX3y2s=\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.key\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDSTCCAjGgAwIBAgIUal1tFY90NA8IgvMVUs/jfyG3N00wDQYJKoZIhvcNAQEL\nBQAwNDEyMDAGA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5l\ncmF0ZWQgQ0EwHhcNMjEwMTE0MTQwODU2WhcNMjQwMTE0MTQwODU2WjA0MTIwMAYD\nVQQDEylFbGFzdGljIENlcnRpZmljYXRlIFRvb2wgQXV0b2dlbmVyYXRlZCBDQTCC\nASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKBXjJFIv4LL2DfqhUdPQGJl\nMmTgxIvqkDsoOxDYuTP27hY27wKUWFBO0sYkWxpddenGQxk66SzkU1XukvGw/Ygs\nezyuvRZq2UlOOt+2eIIjYmb6TIfhpDbMeVhYXfKiFtRHUYskd2VfSUktff8NEnC1\niWLPQaMdv81CmMOGkshjVYn4gaHD+b8Kv7sfnFn5WYMojWez1OOfWke+lJfw+sIa\ntOaZ+ufGZB50H63OZrUJJJa0QahlTakHJpXrk5x0mUq/E9P74FUFQ+tDqUPjLXQq\naPFbzwtSyiT0Rk8nMqu2TQm0kkz79wjR44MmXJo+qFAbMVY/fam+kLEpth0UL9UC\nAwEAAaNTMFEwHQYDVR0OBBYEFGILMWbvri92AW4ehKAe1uPVYJglMB8GA1UdIwQY\nMBaAFGILMWbvri92AW4ehKAe1uPVYJglMA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZI\nhvcNAQELBQADggEBAG4SEZ6beme+BG00ybv4YcFVEhUI24jgrpntyIOHC2ARkLXW\n1yl22Ue/eRg8fP9UhHH2YmUXt2Fy8b1HzihH38WAO/Wxx2K6u38C2ADuTppELWIU\nXUktUCOeYB2B5jbwFSExm6N2Rhj9YJsdlm/Lvph9s2VQThdKUZPOXqdi8u5C6L0k\ns2gkrKJdm4hF7NnVcgIzBPBY86sYzOMW1CXFP6o887KxKjPI7A1JAAkrPZz3ob0n\nB9pBayly5UAtixLJhQbkDfGAB1gRnDWaCDYmN+YT4LaQ/tsqueW8Ba1hQ1PqVLvw\n6J6ytch0M9sCIQe1PzLhEJRpfqtqeacr8yEUgkg=\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/ca.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEAoFeMkUi/gsvYN+qFR09AYmUyZODEi+qQOyg7ENi5M/buFjbv\nApRYUE7SxiRbGl116cZDGTrpLORTVe6S8bD9iCx7PK69FmrZSU4637Z4giNiZvpM\nh+GkNsx5WFhd8qIW1EdRiyR3ZV9JSS19/w0ScLWJYs9Box2/zUKYw4aSyGNVifiB\nocP5vwq/ux+cWflZgyiNZ7PU459aR76Ul/D6whq05pn658ZkHnQfrc5mtQkklrRB\nqGVNqQcmleuTnHSZSr8T0/vgVQVD60OpQ+MtdCpo8VvPC1LKJPRGTycyq7ZNCbSS\nTPv3CNHjgyZcmj6oUBsxVj99qb6QsSm2HRQv1QIDAQABAoIBADz9EBqy8SVvI+8g\n5VEadAL5OxHj7N7LedEGnHDr/oYlhqosev0gL/dcBBAaBA0jP5aMMzmFjuvkbU5i\nUMJd8BG72aRbUtEUE1Iuz3YIkg3uJ5/D1RhaW3v8iqtv8Uw5GzXjasDiPgfxFo8f\nHq3E6x6z7m4HJ5BD4JDSpAi7R1mw0VmklleDG/7FjCbWjyJIbv9I0337uqiFQJ6t\nERPf7+LK0mdaBvhPcWBSShXPi0p6kTe+/DcWmcf9ORBII9PRipd0bViSoot4xSWc\n6DKjY0lDK23C4YPNOFb5aI4BfsMxcXhEnf6oAx1+oKDgGSDrPmMtuUB3OzdWPeBv\nZUtAZAECgYEA+pHYIX+KQ+cDBgFxlzZwQBUVRZzie9twUCZrAXcrV4qTSbI3ML1f\nTrzSzDsdfVYuPUUCy5hbSJ5SqYchZ09oWhKXgfffsHwgMO2faN7iUYPvFKyxUrMS\n+Gj8VgkiQEV3ka820LxDbaVc34R6kkoOnRt1QSohoARZzmN/0K9/pe0CgYEAo9Ef\njdpS0HF7nlJ2lc7WJirKdg8ECCVuswD7Ay4SKz1/pIcSdTES2w1204d1v14jPt/S\ncoy5VT28CT4Zfn/D9yhj/NvIG9IKshAGClcVAkH+Uh9iXBIZvPzashZD6Xp0FfRj\n5QX6koiS8Jxnf2A+1s5SLNKPnWoPwWnKCvJJdIkCgYEA0x/X8EG6ioQ3c/P7deGU\nqyoYhlMuMhYviBkWyGFUz6ofeFUFU7f8eid3pkWZD2ZyB4YCWPHC2GkuVVFav+WU\nk3Be4E+u1tF/fjp5uq8yGmUEKXNo5bmlHlG3a/a+OVFO8h2kHjTCy7wtiNfjPyfP\nMGlWXtXVBzMjSFdl9rwo3fECgYA1tOPxb7hi2jG7EDIMn0kaLkE+P2IFAbCvQw0I\nV9xhDMKCQD5O6Y3S/zEL3Ic//C71+A9YusYwKhMxvIhDLsQijb1qMuwCIvSauCIi\n1bXvjY9BgUSQBuclTIiuhhoxu5G/eOYfObySue/iroRIAFfZuL68LzQiWZlcwcAZ\noqFucQKBgCFUFgFc9LebqwBC/3nEgeAkDRutipp8TQOw4ZFCVI43ShFNz4CJqaq9\nuFBgoI6tMoc5LDBTzs7rJvijcWN44pbo2COVPHQZDCNsI15G0t5JjY0cOz426D9r\nRVhLPYqpE6zQHy35PAnZnYQ8h/VfIsmGvkczjeADPIXR8jyf6FQ7\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/ca.key\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name              = \"${cluster_service_name}\"\n        port              = \"rest\"\n        tags              = [ \"${cluster_service_name}$${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Cluster REST Check Live\"\n          port            = \"rest\"\n          type            = \"tcp\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n        check {\n          name            = \"Elastic Cluster HTTP Check Live\"\n          type            = \"http\"\n          port            = \"rest\"\n          protocol        = \"https\"\n          header {\n            Authorization = [\"Basic ZWxhc3RpY3VzZXI6RWxhc3RpYzEyMzQ=\"]\n          }\n          tls_skip_verify = true\n          path            = \"/_cat/health?pretty\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n      service {\n        name              = \"${cluster_service_name}-transport\"\n        port              = \"transport\"\n        tags              = [ \"${cluster_service_name}-transport$${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Cluster Transport Check Live\"\n          type            = \"tcp\"\n          port            = \"transport\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu        = ${cluster_cpu}\n        memory     = ${cluster_memory}\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        network {\n          port \"rest\" {\n            static = ${cluster_rest_port}\n          }\n          port \"transport\" {\n            static = ${cluster_transport_port}\n          }\n        }\n      }\n    }\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-elastic-kibana\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count              = 1\n\n    constraint {\n      attribute        = \"$${node.unique.name}\"\n      value            = \"s41-nomad-x86_64\"\n    }\n\n    update {\n      max_parallel     = 1\n      health_check     = \"checks\"\n      min_healthy_time = \"10s\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-elastic-kibana\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver           = \"docker\"\n\n      kill_timeout     = \"60s\"\n      kill_signal      = \"SIGTERM\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image         = \"docker.elastic.co/kibana/kibana:${version}\"\n        dns_servers   = [ \"$${attr.unique.network.ip-address}\" ]\n        command       = \"kibana\"\n        args          = [\n          \"--server.name=${kibana_service_name}\",\n          \"--server.host=0.0.0.0\",\n          \"--server.port=$${NOMAD_PORT_http}\",\n          \"--server.ssl.enabled=true\",\n          \"--server.ssl.certificate=/etc/kibana/config/certs/${kibana_service_name}.crt\",\n          \"--server.ssl.key=/etc/kibana/config/certs/${kibana_service_name}.key\",\n          \"--elasticsearch.hosts=https://${cluster_service_name}0.elastic.service.consul:9200\",\n          \"--elasticsearch.username=elasticuser\",\n          \"--elasticsearch.password=${cluster_password}\",\n          \"--elasticsearch.ssl.certificateAuthorities=/etc/kibana/config/certs/ca.crt\",\n          \"--xpack.apm.ui.enabled=false\",\n          \"--xpack.graph.enabled=false\",\n          \"--xpack.grokdebugger.enabled=false\",\n          \"--xpack.maps.enabled=false\",\n          \"--xpack.ml.enabled=false\",\n          \"--xpack.searchprofiler.enabled=false\"\n        ]\n        volumes       = [\n          \"secrets/${kibana_service_name}.crt:/etc/kibana/config/certs/${kibana_service_name}.crt\",\n          \"secrets/${kibana_service_name}.key:/etc/kibana/config/certs/${kibana_service_name}.key\",\n          \"secrets/ca.crt:/etc/kibana/config/certs/ca.crt\"\n        ]\n        ulimit {\n          memlock     = \"-1\"\n          nofile      = \"65536\"\n          nproc       = \"8192\"\n        }\n      }\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDQzCCAiugAwIBAgIVAOckTq6CgphZAuFRgi8dAgkk3mIIMA0GCSqGSIb3DQEB\nCwUAMDQxMjAwBgNVBAMTKUVsYXN0aWMgQ2VydGlmaWNhdGUgVG9vbCBBdXRvZ2Vu\nZXJhdGVkIENBMB4XDTIxMDExNDE0MDg1N1oXDTI0MDExNDE0MDg1N1owETEPMA0G\nA1UEAxMGa2liYW5hMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAsAbC\nTe1t0tZFDO9stsqpl1g16DPGuk4hqe5HrXBDRk1atswitJ5+hVdWnCiZ9zBHsXLO\nDN5m9AkqYe4r9e7+CIAW6v3TzvYa6qP9rQVt873l/zJnZp1mS2Cro+HdLctXCTW3\n3OC7VjIOmGidbNFPWe+Nh5Plp2u9eQSDLJ3OGx0Q9GlQAUp9dbHsauWpLBy0YRsQ\nX+0UHTm9Nx04cNZGilC12F/LuJtuqFiqCSQ/QAPmb+AJWsTZAPlhQl7raNh6Ghcj\nO11GZzlQQ1MWjNCaBtsRFJsOrbYbK5qbvZUbm9FPDjlKCZ7TCrVcjwwYrP/9HvfV\nCS9eQ+Q7JhxjKV6zowIDAQABo28wbTAdBgNVHQ4EFgQU39rJmu9XDHTDDyvKAIPr\nP6Fek5MwHwYDVR0jBBgwFoAUYgsxZu+uL3YBbh6EoB7W49VgmCUwIAYDVR0RBBkw\nF4IVa2liYW5hLnNlcnZpY2UuY29uc3VsMAkGA1UdEwQCMAAwDQYJKoZIhvcNAQEL\nBQADggEBAD8ySxda8bqehs4ZdmdFBe0n0Fqo6KK8rRCGqKu/qpzuSA9/T372NE/k\nWhx3QQPWcb3DhS+oEZ2s8KPrq6pSZtDcQqMWusxeNX7L/V0FLtKneksP8w/y0Wb4\nKeAss66DVrr6Jl1WNzPO0Ia9SugQa4gcXf6M6sH72NqgQZMqfkUoPw3OrKxgD7zD\nww/eKW82CRW1/SkHEbpgIhT4zl2MOnlIT1XGBl+OdFLlTo9QQJtZl4+p9VBvvuVC\nKwdU1h+0YRL7ktT3JcmVdvloxmQljymCx4AttPAZubXDVr0r02ne51bJw92gomHY\noG/i7diepgsKwG0txI8FIaF17URHdg0=\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/${kibana_service_name}.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEAsAbCTe1t0tZFDO9stsqpl1g16DPGuk4hqe5HrXBDRk1atswi\ntJ5+hVdWnCiZ9zBHsXLODN5m9AkqYe4r9e7+CIAW6v3TzvYa6qP9rQVt873l/zJn\nZp1mS2Cro+HdLctXCTW33OC7VjIOmGidbNFPWe+Nh5Plp2u9eQSDLJ3OGx0Q9GlQ\nAUp9dbHsauWpLBy0YRsQX+0UHTm9Nx04cNZGilC12F/LuJtuqFiqCSQ/QAPmb+AJ\nWsTZAPlhQl7raNh6GhcjO11GZzlQQ1MWjNCaBtsRFJsOrbYbK5qbvZUbm9FPDjlK\nCZ7TCrVcjwwYrP/9HvfVCS9eQ+Q7JhxjKV6zowIDAQABAoIBAQCQJQjKTaqIY6Rp\n4kpRKYZVBAwo2PVcrQyOHi0eDvdYQ5IMbP/ijoOm541qFSl3rVaYLh4jlaATKMpH\nJYVkQFBQX6vkxPTE3u3NxXq/S9ntJk2IfBsGgdA527DSY+v+Syw7w3yL6JAgFp+z\nGMAJUyG60RtBsc/3GJgw2IweZh9YPUdmgPy6Ci2MutXVJwiRlies9xSDJwgzQHCr\nCrJJmhc2kTxX16yBJ/KM1aJjME+/fVBvvr9+QHrK4sqqgMXCtzLCfBRocWcv71kq\ntnTVBrYrPH/Wwq//5B5qlshJ99d1V+k4btVpLDqL17iB2bWRYZ00QTluepf0C77n\nis/j/uR5AoGBAPJE8zOYePYb6pMnVdMjAtEeR7zPMyr+fqpkTtzaSydLor58cB8z\nhyLzW2AeD+2qGAIoojjfnkhZm56/kYMnyuS6+qW2wO6527ts5K37XEMHsPs+33fJ\nWN0sOYZM+kBw/3YFsq4o/rl0poj7WObe47GEQqp8cV+r5RgPMDy9AoBvAoGBALoA\nszcwYODvR0H8UMCIk0l3QWVAgP8HLDP0ZVJ2GYzek+0nx30LFJLkoMohoxg01FKd\n0t3HvsQnyR582hXGmTt5g87BjOTKiFJO1ivMgaNaVoYSF4G4Jiobf5R3Lq0tY5od\nAPu1uW0RfnX1vDaCEFaphFhb57n+7us0rhfSn7INAoGAT3a+LpY8Vr0hW9LzG6XI\nLr832H490kRXV5w/IcGYFPOCFejK/fDwyk34ErbJkrLP3SVm0DDIwgJiQNek6tgK\nfKu3utMOxT7BC+DTwR1JTdMgAcjFk4y/UQxIcfyduLVXlWaZDPb1Ve8lEJkgt9kz\n5e3zz+exaCgBpLqWn9V/FJECgYBFlpN2N2RXY04Okt6HWdF47+QIhJx+TWmtOmdZ\n9ZNTj8ZaOMK6tpWI6354gSMqoEE7c457qQpnCteEz4MsGHQluy2kAee7hUaBPLuG\nAWoS+m5alJQ01Pd6U3Vkzz4oTk3wT5+ZjICGHMBqU3iKEBkawysff6rvfEBYwQnN\nIeDbVQKBgFIz6Inou83siOUhFbDZnLmedRGINPw5NJiHpRS/x8rdWAN1brrvqj+l\nFKwTUHgHnazREeLFlQ4kRvdKK+zogJPfA9T0Zetf49YHYuG08+oo4pZSrtbabacZ\nnES7vAtMC5TCvwKTNXBk1nbxdX3jVFJl88wiTpBbnwSqnjt/Ta8t\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/${kibana_service_name}.key\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDSTCCAjGgAwIBAgIUal1tFY90NA8IgvMVUs/jfyG3N00wDQYJKoZIhvcNAQEL\nBQAwNDEyMDAGA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5l\ncmF0ZWQgQ0EwHhcNMjEwMTE0MTQwODU2WhcNMjQwMTE0MTQwODU2WjA0MTIwMAYD\nVQQDEylFbGFzdGljIENlcnRpZmljYXRlIFRvb2wgQXV0b2dlbmVyYXRlZCBDQTCC\nASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKBXjJFIv4LL2DfqhUdPQGJl\nMmTgxIvqkDsoOxDYuTP27hY27wKUWFBO0sYkWxpddenGQxk66SzkU1XukvGw/Ygs\nezyuvRZq2UlOOt+2eIIjYmb6TIfhpDbMeVhYXfKiFtRHUYskd2VfSUktff8NEnC1\niWLPQaMdv81CmMOGkshjVYn4gaHD+b8Kv7sfnFn5WYMojWez1OOfWke+lJfw+sIa\ntOaZ+ufGZB50H63OZrUJJJa0QahlTakHJpXrk5x0mUq/E9P74FUFQ+tDqUPjLXQq\naPFbzwtSyiT0Rk8nMqu2TQm0kkz79wjR44MmXJo+qFAbMVY/fam+kLEpth0UL9UC\nAwEAAaNTMFEwHQYDVR0OBBYEFGILMWbvri92AW4ehKAe1uPVYJglMB8GA1UdIwQY\nMBaAFGILMWbvri92AW4ehKAe1uPVYJglMA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZI\nhvcNAQELBQADggEBAG4SEZ6beme+BG00ybv4YcFVEhUI24jgrpntyIOHC2ARkLXW\n1yl22Ue/eRg8fP9UhHH2YmUXt2Fy8b1HzihH38WAO/Wxx2K6u38C2ADuTppELWIU\nXUktUCOeYB2B5jbwFSExm6N2Rhj9YJsdlm/Lvph9s2VQThdKUZPOXqdi8u5C6L0k\ns2gkrKJdm4hF7NnVcgIzBPBY86sYzOMW1CXFP6o887KxKjPI7A1JAAkrPZz3ob0n\nB9pBayly5UAtixLJhQbkDfGAB1gRnDWaCDYmN+YT4LaQ/tsqueW8Ba1hQ1PqVLvw\n6J6ytch0M9sCIQe1PzLhEJRpfqtqeacr8yEUgkg=\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/ca.crt\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name              = \"${kibana_service_name}\"\n        port              = \"http\"\n        tags              = [ \"${kibana_service_name}$${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Kibana Transport Check Live\"\n          port            = \"http\"\n          type            = \"tcp\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n        check {\n          name            = \"Elastic Kibana HTTP Check Live\"\n          type            = \"http\"\n          port            = \"http\"\n          protocol        = \"https\"\n          method          = \"GET\"\n          tls_skip_verify = true\n          path            = \"/\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu           = ${kibana_cpu}\n        memory        = ${kibana_memory}\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        network {\n          port \"http\" {\n            static    = ${kibana_port}\n          }\n        }\n      }\n    }\n  }\n}\n",
            "vars": {
              "cluster_cpu": "40000",
              "cluster_memory": "40000",
              "cluster_password": "Elastic1234",
              "cluster_rest_port": "9200",
              "cluster_service_name": "elastic",
              "cluster_transport_port": "9300",
              "datacenters": "yul1",
              "group_count": "1",
              "job_name": "prod-elastic",
              "kibana_cpu": "1000",
              "kibana_memory": "8192",
              "kibana_port": "5601",
              "kibana_service_name": "kibana",
              "use_canary": "true",
              "version": "7.10.1"
            }
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.elastic",
      "mode": "managed",
      "type": "nomad_job",
      "name": "nomad_job_elastic",
      "provider": "provider[\"registry.terraform.io/hashicorp/nomad\"].yul1",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "allocation_ids": [
              "c1a74025-b219-f46d-c034-2453f1c212fb",
              "9883d0d8-79eb-8c44-4b2e-fb3f5c1ac227"
            ],
            "datacenters": [
              "yul1"
            ],
            "deployment_id": "b0a918aa-5201-ce4e-cd8b-c7ea58a6a93e",
            "deployment_status": "successful",
            "deregister_on_destroy": true,
            "deregister_on_id_change": true,
            "detach": false,
            "id": "prod-elastic",
            "jobspec": "job \"prod-elastic\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 1\n\n    health_check      = \"checks\"\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 1\n\n    # Specifies if the job should auto-promote to the canary version when all\n    # canaries become healthy during a deployment. Defaults to false which means\n    # canaries must be manually updated with the nomad deployment promote\n    # command.\n    auto_promote      = true\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = true\n\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-elastic-cluster\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count             = 1\n\n    constraint {\n      attribute       = \"${node.unique.name}\"\n      value           = \"s41-nomad-x86_64\"\n    }\n\n    ephemeral_disk {\n      size            = \"50000\"\n      sticky          = true\n      migrate         = false\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-elastic-cluster\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver          = \"docker\"\n\n      kill_timeout    = \"600s\"\n      kill_signal     = \"SIGTERM\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image         = \"docker.elastic.co/elasticsearch/elasticsearch:7.10.1\"\n        dns_servers   = [ \"${attr.unique.network.ip-address}\" ]\n        command       = \"elasticsearch\"\n        args          = [\n          \"-Enode.name=elastic${NOMAD_ALLOC_INDEX}\",\n          \"-Enetwork.host=0.0.0.0\",\n          \"-Ecluster.name=elastic\",\n          \"-Ehttp.port=${NOMAD_PORT_rest}\",\n          \"-Ehttp.publish_port=${NOMAD_HOST_PORT_rest}\",\n          \"-Ebootstrap.memory_lock=true\",\n          \"-Epath.logs=/alloc/logs/\",\n          \"-Ediscovery.type=single-node\",\n          \"-Etransport.publish_port=${NOMAD_HOST_PORT_transport}\",\n          \"-Etransport.port=${NOMAD_PORT_transport}\",\n          \"-Expack.license.self_generated.type=basic\",\n          \"-Expack.security.enabled=true\",\n          \"-Expack.security.http.ssl.enabled=true\",\n          \"-Expack.security.http.ssl.key=certs/elastic${NOMAD_ALLOC_INDEX}.key\",\n          \"-Expack.security.http.ssl.certificate=certs/elastic${NOMAD_ALLOC_INDEX}.crt\",\n          \"-Expack.security.http.ssl.certificate_authorities=certs/ca.crt\",\n          \"-Expack.security.transport.ssl.enabled=true\",\n          \"-Expack.security.transport.ssl.key=certs/elastic${NOMAD_ALLOC_INDEX}.key\",\n          \"-Expack.security.transport.ssl.certificate=certs/elastic${NOMAD_ALLOC_INDEX}.crt\",\n          \"-Expack.security.transport.ssl.certificate_authorities=certs/ca.crt\",\n          \"-Expack.security.transport.ssl.verification_mode=certificate\"\n        ]\n        volumes       = [\n          \"secrets/elastic${NOMAD_ALLOC_INDEX}.crt:/usr/share/elasticsearch/config/certs/elastic${NOMAD_ALLOC_INDEX}.crt\",\n          \"secrets/elastic${NOMAD_ALLOC_INDEX}.key:/usr/share/elasticsearch/config/certs/elastic${NOMAD_ALLOC_INDEX}.key\",\n          \"secrets/ca.crt:/usr/share/elasticsearch/config/certs/ca.crt\",\n          \"secrets/ca.key:/usr/share/elasticsearch/config/certs/ca.key\",\n          \"secrets/instances.yml:/usr/share/elasticsearch/config/instances.yaml\",\n          \"secrets/users_roles:/usr/share/elasticsearch/config/users_roles\",\n          \"secrets/users:/usr/share/elasticsearch/config/users\"\n        ]\n        ulimit {\n          memlock     = \"-1\"\n          nofile      = \"65536\"\n          nproc       = \"8192\"\n        }\n      }\n\n#      # The env stanza configures a list of environment variables to populate\n#      # the task's environment before starting.\n#      env {\n#        ELASTIC_PASSWORD       = \"Elastic1234\"\n#        ELASTIC_PASSWORD_FILE  = \"/usr/share/elasticsearch/config/password\"\n#      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\ninstances:\n  - name: 'elastic0'\n    dns: [ 'elastic0.elastic.service.consul', 'elastic.service.consul']\n  - name: 'kibana'\n    dns: [ 'kibana.service.consul' ]\nEOF\n        destination  = \"secrets/instances.yml\"\n      }\n      template {\n        data         = \u003c\u003cEOF\nelasticuser:$2a$10$kgJDjo1/pBaBsnUHvsGOiOT3IAJhk2TBVNJeTv/1EPg//klcJCK4y\nEOF\n        destination  = \"secrets/users\"\n      }\n      template {\n        data         = \u003c\u003cEOF\nkibana_admin:elasticuser\nmonitoring_user:elasticuser\nsuperuser:elasticuser\nEOF\n        destination  = \"secrets/users_roles\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDaDCCAlCgAwIBAgIUG3esFYSWamMDpavP3zw/JTA5svswDQYJKoZIhvcNAQEL\nBQAwNDEyMDAGA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5l\ncmF0ZWQgQ0EwHhcNMjEwMTE0MTQwODU2WhcNMjQwMTE0MTQwODU2WjATMREwDwYD\nVQQDEwhlbGFzdGljMDCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALUU\nZYRGTcGz4NrP7vIzZo5kDBekffRYugqBzfDztNtfqBPEKB/rK9tJr8Qp2IfKJJ0l\nyuMTkYUcebjKIHfOJ4MAvNKtA1lSj3j0FJSOw9LXFuIYKCD661RmdWiwnz4pAVIs\noAriQmz/Z1hib38vmZ7dyYgS1VkUssHDxKM9q/9Q1ILmuOtuOJN6e5shnGNTm2Ft\nj3uNk8ZTHBOM4LiCBu4tQ3v1DIjdT4t2pG5grQu374Sinux3YlcIeuxQbpAVaOl+\ngfQnK7skdtXWMYVmZnuMJpRRIxzWyhsAlWaOletbjQ3xOqW/oXtfg9ve6qPhFJuC\ns5l3YqufKlXVoyrbXCkCAwEAAaOBkjCBjzAdBgNVHQ4EFgQUKLG4/6t+9LtjH18Q\n7PTFas+YXzAwHwYDVR0jBBgwFoAUYgsxZu+uL3YBbh6EoB7W49VgmCUwQgYDVR0R\nBDswOYIfZWxhc3RpYzAuZWxhc3RpYy5zZXJ2aWNlLmNvbnN1bIIWZWxhc3RpYy5z\nZXJ2aWNlLmNvbnN1bDAJBgNVHRMEAjAAMA0GCSqGSIb3DQEBCwUAA4IBAQAEepAs\nh7d+a2k6Qj7B3KyZnX0O50toeZW+tKnnfGin0H5LGgvVn40mRJEJKBzatp/LvHh+\nx//YM+x8IbZe7bDtf69EUqq6C3881Xsq1jj77GZ1buEP+W9nRNjM3o4mjcn3RfPw\nGRV6lHnpHvhqAUIFtlOHvaa0UuEbqQkomxr/e44btRdnDQ6SRRh4xwBKYT/O/a7O\nYpS514Q4vKNQ6XLSAdGpJjK6KdHO5xkzCi4zYastpuy8ct/qqiAklAtXaF+7F0OX\nriSs0AsAokMsF+hLv5d1kAH535uHs7Mr85gw7Y7/qlXmJovh4oWcys8XTPvlHhkQ\n98I9yFJ7HRAUgbah\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/elastic${NOMAD_ALLOC_INDEX}.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEpQIBAAKCAQEAtRRlhEZNwbPg2s/u8jNmjmQMF6R99Fi6CoHN8PO021+oE8Qo\nH+sr20mvxCnYh8oknSXK4xORhRx5uMogd84ngwC80q0DWVKPePQUlI7D0tcW4hgo\nIPrrVGZ1aLCfPikBUiygCuJCbP9nWGJvfy+Znt3JiBLVWRSywcPEoz2r/1DUgua4\n6244k3p7myGcY1ObYW2Pe42TxlMcE4zguIIG7i1De/UMiN1Pi3akbmCtC7fvhKKe\n7HdiVwh67FBukBVo6X6B9CcruyR21dYxhWZme4wmlFEjHNbKGwCVZo6V61uNDfE6\npb+he1+D297qo+EUm4KzmXdiq58qVdWjKttcKQIDAQABAoIBAQCMeL4n1sILOhd8\np0Gd8fHlFAetb5WmMA5iiD/SY7wxUgt5Cfp2iGEFRCxt6GhpLo8ouWCit1N0B5sF\nlweI6QwNvEy+wiiO6lUSZ4ZvmDChJupBiqvWqdBVMQZzqFBgUD8OGEAvMUaGd7sb\n/YCxEaQCcdsdDD8lU8E4Pz4TxIvhCukcFV/Jd60yh2K7UAnruG8ZcytmTsfnA7s7\nIIsnu20YllwflSjkb3oKtOsgKjMc6Oj01vjH90W+UXC7S+OJ/bXdCCBJn6QYfVRd\nIQ9l0bvtu+4yoRBmXRbIJ6zf8++FOBXa25I5U2VJbw44VszlZfeRahqoxF8jrSTh\n7uZZr3HhAoGBAPwM30AmGP0msTbJSm5KhSdwhxLtQ1xAd3z8HmgEQ99ncdeVqJ/b\nkHh6L3UW+0kLHHvJs0D4xlr6CJrZBRtwJLxuE7dtlBD7ouvVZjdQ09drKio7/aC3\nL3/spyRAw7yHlZFVkvNapZV9TUfcpwrza4FUiHdhATw7YhPQQ53x54t3AoGBALfq\n0WRksjZrMSxQWzPCBkVx1efc6WzednV8FJFldLbA1+sVi1g0fSW6y5EK/gHgezIp\nEdyG937lUnXyCYxMH+LuZUkq2ZyUjpH/pQxCBXpmwDA8shRD/UTBtpd/x/ORapCs\nPF/sC5eXATyQX7ADWQeeTZndWICNQuLNf0mKNv1fAoGBAOd8cvWZh83IYW2txTwy\nGMS2JngNjJYHZzZU3yAs+qENgpK7Eplur+rWXQuuxa66E7jk8Eq1sIcRqCF/O5+N\niU+90UHf0+MdGO57mVsoUsc/1wPfAPtAAtH8aS10hdB6vbUy4Lm8AOOgpv9e+dOm\n6I9pMcRiRR4qc9M6rT88UqnVAoGAblX0eusiMx2JqZEntdxf0MejUW+ppkOsA32G\nBVg9deopXwJUz3zl23295G0Yx915azVSXt+lmT5QgyvKaJ2+v3DP2N5ZIOPKyHH6\n/WiaSr1b7VRsbVYAmoAwX6EsPsZtjQ+XROCib7YK6t+eWEUZ40UoPveYwb59cv1f\nsKm3pbcCgYEAgyYc6DdrsAgT4S9g38hdqtuiSejhcGojHVeaMmPNxgoaXZZ66+g6\nKujDXHJtkUCBluXOhfsQcjAr2UzvxpGdvDd5Ym5HA6l/2qAtph+f8DUofESSz7vh\nT9/4PLw82sIU9/wFM40F1IV20W1TgUnJZln4HdpWPpwmXottoOX3y2s=\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/elastic${NOMAD_ALLOC_INDEX}.key\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDSTCCAjGgAwIBAgIUal1tFY90NA8IgvMVUs/jfyG3N00wDQYJKoZIhvcNAQEL\nBQAwNDEyMDAGA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5l\ncmF0ZWQgQ0EwHhcNMjEwMTE0MTQwODU2WhcNMjQwMTE0MTQwODU2WjA0MTIwMAYD\nVQQDEylFbGFzdGljIENlcnRpZmljYXRlIFRvb2wgQXV0b2dlbmVyYXRlZCBDQTCC\nASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKBXjJFIv4LL2DfqhUdPQGJl\nMmTgxIvqkDsoOxDYuTP27hY27wKUWFBO0sYkWxpddenGQxk66SzkU1XukvGw/Ygs\nezyuvRZq2UlOOt+2eIIjYmb6TIfhpDbMeVhYXfKiFtRHUYskd2VfSUktff8NEnC1\niWLPQaMdv81CmMOGkshjVYn4gaHD+b8Kv7sfnFn5WYMojWez1OOfWke+lJfw+sIa\ntOaZ+ufGZB50H63OZrUJJJa0QahlTakHJpXrk5x0mUq/E9P74FUFQ+tDqUPjLXQq\naPFbzwtSyiT0Rk8nMqu2TQm0kkz79wjR44MmXJo+qFAbMVY/fam+kLEpth0UL9UC\nAwEAAaNTMFEwHQYDVR0OBBYEFGILMWbvri92AW4ehKAe1uPVYJglMB8GA1UdIwQY\nMBaAFGILMWbvri92AW4ehKAe1uPVYJglMA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZI\nhvcNAQELBQADggEBAG4SEZ6beme+BG00ybv4YcFVEhUI24jgrpntyIOHC2ARkLXW\n1yl22Ue/eRg8fP9UhHH2YmUXt2Fy8b1HzihH38WAO/Wxx2K6u38C2ADuTppELWIU\nXUktUCOeYB2B5jbwFSExm6N2Rhj9YJsdlm/Lvph9s2VQThdKUZPOXqdi8u5C6L0k\ns2gkrKJdm4hF7NnVcgIzBPBY86sYzOMW1CXFP6o887KxKjPI7A1JAAkrPZz3ob0n\nB9pBayly5UAtixLJhQbkDfGAB1gRnDWaCDYmN+YT4LaQ/tsqueW8Ba1hQ1PqVLvw\n6J6ytch0M9sCIQe1PzLhEJRpfqtqeacr8yEUgkg=\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/ca.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEAoFeMkUi/gsvYN+qFR09AYmUyZODEi+qQOyg7ENi5M/buFjbv\nApRYUE7SxiRbGl116cZDGTrpLORTVe6S8bD9iCx7PK69FmrZSU4637Z4giNiZvpM\nh+GkNsx5WFhd8qIW1EdRiyR3ZV9JSS19/w0ScLWJYs9Box2/zUKYw4aSyGNVifiB\nocP5vwq/ux+cWflZgyiNZ7PU459aR76Ul/D6whq05pn658ZkHnQfrc5mtQkklrRB\nqGVNqQcmleuTnHSZSr8T0/vgVQVD60OpQ+MtdCpo8VvPC1LKJPRGTycyq7ZNCbSS\nTPv3CNHjgyZcmj6oUBsxVj99qb6QsSm2HRQv1QIDAQABAoIBADz9EBqy8SVvI+8g\n5VEadAL5OxHj7N7LedEGnHDr/oYlhqosev0gL/dcBBAaBA0jP5aMMzmFjuvkbU5i\nUMJd8BG72aRbUtEUE1Iuz3YIkg3uJ5/D1RhaW3v8iqtv8Uw5GzXjasDiPgfxFo8f\nHq3E6x6z7m4HJ5BD4JDSpAi7R1mw0VmklleDG/7FjCbWjyJIbv9I0337uqiFQJ6t\nERPf7+LK0mdaBvhPcWBSShXPi0p6kTe+/DcWmcf9ORBII9PRipd0bViSoot4xSWc\n6DKjY0lDK23C4YPNOFb5aI4BfsMxcXhEnf6oAx1+oKDgGSDrPmMtuUB3OzdWPeBv\nZUtAZAECgYEA+pHYIX+KQ+cDBgFxlzZwQBUVRZzie9twUCZrAXcrV4qTSbI3ML1f\nTrzSzDsdfVYuPUUCy5hbSJ5SqYchZ09oWhKXgfffsHwgMO2faN7iUYPvFKyxUrMS\n+Gj8VgkiQEV3ka820LxDbaVc34R6kkoOnRt1QSohoARZzmN/0K9/pe0CgYEAo9Ef\njdpS0HF7nlJ2lc7WJirKdg8ECCVuswD7Ay4SKz1/pIcSdTES2w1204d1v14jPt/S\ncoy5VT28CT4Zfn/D9yhj/NvIG9IKshAGClcVAkH+Uh9iXBIZvPzashZD6Xp0FfRj\n5QX6koiS8Jxnf2A+1s5SLNKPnWoPwWnKCvJJdIkCgYEA0x/X8EG6ioQ3c/P7deGU\nqyoYhlMuMhYviBkWyGFUz6ofeFUFU7f8eid3pkWZD2ZyB4YCWPHC2GkuVVFav+WU\nk3Be4E+u1tF/fjp5uq8yGmUEKXNo5bmlHlG3a/a+OVFO8h2kHjTCy7wtiNfjPyfP\nMGlWXtXVBzMjSFdl9rwo3fECgYA1tOPxb7hi2jG7EDIMn0kaLkE+P2IFAbCvQw0I\nV9xhDMKCQD5O6Y3S/zEL3Ic//C71+A9YusYwKhMxvIhDLsQijb1qMuwCIvSauCIi\n1bXvjY9BgUSQBuclTIiuhhoxu5G/eOYfObySue/iroRIAFfZuL68LzQiWZlcwcAZ\noqFucQKBgCFUFgFc9LebqwBC/3nEgeAkDRutipp8TQOw4ZFCVI43ShFNz4CJqaq9\nuFBgoI6tMoc5LDBTzs7rJvijcWN44pbo2COVPHQZDCNsI15G0t5JjY0cOz426D9r\nRVhLPYqpE6zQHy35PAnZnYQ8h/VfIsmGvkczjeADPIXR8jyf6FQ7\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/ca.key\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name              = \"elastic\"\n        port              = \"rest\"\n        tags              = [ \"elastic${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Cluster REST Check Live\"\n          port            = \"rest\"\n          type            = \"tcp\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n        check {\n          name            = \"Elastic Cluster HTTP Check Live\"\n          type            = \"http\"\n          port            = \"rest\"\n          protocol        = \"https\"\n          header {\n            Authorization = [\"Basic ZWxhc3RpY3VzZXI6RWxhc3RpYzEyMzQ=\"]\n          }\n          tls_skip_verify = true\n          path            = \"/_cat/health?pretty\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n      service {\n        name              = \"elastic-transport\"\n        port              = \"transport\"\n        tags              = [ \"elastic-transport${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Cluster Transport Check Live\"\n          type            = \"tcp\"\n          port            = \"transport\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu        = 40000\n        memory     = 40000\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        network {\n          port \"rest\" {\n            static = 9200\n          }\n          port \"transport\" {\n            static = 9300\n          }\n        }\n      }\n    }\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-elastic-kibana\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count              = 1\n\n    constraint {\n      attribute        = \"${node.unique.name}\"\n      value            = \"s41-nomad-x86_64\"\n    }\n\n    update {\n      max_parallel     = 1\n      health_check     = \"checks\"\n      min_healthy_time = \"10s\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-elastic-kibana\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver           = \"docker\"\n\n      kill_timeout     = \"60s\"\n      kill_signal      = \"SIGTERM\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image         = \"docker.elastic.co/kibana/kibana:7.10.1\"\n        dns_servers   = [ \"${attr.unique.network.ip-address}\" ]\n        command       = \"kibana\"\n        args          = [\n          \"--server.name=kibana\",\n          \"--server.host=0.0.0.0\",\n          \"--server.port=${NOMAD_PORT_http}\",\n          \"--server.ssl.enabled=true\",\n          \"--server.ssl.certificate=/etc/kibana/config/certs/kibana.crt\",\n          \"--server.ssl.key=/etc/kibana/config/certs/kibana.key\",\n          \"--elasticsearch.hosts=https://elastic0.elastic.service.consul:9200\",\n          \"--elasticsearch.username=elasticuser\",\n          \"--elasticsearch.password=Elastic1234\",\n          \"--elasticsearch.ssl.certificateAuthorities=/etc/kibana/config/certs/ca.crt\",\n          \"--xpack.apm.ui.enabled=false\",\n          \"--xpack.graph.enabled=false\",\n          \"--xpack.grokdebugger.enabled=false\",\n          \"--xpack.maps.enabled=false\",\n          \"--xpack.ml.enabled=false\",\n          \"--xpack.searchprofiler.enabled=false\"\n        ]\n        volumes       = [\n          \"secrets/kibana.crt:/etc/kibana/config/certs/kibana.crt\",\n          \"secrets/kibana.key:/etc/kibana/config/certs/kibana.key\",\n          \"secrets/ca.crt:/etc/kibana/config/certs/ca.crt\"\n        ]\n        ulimit {\n          memlock     = \"-1\"\n          nofile      = \"65536\"\n          nproc       = \"8192\"\n        }\n      }\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDQzCCAiugAwIBAgIVAOckTq6CgphZAuFRgi8dAgkk3mIIMA0GCSqGSIb3DQEB\nCwUAMDQxMjAwBgNVBAMTKUVsYXN0aWMgQ2VydGlmaWNhdGUgVG9vbCBBdXRvZ2Vu\nZXJhdGVkIENBMB4XDTIxMDExNDE0MDg1N1oXDTI0MDExNDE0MDg1N1owETEPMA0G\nA1UEAxMGa2liYW5hMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAsAbC\nTe1t0tZFDO9stsqpl1g16DPGuk4hqe5HrXBDRk1atswitJ5+hVdWnCiZ9zBHsXLO\nDN5m9AkqYe4r9e7+CIAW6v3TzvYa6qP9rQVt873l/zJnZp1mS2Cro+HdLctXCTW3\n3OC7VjIOmGidbNFPWe+Nh5Plp2u9eQSDLJ3OGx0Q9GlQAUp9dbHsauWpLBy0YRsQ\nX+0UHTm9Nx04cNZGilC12F/LuJtuqFiqCSQ/QAPmb+AJWsTZAPlhQl7raNh6Ghcj\nO11GZzlQQ1MWjNCaBtsRFJsOrbYbK5qbvZUbm9FPDjlKCZ7TCrVcjwwYrP/9HvfV\nCS9eQ+Q7JhxjKV6zowIDAQABo28wbTAdBgNVHQ4EFgQU39rJmu9XDHTDDyvKAIPr\nP6Fek5MwHwYDVR0jBBgwFoAUYgsxZu+uL3YBbh6EoB7W49VgmCUwIAYDVR0RBBkw\nF4IVa2liYW5hLnNlcnZpY2UuY29uc3VsMAkGA1UdEwQCMAAwDQYJKoZIhvcNAQEL\nBQADggEBAD8ySxda8bqehs4ZdmdFBe0n0Fqo6KK8rRCGqKu/qpzuSA9/T372NE/k\nWhx3QQPWcb3DhS+oEZ2s8KPrq6pSZtDcQqMWusxeNX7L/V0FLtKneksP8w/y0Wb4\nKeAss66DVrr6Jl1WNzPO0Ia9SugQa4gcXf6M6sH72NqgQZMqfkUoPw3OrKxgD7zD\nww/eKW82CRW1/SkHEbpgIhT4zl2MOnlIT1XGBl+OdFLlTo9QQJtZl4+p9VBvvuVC\nKwdU1h+0YRL7ktT3JcmVdvloxmQljymCx4AttPAZubXDVr0r02ne51bJw92gomHY\noG/i7diepgsKwG0txI8FIaF17URHdg0=\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/kibana.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEAsAbCTe1t0tZFDO9stsqpl1g16DPGuk4hqe5HrXBDRk1atswi\ntJ5+hVdWnCiZ9zBHsXLODN5m9AkqYe4r9e7+CIAW6v3TzvYa6qP9rQVt873l/zJn\nZp1mS2Cro+HdLctXCTW33OC7VjIOmGidbNFPWe+Nh5Plp2u9eQSDLJ3OGx0Q9GlQ\nAUp9dbHsauWpLBy0YRsQX+0UHTm9Nx04cNZGilC12F/LuJtuqFiqCSQ/QAPmb+AJ\nWsTZAPlhQl7raNh6GhcjO11GZzlQQ1MWjNCaBtsRFJsOrbYbK5qbvZUbm9FPDjlK\nCZ7TCrVcjwwYrP/9HvfVCS9eQ+Q7JhxjKV6zowIDAQABAoIBAQCQJQjKTaqIY6Rp\n4kpRKYZVBAwo2PVcrQyOHi0eDvdYQ5IMbP/ijoOm541qFSl3rVaYLh4jlaATKMpH\nJYVkQFBQX6vkxPTE3u3NxXq/S9ntJk2IfBsGgdA527DSY+v+Syw7w3yL6JAgFp+z\nGMAJUyG60RtBsc/3GJgw2IweZh9YPUdmgPy6Ci2MutXVJwiRlies9xSDJwgzQHCr\nCrJJmhc2kTxX16yBJ/KM1aJjME+/fVBvvr9+QHrK4sqqgMXCtzLCfBRocWcv71kq\ntnTVBrYrPH/Wwq//5B5qlshJ99d1V+k4btVpLDqL17iB2bWRYZ00QTluepf0C77n\nis/j/uR5AoGBAPJE8zOYePYb6pMnVdMjAtEeR7zPMyr+fqpkTtzaSydLor58cB8z\nhyLzW2AeD+2qGAIoojjfnkhZm56/kYMnyuS6+qW2wO6527ts5K37XEMHsPs+33fJ\nWN0sOYZM+kBw/3YFsq4o/rl0poj7WObe47GEQqp8cV+r5RgPMDy9AoBvAoGBALoA\nszcwYODvR0H8UMCIk0l3QWVAgP8HLDP0ZVJ2GYzek+0nx30LFJLkoMohoxg01FKd\n0t3HvsQnyR582hXGmTt5g87BjOTKiFJO1ivMgaNaVoYSF4G4Jiobf5R3Lq0tY5od\nAPu1uW0RfnX1vDaCEFaphFhb57n+7us0rhfSn7INAoGAT3a+LpY8Vr0hW9LzG6XI\nLr832H490kRXV5w/IcGYFPOCFejK/fDwyk34ErbJkrLP3SVm0DDIwgJiQNek6tgK\nfKu3utMOxT7BC+DTwR1JTdMgAcjFk4y/UQxIcfyduLVXlWaZDPb1Ve8lEJkgt9kz\n5e3zz+exaCgBpLqWn9V/FJECgYBFlpN2N2RXY04Okt6HWdF47+QIhJx+TWmtOmdZ\n9ZNTj8ZaOMK6tpWI6354gSMqoEE7c457qQpnCteEz4MsGHQluy2kAee7hUaBPLuG\nAWoS+m5alJQ01Pd6U3Vkzz4oTk3wT5+ZjICGHMBqU3iKEBkawysff6rvfEBYwQnN\nIeDbVQKBgFIz6Inou83siOUhFbDZnLmedRGINPw5NJiHpRS/x8rdWAN1brrvqj+l\nFKwTUHgHnazREeLFlQ4kRvdKK+zogJPfA9T0Zetf49YHYuG08+oo4pZSrtbabacZ\nnES7vAtMC5TCvwKTNXBk1nbxdX3jVFJl88wiTpBbnwSqnjt/Ta8t\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/kibana.key\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDSTCCAjGgAwIBAgIUal1tFY90NA8IgvMVUs/jfyG3N00wDQYJKoZIhvcNAQEL\nBQAwNDEyMDAGA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5l\ncmF0ZWQgQ0EwHhcNMjEwMTE0MTQwODU2WhcNMjQwMTE0MTQwODU2WjA0MTIwMAYD\nVQQDEylFbGFzdGljIENlcnRpZmljYXRlIFRvb2wgQXV0b2dlbmVyYXRlZCBDQTCC\nASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKBXjJFIv4LL2DfqhUdPQGJl\nMmTgxIvqkDsoOxDYuTP27hY27wKUWFBO0sYkWxpddenGQxk66SzkU1XukvGw/Ygs\nezyuvRZq2UlOOt+2eIIjYmb6TIfhpDbMeVhYXfKiFtRHUYskd2VfSUktff8NEnC1\niWLPQaMdv81CmMOGkshjVYn4gaHD+b8Kv7sfnFn5WYMojWez1OOfWke+lJfw+sIa\ntOaZ+ufGZB50H63OZrUJJJa0QahlTakHJpXrk5x0mUq/E9P74FUFQ+tDqUPjLXQq\naPFbzwtSyiT0Rk8nMqu2TQm0kkz79wjR44MmXJo+qFAbMVY/fam+kLEpth0UL9UC\nAwEAAaNTMFEwHQYDVR0OBBYEFGILMWbvri92AW4ehKAe1uPVYJglMB8GA1UdIwQY\nMBaAFGILMWbvri92AW4ehKAe1uPVYJglMA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZI\nhvcNAQELBQADggEBAG4SEZ6beme+BG00ybv4YcFVEhUI24jgrpntyIOHC2ARkLXW\n1yl22Ue/eRg8fP9UhHH2YmUXt2Fy8b1HzihH38WAO/Wxx2K6u38C2ADuTppELWIU\nXUktUCOeYB2B5jbwFSExm6N2Rhj9YJsdlm/Lvph9s2VQThdKUZPOXqdi8u5C6L0k\ns2gkrKJdm4hF7NnVcgIzBPBY86sYzOMW1CXFP6o887KxKjPI7A1JAAkrPZz3ob0n\nB9pBayly5UAtixLJhQbkDfGAB1gRnDWaCDYmN+YT4LaQ/tsqueW8Ba1hQ1PqVLvw\n6J6ytch0M9sCIQe1PzLhEJRpfqtqeacr8yEUgkg=\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/ca.crt\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name              = \"kibana\"\n        port              = \"http\"\n        tags              = [ \"kibana${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Kibana Transport Check Live\"\n          port            = \"http\"\n          type            = \"tcp\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n        check {\n          name            = \"Elastic Kibana HTTP Check Live\"\n          type            = \"http\"\n          port            = \"http\"\n          protocol        = \"https\"\n          method          = \"GET\"\n          tls_skip_verify = true\n          path            = \"/\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu           = 1000\n        memory        = 8192\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        network {\n          port \"http\" {\n            static    = 5601\n          }\n        }\n      }\n    }\n  }\n}\n",
            "json": null,
            "modify_index": "7027981",
            "name": "prod-elastic",
            "namespace": "default",
            "policy_override": null,
            "purge_on_destroy": null,
            "region": "global",
            "task_groups": [
              {
                "count": 1,
                "meta": {},
                "name": "prod-group1-elastic-cluster",
                "task": [
                  {
                    "driver": "docker",
                    "meta": {},
                    "name": "prod-task1-elastic-cluster",
                    "volume_mounts": []
                  }
                ],
                "volumes": []
              },
              {
                "count": 1,
                "meta": {},
                "name": "prod-group1-elastic-kibana",
                "task": [
                  {
                    "driver": "docker",
                    "meta": {},
                    "name": "prod-task1-elastic-kibana",
                    "volume_mounts": []
                  }
                ],
                "volumes": []
              }
            ],
            "type": "service"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.elastic.data.template_file.nomad_job_elastic"
          ]
        }
      ]
    },
    {
      "module": "module.minio",
      "mode": "data",
      "type": "template_file",
      "name": "nomad_job_mc",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "ca0c95bbe91c4ac393d5cd5bef8efb90fb5f176f266ba233542fd4338b51c6cc",
            "rendered": "job \"prod-mc\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"batch\"\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-mc\" {\n    task \"prod-task1-create-buckets\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver        = \"docker\"\n\n      \n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image       = \"minio/mc:RELEASE.2020-12-10T01-26-17Z\"\n        entrypoint  = [\n          \"/bin/sh\",\n          \"-c\",\n          \"mc config host add LOCALMINIO http://storage.service.consul:9000 $MINIO_ACCESS_KEY $MINIO_SECRET_KEY \u0026\u0026 mc mb -p LOCALMINIO/logs.fd.io LOCALMINIO/docs.fd.io ; mc policy set public LOCALMINIO/logs.fd.io mc policy set public LOCALMINIO/docs.fd.io mc ilm add --expiry-days '180' LOCALMINIO/logs.fd.io mc admin user add LOCALMINIO storage Storage1234 mc admin policy set LOCALMINIO writeonly user=storage\"\n        ]\n        dns_servers  = [ \"${attr.unique.network.ip-address}\" ]\n        privileged   = false\n      }\n\n      # The env stanza configures a list of environment variables to populate\n      # the task's environment before starting.\n      env {\n        \n        MINIO_ACCESS_KEY = \"minio\"\n        MINIO_SECRET_KEY = \"minio123\"\n        \n        \n      }\n    }\n  }\n}\n",
            "template": "job \"${job_name}\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"${datacenters}\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"batch\"\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-mc\" {\n    task \"prod-task1-create-buckets\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver        = \"docker\"\n\n      %{ if use_vault_provider }\n      vault {\n        policies    = \"${vault_kv_policy_name}\"\n      }\n     %{ endif }\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image       = \"${image}\"\n        entrypoint  = [\n          \"/bin/sh\",\n          \"-c\",\n          \"${command}\"\n        ]\n        dns_servers  = [ \"$${attr.unique.network.ip-address}\" ]\n        privileged   = false\n      }\n\n      # The env stanza configures a list of environment variables to populate\n      # the task's environment before starting.\n      env {\n        %{ if use_vault_provider }\n        {{ with secret \"${vault_kv_path}\" }}\n        MINIO_ACCESS_KEY = \"{{ .Data.data.${vault_kv_field_access_key} }}\"\n        MINIO_SECRET_KEY = \"{{ .Data.data.${vault_kv_field_secret_key} }}\"\n        {{ end }}\n        %{ else }\n        MINIO_ACCESS_KEY = \"${access_key}\"\n        MINIO_SECRET_KEY = \"${secret_key}\"\n        %{ endif }\n        ${ envs }\n      }\n    }\n  }\n}\n",
            "vars": {
              "access_key": "minio",
              "command": "mc config host add LOCALMINIO http://storage.service.consul:9000 $MINIO_ACCESS_KEY $MINIO_SECRET_KEY \u0026\u0026 mc mb -p LOCALMINIO/logs.fd.io LOCALMINIO/docs.fd.io ; mc policy set public LOCALMINIO/logs.fd.io mc policy set public LOCALMINIO/docs.fd.io mc ilm add --expiry-days '180' LOCALMINIO/logs.fd.io mc admin user add LOCALMINIO storage Storage1234 mc admin policy set LOCALMINIO writeonly user=storage",
              "datacenters": "yul1",
              "envs": "",
              "image": "minio/mc:RELEASE.2020-12-10T01-26-17Z",
              "job_name": "prod-mc",
              "minio_port": "9000",
              "minio_service_name": "storage",
              "secret_key": "minio123",
              "service_name": "mc",
              "use_vault_provider": "false"
            }
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.minio",
      "mode": "data",
      "type": "template_file",
      "name": "nomad_job_minio",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "c38a0b7182d39c70ec07bdaa998f57a215d7193b264c1b1c1a299c84e7ca53de",
            "rendered": "job \"prod-minio\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 1\n\n    health_check      = \"checks\"\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 1\n\n    # Specifies if the job should auto-promote to the canary version when all\n    # canaries become healthy during a deployment. Defaults to false which means\n    # canaries must be manually updated with the nomad deployment promote\n    # command.\n    auto_promote      = true\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = true\n\n  }\n\n  # All groups in this job should be scheduled on different hosts.\n  constraint {\n    operator          = \"distinct_hosts\"\n    value             = \"true\"\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-minio\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count       = 4\n\n    # https://www.nomadproject.io/docs/job-specification/volume\n    \n    volume \"prod-volume1-minio\" {\n      type      = \"host\"\n      read_only = false\n      source    = \"prod-volume-data1-1\"\n    }\n    \n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-minio\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver             = \"docker\"\n\n    \n      volume_mount {\n        volume           = \"prod-volume1-minio\"\n        destination      = \"/data/\"\n        read_only        = false\n      }\n    \n\n    \n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image            = \"minio/minio:RELEASE.2020-12-03T05-49-24Z\"\n        dns_servers      = [ \"${attr.unique.network.ip-address}\" ]\n        network_mode     = \"host\"\n        command          = \"server\"\n        args             = [ \"http://10.32.8.1{4...7}:9000/data/\" ]\n        port_map {\n          http           = 9000\n        }\n        privileged       = false\n      }\n\n      # The env stanza configures a list of environment variables to populate\n      # the task's environment before starting.\n      env {\n\n        MINIO_ACCESS_KEY = \"minio\"\n        MINIO_SECRET_KEY = \"minio123\"\n\n        MINIO_BROWSER=\"off\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name       = \"storage\"\n        port       = \"http\"\n        tags       = [ \"storage${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name     = \"Min.io Server HTTP Check Live\"\n          type     = \"http\"\n          port     = \"http\"\n          protocol = \"http\"\n          method   = \"GET\"\n          path     = \"/minio/health/live\"\n          interval = \"10s\"\n          timeout  = \"2s\"\n        }\n        check {\n          name     = \"Min.io Server HTTP Check Ready\"\n          type     = \"http\"\n          port     = \"http\"\n          protocol = \"http\"\n          method   = \"GET\"\n          path     = \"/minio/health/ready\"\n          interval = \"10s\"\n          timeout  = \"2s\"\n        }\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu        = 40000\n        memory     = 40000\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        network {\n          port \"http\" {\n            static = 9000\n          }\n        }\n      }\n    }\n  }\n}\n",
            "template": "job \"${job_name}\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"${datacenters}\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 1\n\n    health_check      = \"checks\"\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n%{ if use_canary }\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 1\n\n    # Specifies if the job should auto-promote to the canary version when all\n    # canaries become healthy during a deployment. Defaults to false which means\n    # canaries must be manually updated with the nomad deployment promote\n    # command.\n    auto_promote      = true\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = true\n%{ endif }\n  }\n\n  # All groups in this job should be scheduled on different hosts.\n  constraint {\n    operator          = \"distinct_hosts\"\n    value             = \"true\"\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-minio\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count       = ${group_count}\n\n    # https://www.nomadproject.io/docs/job-specification/volume\n    %{ if use_host_volume }\n    volume \"prod-volume1-minio\" {\n      type      = \"host\"\n      read_only = false\n      source    = \"${host_volume}\"\n    }\n    %{ endif }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-minio\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver             = \"docker\"\n\n    %{ if use_host_volume }\n      volume_mount {\n        volume           = \"prod-volume1-minio\"\n        destination      = \"${data_dir}\"\n        read_only        = false\n      }\n    %{ endif }\n\n    %{ if use_vault_provider }\n      vault {\n        policies         = \"${vault_kv_policy_name}\"\n      }\n    %{ endif }\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image            = \"${image}\"\n        dns_servers      = [ \"$${attr.unique.network.ip-address}\" ]\n        network_mode     = \"host\"\n        command          = \"server\"\n        args             = [ \"${host}:${port}${data_dir}\" ]\n        port_map {\n          http           = ${port}\n        }\n        privileged       = false\n      }\n\n      # The env stanza configures a list of environment variables to populate\n      # the task's environment before starting.\n      env {\n%{ if use_vault_provider }\n{{ with secret \"${vault_kv_path}\" }}\n        MINIO_ACCESS_KEY = \"{{ .Data.data.${vault_kv_field_access_key} }}\"\n        MINIO_SECRET_KEY = \"{{ .Data.data.${vault_kv_field_secret_key} }}\"\n{{ end }}\n%{ else }\n        MINIO_ACCESS_KEY = \"${access_key}\"\n        MINIO_SECRET_KEY = \"${secret_key}\"\n%{ endif }\n        ${ envs }\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name       = \"${service_name}\"\n        port       = \"http\"\n        tags       = [ \"storage$${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name     = \"Min.io Server HTTP Check Live\"\n          type     = \"http\"\n          port     = \"http\"\n          protocol = \"http\"\n          method   = \"GET\"\n          path     = \"/minio/health/live\"\n          interval = \"10s\"\n          timeout  = \"2s\"\n        }\n        check {\n          name     = \"Min.io Server HTTP Check Ready\"\n          type     = \"http\"\n          port     = \"http\"\n          protocol = \"http\"\n          method   = \"GET\"\n          path     = \"/minio/health/ready\"\n          interval = \"10s\"\n          timeout  = \"2s\"\n        }\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu        = ${cpu}\n        memory     = ${memory}\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        network {\n          port \"http\" {\n            static = ${port}\n          }\n        }\n      }\n    }\n  }\n}\n",
            "vars": {
              "access_key": "minio",
              "cpu": "40000",
              "cpu_proxy": "200",
              "data_dir": "/data/",
              "datacenters": "yul1",
              "envs": "MINIO_BROWSER=\"off\"",
              "group_count": "4",
              "host": "http://10.32.8.1{4...7}",
              "host_volume": "prod-volume-data1-1",
              "image": "minio/minio:RELEASE.2020-12-03T05-49-24Z",
              "job_name": "prod-minio",
              "memory": "40000",
              "memory_proxy": "128",
              "port": "9000",
              "secret_key": "minio123",
              "service_name": "storage",
              "upstreams": "[]",
              "use_canary": "true",
              "use_host_volume": "true",
              "use_vault_provider": "false"
            }
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.minio",
      "mode": "managed",
      "type": "nomad_job",
      "name": "nomad_job_mc",
      "provider": "provider[\"registry.terraform.io/hashicorp/nomad\"].yul1",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "allocation_ids": [
              "a8597890-73df-b6a5-e0b5-a0d146fe18f6"
            ],
            "datacenters": [
              "yul1"
            ],
            "deployment_id": "",
            "deployment_status": "",
            "deregister_on_destroy": true,
            "deregister_on_id_change": true,
            "detach": false,
            "id": "prod-mc",
            "jobspec": "job \"prod-mc\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"batch\"\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-mc\" {\n    task \"prod-task1-create-buckets\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver        = \"docker\"\n\n      \n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image       = \"minio/mc:RELEASE.2020-12-10T01-26-17Z\"\n        entrypoint  = [\n          \"/bin/sh\",\n          \"-c\",\n          \"mc config host add LOCALMINIO http://storage.service.consul:9000 $MINIO_ACCESS_KEY $MINIO_SECRET_KEY \u0026\u0026 mc mb -p LOCALMINIO/logs.fd.io LOCALMINIO/docs.fd.io ; mc policy set public LOCALMINIO/logs.fd.io mc policy set public LOCALMINIO/docs.fd.io mc ilm add --expiry-days '180' LOCALMINIO/logs.fd.io mc admin user add LOCALMINIO storage Storage1234 mc admin policy set LOCALMINIO writeonly user=storage\"\n        ]\n        dns_servers  = [ \"${attr.unique.network.ip-address}\" ]\n        privileged   = false\n      }\n\n      # The env stanza configures a list of environment variables to populate\n      # the task's environment before starting.\n      env {\n        \n        MINIO_ACCESS_KEY = \"minio\"\n        MINIO_SECRET_KEY = \"minio123\"\n        \n        \n      }\n    }\n  }\n}\n",
            "json": null,
            "modify_index": "7077750",
            "name": "prod-mc",
            "namespace": "default",
            "policy_override": null,
            "purge_on_destroy": null,
            "region": "global",
            "task_groups": [
              {
                "count": 1,
                "meta": {},
                "name": "prod-group1-mc",
                "task": [
                  {
                    "driver": "docker",
                    "meta": {},
                    "name": "prod-task1-create-buckets",
                    "volume_mounts": null
                  }
                ],
                "volumes": null
              }
            ],
            "type": "batch"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.minio.data.template_file.nomad_job_mc",
            "module.minio.data.template_file.nomad_job_minio",
            "module.minio.nomad_job.nomad_job_minio"
          ]
        }
      ]
    },
    {
      "module": "module.minio",
      "mode": "managed",
      "type": "nomad_job",
      "name": "nomad_job_minio",
      "provider": "provider[\"registry.terraform.io/hashicorp/nomad\"].yul1",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "allocation_ids": [
              "322e6ea5-acc7-2f37-3022-cd0a31c2f3db",
              "619833b9-dd70-46ae-f576-75c3cd058a13",
              "0510561a-c828-dd23-5910-b92d9f2c41ef",
              "504924c8-387b-abe2-c4d5-d6a7424a4689"
            ],
            "datacenters": [
              "yul1"
            ],
            "deployment_id": "cd6d79cf-6e6f-289d-a0bb-6b2bb8da769a",
            "deployment_status": "successful",
            "deregister_on_destroy": true,
            "deregister_on_id_change": true,
            "detach": false,
            "id": "prod-minio",
            "jobspec": "job \"prod-minio\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 1\n\n    health_check      = \"checks\"\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 1\n\n    # Specifies if the job should auto-promote to the canary version when all\n    # canaries become healthy during a deployment. Defaults to false which means\n    # canaries must be manually updated with the nomad deployment promote\n    # command.\n    auto_promote      = true\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = true\n\n  }\n\n  # All groups in this job should be scheduled on different hosts.\n  constraint {\n    operator          = \"distinct_hosts\"\n    value             = \"true\"\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-minio\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count       = 4\n\n    # https://www.nomadproject.io/docs/job-specification/volume\n    \n    volume \"prod-volume1-minio\" {\n      type      = \"host\"\n      read_only = false\n      source    = \"prod-volume-data1-1\"\n    }\n    \n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-minio\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver             = \"docker\"\n\n    \n      volume_mount {\n        volume           = \"prod-volume1-minio\"\n        destination      = \"/data/\"\n        read_only        = false\n      }\n    \n\n    \n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image            = \"minio/minio:RELEASE.2020-12-03T05-49-24Z\"\n        dns_servers      = [ \"${attr.unique.network.ip-address}\" ]\n        network_mode     = \"host\"\n        command          = \"server\"\n        args             = [ \"http://10.32.8.1{4...7}:9000/data/\" ]\n        port_map {\n          http           = 9000\n        }\n        privileged       = false\n      }\n\n      # The env stanza configures a list of environment variables to populate\n      # the task's environment before starting.\n      env {\n\n        MINIO_ACCESS_KEY = \"minio\"\n        MINIO_SECRET_KEY = \"minio123\"\n\n        MINIO_BROWSER=\"off\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name       = \"storage\"\n        port       = \"http\"\n        tags       = [ \"storage${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name     = \"Min.io Server HTTP Check Live\"\n          type     = \"http\"\n          port     = \"http\"\n          protocol = \"http\"\n          method   = \"GET\"\n          path     = \"/minio/health/live\"\n          interval = \"10s\"\n          timeout  = \"2s\"\n        }\n        check {\n          name     = \"Min.io Server HTTP Check Ready\"\n          type     = \"http\"\n          port     = \"http\"\n          protocol = \"http\"\n          method   = \"GET\"\n          path     = \"/minio/health/ready\"\n          interval = \"10s\"\n          timeout  = \"2s\"\n        }\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        cpu      = 40000\n        memory   = 40000\n        network {\n          port \"http\" {\n            static = 9000\n          }\n        }\n      }\n    }\n  }\n}\n",
            "json": null,
            "modify_index": "5933603",
            "name": "prod-minio",
            "namespace": "default",
            "policy_override": null,
            "purge_on_destroy": null,
            "region": "global",
            "task_groups": [
              {
                "count": 4,
                "meta": {},
                "name": "prod-group1-minio",
                "task": [
                  {
                    "driver": "docker",
                    "meta": {},
                    "name": "prod-task1-minio",
                    "volume_mounts": [
                      {
                        "destination": "/data/",
                        "read_only": false,
                        "volume": "prod-volume1-minio"
                      }
                    ]
                  }
                ],
                "volumes": [
                  {
                    "name": "prod-volume1-minio",
                    "read_only": false,
                    "source": "prod-volume-data1-1",
                    "type": "host"
                  }
                ]
              }
            ],
            "type": "service"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.minio.data.template_file.nomad_job_minio"
          ]
        }
      ]
    },
    {
      "module": "module.nginx",
      "mode": "data",
      "type": "template_file",
      "name": "nomad_job_nginx",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "2816603491214d9abe10981b9f9cc6a4cb806acbc6981b952653847fec86e9ff",
            "rendered": "job \"prod-nginx\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 0\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = false\n\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 0\n  }\n\n  # The reschedule stanza specifies the group's rescheduling strategy. If\n  # specified at the job level, the configuration will apply to all groups\n  # within the job. If the reschedule stanza is present on both the job and the\n  # group, they are merged with the group stanza taking the highest precedence\n  # and then the job.\n  reschedule {\n    delay             = \"30s\"\n    delay_function    = \"constant\"\n    unlimited         = true\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-nginx\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count = 1\n\n    # The restart stanza configures a tasks's behavior on task failure. Restarts\n    # happen on the client that is running the task.\n    restart {\n      interval  = \"10m\"\n      attempts  = 2\n      delay     = \"15s\"\n      mode      = \"fail\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-nginx\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"nginx:stable\"\n        dns_servers  = [ \"${attr.unique.network.ip-address}\" ]\n        port_map {\n          https      = 443\n        }\n        privileged   = false\n        volumes      = [\n          \"/etc/consul.d/ssl/consul.pem:/etc/ssl/certs/nginx-cert.pem\",\n          \"/etc/consul.d/ssl/consul-key.pem:/etc/ssl/private/nginx-key.pem\",\n          \"custom/upstream.conf:/etc/nginx/conf.d/upstream.conf\",\n          \"custom/logs.conf:/etc/nginx/conf.d/logs.conf\",\n          \"custom/docs.conf:/etc/nginx/conf.d/docs.conf\"\n        ]\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data = \u003c\u003cEOH\n          upstream storage {\n            server storage0.storage.service.consul:9000;\n            server storage1.storage.service.consul:9000;\n            server storage2.storage.service.consul:9000;\n            server storage3.storage.service.consul:9000;\n          }\n        EOH\n        destination = \"custom/upstream.conf\"\n      }\n      template {\n        data = \u003c\u003cEOH\n          server {\n            listen 443 ssl default_server;\n            server_name logs.nginx.service.consul;\n            keepalive_timeout 70;\n            ssl_session_cache shared:SSL:10m;\n            ssl_session_timeout 10m;\n            ssl_protocols TLSv1.2;\n            ssl_prefer_server_ciphers on;\n            ssl_ciphers \"ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384\";\n            ssl_certificate /etc/ssl/certs/nginx-cert.pem;\n            ssl_certificate_key /etc/ssl/private/nginx-key.pem;\n            location / {\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/;\n              server_name_in_redirect off;\n            }\n            location ~ (.*html.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type text/html;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n            location ~ (.*txt.gz|.*log.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type text/plain;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n            location ~ (.*xml.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type application/xml;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n        }\n        EOH\n        destination = \"custom/logs.conf\"\n      }\n      template {\n        data = \u003c\u003cEOH\n          server {\n            listen 443 ssl;\n            server_name docs.nginx.service.consul;\n            keepalive_timeout 70;\n            ssl_session_cache shared:SSL:10m;\n            ssl_session_timeout 10m;\n            ssl_protocols TLSv1.2;\n            ssl_prefer_server_ciphers on;\n            ssl_ciphers \"ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384\";\n            ssl_certificate /etc/ssl/certs/nginx-cert.pem;\n            ssl_certificate_key /etc/ssl/private/nginx-key.pem;\n            location / {\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/docs.fd.io/;\n              server_name_in_redirect off;\n            }\n          }\n        EOH\n        destination = \"custom/docs.conf\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name       = \"nginx\"\n        port       = \"https\"\n        tags       = [ \"docs\", \"logs\" ]\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu        = 1000\n        memory     = 1024\n        network {\n          mode     = \"bridge\"\n          port \"https\" {\n            static = 443\n          }\n        }\n      }\n    }\n  }\n}",
            "template": "job \"${job_name}\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"${datacenters}\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 0\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = false\n\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 0\n  }\n\n  # The reschedule stanza specifies the group's rescheduling strategy. If\n  # specified at the job level, the configuration will apply to all groups\n  # within the job. If the reschedule stanza is present on both the job and the\n  # group, they are merged with the group stanza taking the highest precedence\n  # and then the job.\n  reschedule {\n    delay             = \"30s\"\n    delay_function    = \"constant\"\n    unlimited         = true\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-nginx\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count = 1\n\n    # The restart stanza configures a tasks's behavior on task failure. Restarts\n    # happen on the client that is running the task.\n    restart {\n      interval  = \"10m\"\n      attempts  = 2\n      delay     = \"15s\"\n      mode      = \"fail\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-nginx\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"nginx:stable\"\n        dns_servers  = [ \"$${attr.unique.network.ip-address}\" ]\n        port_map {\n          https      = 443\n        }\n        privileged   = false\n        volumes      = [\n          \"/etc/consul.d/ssl/consul.pem:/etc/ssl/certs/nginx-cert.pem\",\n          \"/etc/consul.d/ssl/consul-key.pem:/etc/ssl/private/nginx-key.pem\",\n          \"custom/upstream.conf:/etc/nginx/conf.d/upstream.conf\",\n          \"custom/logs.conf:/etc/nginx/conf.d/logs.conf\",\n          \"custom/docs.conf:/etc/nginx/conf.d/docs.conf\"\n        ]\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data = \u003c\u003cEOH\n          upstream storage {\n            server storage0.storage.service.consul:9000;\n            server storage1.storage.service.consul:9000;\n            server storage2.storage.service.consul:9000;\n            server storage3.storage.service.consul:9000;\n          }\n        EOH\n        destination = \"custom/upstream.conf\"\n      }\n      template {\n        data = \u003c\u003cEOH\n          server {\n            listen 443 ssl default_server;\n            server_name logs.nginx.service.consul;\n            keepalive_timeout 70;\n            ssl_session_cache shared:SSL:10m;\n            ssl_session_timeout 10m;\n            ssl_protocols TLSv1.2;\n            ssl_prefer_server_ciphers on;\n            ssl_ciphers \"ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384\";\n            ssl_certificate /etc/ssl/certs/nginx-cert.pem;\n            ssl_certificate_key /etc/ssl/private/nginx-key.pem;\n            location / {\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/;\n              server_name_in_redirect off;\n            }\n            location ~ (.*html.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type text/html;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n            location ~ (.*txt.gz|.*log.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type text/plain;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n            location ~ (.*xml.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type application/xml;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n        }\n        EOH\n        destination = \"custom/logs.conf\"\n      }\n      template {\n        data = \u003c\u003cEOH\n          server {\n            listen 443 ssl;\n            server_name docs.nginx.service.consul;\n            keepalive_timeout 70;\n            ssl_session_cache shared:SSL:10m;\n            ssl_session_timeout 10m;\n            ssl_protocols TLSv1.2;\n            ssl_prefer_server_ciphers on;\n            ssl_ciphers \"ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384\";\n            ssl_certificate /etc/ssl/certs/nginx-cert.pem;\n            ssl_certificate_key /etc/ssl/private/nginx-key.pem;\n            location / {\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/docs.fd.io/;\n              server_name_in_redirect off;\n            }\n          }\n        EOH\n        destination = \"custom/docs.conf\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name       = \"nginx\"\n        port       = \"https\"\n        tags       = [ \"docs\", \"logs\" ]\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu        = 1000\n        memory     = 1024\n        network {\n          mode     = \"bridge\"\n          port \"https\" {\n            static = 443\n          }\n        }\n      }\n    }\n  }\n}",
            "vars": {
              "datacenters": "yul1",
              "job_name": "prod-nginx"
            }
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.nginx",
      "mode": "managed",
      "type": "nomad_job",
      "name": "nomad_job_nginx",
      "provider": "provider[\"registry.terraform.io/hashicorp/nomad\"].yul1",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "allocation_ids": [
              "de9af589-8f0b-6634-7199-c661f8f17df0"
            ],
            "datacenters": [
              "yul1"
            ],
            "deployment_id": "",
            "deployment_status": "",
            "deregister_on_destroy": true,
            "deregister_on_id_change": true,
            "detach": false,
            "id": "prod-nginx",
            "jobspec": "job \"prod-nginx\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 0\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = false\n\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 0\n  }\n\n  # The reschedule stanza specifies the group's rescheduling strategy. If\n  # specified at the job level, the configuration will apply to all groups\n  # within the job. If the reschedule stanza is present on both the job and the\n  # group, they are merged with the group stanza taking the highest precedence\n  # and then the job.\n  reschedule {\n    delay             = \"30s\"\n    delay_function    = \"constant\"\n    unlimited         = true\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-nginx\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count = 1\n\n    # The restart stanza configures a tasks's behavior on task failure. Restarts\n    # happen on the client that is running the task.\n    restart {\n      interval  = \"10m\"\n      attempts  = 2\n      delay     = \"15s\"\n      mode      = \"fail\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-nginx\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"nginx:stable\"\n        dns_servers  = [ \"${attr.unique.network.ip-address}\" ]\n        port_map {\n          https      = 443\n        }\n        privileged   = false\n        volumes      = [\n          \"/etc/consul.d/ssl/consul.pem:/etc/ssl/certs/nginx-cert.pem\",\n          \"/etc/consul.d/ssl/consul-key.pem:/etc/ssl/private/nginx-key.pem\",\n          \"custom/upstream.conf:/etc/nginx/conf.d/upstream.conf\",\n          \"custom/logs.conf:/etc/nginx/conf.d/logs.conf\",\n          \"custom/docs.conf:/etc/nginx/conf.d/docs.conf\"\n        ]\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data = \u003c\u003cEOH\n          upstream storage {\n            server storage0.storage.service.consul:9000;\n            server storage1.storage.service.consul:9000;\n            server storage2.storage.service.consul:9000;\n            server storage3.storage.service.consul:9000;\n          }\n        EOH\n        destination = \"custom/upstream.conf\"\n      }\n      template {\n        data = \u003c\u003cEOH\n          server {\n            listen 443 ssl default_server;\n            server_name logs.nginx.service.consul;\n            keepalive_timeout 70;\n            ssl_session_cache shared:SSL:10m;\n            ssl_session_timeout 10m;\n            ssl_protocols TLSv1.2;\n            ssl_prefer_server_ciphers on;\n            ssl_ciphers \"ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384\";\n            ssl_certificate /etc/ssl/certs/nginx-cert.pem;\n            ssl_certificate_key /etc/ssl/private/nginx-key.pem;\n            location / {\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/;\n              server_name_in_redirect off;\n            }\n            location ~ (.*html.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type text/html;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n            location ~ (.*txt.gz|.*log.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type text/plain;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n            location ~ (.*xml.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type application/xml;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n        }\n        EOH\n        destination = \"custom/logs.conf\"\n      }\n      template {\n        data = \u003c\u003cEOH\n          server {\n            listen 443 ssl;\n            server_name docs.nginx.service.consul;\n            keepalive_timeout 70;\n            ssl_session_cache shared:SSL:10m;\n            ssl_session_timeout 10m;\n            ssl_protocols TLSv1.2;\n            ssl_prefer_server_ciphers on;\n            ssl_ciphers \"ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384\";\n            ssl_certificate /etc/ssl/certs/nginx-cert.pem;\n            ssl_certificate_key /etc/ssl/private/nginx-key.pem;\n            location / {\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/docs.fd.io/;\n              server_name_in_redirect off;\n            }\n          }\n        EOH\n        destination = \"custom/docs.conf\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name       = \"nginx\"\n        port       = \"https\"\n        tags       = [ \"docs\", \"logs\" ]\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu        = 1000\n        memory     = 1024\n        network {\n          mode     = \"bridge\"\n          port \"https\" {\n            static = 443\n          }\n        }\n      }\n    }\n  }\n}",
            "json": null,
            "modify_index": "5922474",
            "name": "prod-nginx",
            "namespace": "default",
            "policy_override": null,
            "purge_on_destroy": null,
            "region": "global",
            "task_groups": [
              {
                "count": 1,
                "meta": {},
                "name": "prod-group1-nginx",
                "task": [
                  {
                    "driver": "docker",
                    "meta": {},
                    "name": "prod-task1-nginx",
                    "volume_mounts": []
                  }
                ],
                "volumes": []
              }
            ],
            "type": "service"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.nginx.data.template_file.nomad_job_nginx"
          ]
        }
      ]
    },
    {
      "module": "module.vpp_device",
      "mode": "data",
      "type": "template_file",
      "name": "nomad_job_csit_shim",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "a285223159ce0af9e5427d857021e5651c09408af90decd3b93a90efd4fd7ce8",
            "rendered": "job \"prod-device-csit-shim\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"system\"\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-csit-shim-amd\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count            = 1\n\n    constraint {\n      attribute      = \"${node.class}\"\n      value          = \"csit\"\n    }\n\n    restart {\n      interval       = \"1m\"\n      attempts       = 3\n      delay          = \"15s\"\n      mode           = \"delay\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-csit-shim-amd\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver         = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"csit_shim-ubuntu1804:local\"\n        network_mode = \"host\"\n        pid_mode     = \"host\"\n        volumes      = [\n          \"/var/run/docker.sock:/var/run/docker.sock\"\n        ]\n        privileged   = true\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu          = 1000\n        memory       = 5000\n        network {\n          port \"ssh\" {\n              static = 6022\n          }\n          port \"ssh2\" {\n              static = 6023\n          }\n        }\n      }\n    }\n  }\n\n  group \"prod-group1-csit-shim-arm\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count            = 1\n\n    constraint {\n      attribute      = \"${node.class}\"\n      value          = \"csitarm\"\n    }\n\n    restart {\n      interval       = \"1m\"\n      attempts       = 3\n      delay          = \"15s\"\n      mode           = \"delay\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-csit-shim-arm\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver         = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"csit_shim-ubuntu1804:local\"\n        network_mode = \"host\"\n        pid_mode     = \"host\"\n        volumes      = [\n          \"/var/run/docker.sock:/var/run/docker.sock\"\n        ]\n        privileged   = true\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu          = 1000\n        memory       = 5000\n        network {\n          port \"ssh\" {\n              static = 6022\n          }\n          port \"ssh2\" {\n              static = 6023\n          }\n        }\n      }\n    }\n  }\n}",
            "template": "job \"${job_name}\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"${datacenters}\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"system\"\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-csit-shim-amd\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count            = ${group_count}\n\n    constraint {\n      attribute      = \"$${node.class}\"\n      value          = \"csit\"\n    }\n\n    restart {\n      interval       = \"1m\"\n      attempts       = 3\n      delay          = \"15s\"\n      mode           = \"delay\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-csit-shim-amd\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver         = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"csit_shim-ubuntu1804:local\"\n        network_mode = \"host\"\n        pid_mode     = \"host\"\n        volumes      = [\n          \"/var/run/docker.sock:/var/run/docker.sock\"\n        ]\n        privileged   = true\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu          = ${cpu}\n        memory       = ${mem}\n        network {\n          port \"ssh\" {\n              static = 6022\n          }\n          port \"ssh2\" {\n              static = 6023\n          }\n        }\n      }\n    }\n  }\n\n  group \"prod-group1-csit-shim-arm\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count            = ${group_count}\n\n    constraint {\n      attribute      = \"$${node.class}\"\n      value          = \"csitarm\"\n    }\n\n    restart {\n      interval       = \"1m\"\n      attempts       = 3\n      delay          = \"15s\"\n      mode           = \"delay\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-csit-shim-arm\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver         = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"csit_shim-ubuntu1804:local\"\n        network_mode = \"host\"\n        pid_mode     = \"host\"\n        volumes      = [\n          \"/var/run/docker.sock:/var/run/docker.sock\"\n        ]\n        privileged   = true\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu          = ${cpu}\n        memory       = ${mem}\n        network {\n          port \"ssh\" {\n              static = 6022\n          }\n          port \"ssh2\" {\n              static = 6023\n          }\n        }\n      }\n    }\n  }\n}",
            "vars": {
              "cpu": "1000",
              "datacenters": "yul1",
              "group_count": "1",
              "job_name": "prod-device-csit-shim",
              "mem": "5000"
            }
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.vpp_device",
      "mode": "managed",
      "type": "nomad_job",
      "name": "nomad_job_csit_shim",
      "provider": "provider[\"registry.terraform.io/hashicorp/nomad\"].yul1",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "allocation_ids": [
              "190f5699-0d10-7f85-ac68-96927702070b",
              "2fbbef45-f00a-1367-08c4-c2239de9e78d",
              "dd57bc83-2229-d242-1caf-b743f186e7a2"
            ],
            "datacenters": [
              "yul1"
            ],
            "deployment_id": "",
            "deployment_status": "",
            "deregister_on_destroy": true,
            "deregister_on_id_change": true,
            "detach": false,
            "id": "prod-device-csit-shim",
            "jobspec": "job \"prod-device-csit-shim\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"system\"\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-csit-shim-amd\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count            = 1\n\n    constraint {\n      attribute      = \"${node.class}\"\n      value          = \"csit\"\n    }\n\n    restart {\n      interval       = \"1m\"\n      attempts       = 3\n      delay          = \"15s\"\n      mode           = \"delay\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-csit-shim-amd\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver         = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"csit_shim-ubuntu1804:local\"\n        network_mode = \"host\"\n        pid_mode     = \"host\"\n        volumes      = [\n          \"/var/run/docker.sock:/var/run/docker.sock\"\n        ]\n        privileged   = true\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu          = 1000\n        memory       = 5000\n        network {\n          port \"ssh\" {\n              static = 6022\n          }\n          port \"ssh2\" {\n              static = 6023\n          }\n        }\n      }\n    }\n  }\n\n  group \"prod-group1-csit-shim-arm\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count            = 1\n\n    constraint {\n      attribute      = \"${node.class}\"\n      value          = \"csitarm\"\n    }\n\n    restart {\n      interval       = \"1m\"\n      attempts       = 3\n      delay          = \"15s\"\n      mode           = \"delay\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-csit-shim-arm\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver         = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"csit_shim-ubuntu1804:local\"\n        network_mode = \"host\"\n        pid_mode     = \"host\"\n        volumes      = [\n          \"/var/run/docker.sock:/var/run/docker.sock\"\n        ]\n        privileged   = true\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu          = 1000\n        memory       = 5000\n        network {\n          port \"ssh\" {\n              static = 6022\n          }\n          port \"ssh2\" {\n              static = 6023\n          }\n        }\n      }\n    }\n  }\n}",
            "json": null,
            "modify_index": "7077736",
            "name": "prod-device-csit-shim",
            "namespace": "default",
            "policy_override": null,
            "purge_on_destroy": null,
            "region": "global",
            "task_groups": [
              {
                "count": 1,
                "meta": {},
                "name": "prod-group1-csit-shim-amd",
                "task": [
                  {
                    "driver": "docker",
                    "meta": {},
                    "name": "prod-task1-csit-shim-amd",
                    "volume_mounts": []
                  }
                ],
                "volumes": []
              },
              {
                "count": 1,
                "meta": {},
                "name": "prod-group1-csit-shim-arm",
                "task": [
                  {
                    "driver": "docker",
                    "meta": {},
                    "name": "prod-task1-csit-shim-arm",
                    "volume_mounts": []
                  }
                ],
                "volumes": []
              }
            ],
            "type": "system"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.vpp_device.data.template_file.nomad_job_csit_shim"
          ]
        }
      ]
    }
  ]
}
