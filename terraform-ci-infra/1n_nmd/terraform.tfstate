{
  "version": 4,
  "terraform_version": "0.14.4",
  "serial": 378,
  "lineage": "e4e7f30a-652d-7a31-e31c-5e3a3388c9b9",
  "outputs": {},
  "resources": [
    {
      "module": "module.elastic",
      "mode": "data",
      "type": "template_file",
      "name": "nomad_job_beats",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "8727dcfa93e4d7c9e550a8b9bf30b8959f95d62921ee0a12be3fc6062a50f068",
            "rendered": "job \"prod-beats\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. For a full list of job types and their differences,\n  # please see the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"system\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 1\n\n    health_check      = \"checks\"\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 1\n\n    # Specifies if the job should auto-promote to the canary version when all\n    # canaries become healthy during a deployment. Defaults to false which means\n    # canaries must be manually updated with the nomad deployment promote\n    # command.\n    auto_promote      = true\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = true\n\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-beats\" {\n\n    restart {\n      interval        = \"1m\"\n      attempts        = 3\n      delay           = \"15s\"\n      mode            = \"delay\"\n    }\n\n    task \"prod-task1-filebeat\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver          = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"docker.elastic.co/beats/filebeat:7.10.1\"\n        dns_servers  = [ \"${attr.unique.network.ip-address}\" ]\n        privileged   = true\n        volumes      = [\n          \"/var/lib/docker/containers:/var/lib/docker/containers\",\n          \"/var/run/docker.sock:/var/run/docker.sock\",\n          \"local/filebeat.yml:/usr/share/filebeat/filebeat.yml\"\n        ]\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data          = \u003c\u003cEOF\nfilebeat.inputs:\n- type: log\n  enabled: true\n  paths:\n    - /var/log/syslog\n    - /var/log/*.log\n- type: container\n  enabled: false\n  paths:\n    - /var/lib/docker/containers/*/*.log\n  stream: all\noutput.elasticsearch:\n  enabled: true\n  hosts: [\"elastic-rest.service.consul:9200\"]\n  compression_level: 0\n  escape_html: true\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  worker: 1\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nsetup.kibana:\n  host: \"kibana.service.consul:5601\"\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  #path: \"\"\n  #space.id: \"\"\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nEOF\n        destination  = \"local/filebeat.yml\"\n      }\n    }\n\n    task \"prod-task1-metricbeat\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver         = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"docker.elastic.co/beats/metricbeat:7.10.1\"\n        dns_servers  = [ \"${attr.unique.network.ip-address}\" ]\n        privileged   = true\n#        command      = \"metricbeat\"\n        args = [\n          \"-e\", \"-system.hostfs=/hostfs\"\n        ]\n        volumes      = [\n          \"/:/hostfs:ro\",\n          \"local/metricbeat.yml:/usr/share/metricbeat/metricbeat.yml\"\n        ]\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\nmetricbeat.config.modules:\n  path: ${path.config}/modules.d/*.yml\n  reload.period: 10s\n  reload.enabled: false\nmetricbeat.max_start_delay: 10s\nmetricbeat.modules:\n- module: system\n  metricsets:\n    - cpu\n    - load\n    - memory\n    - network\n    - process\n    - process_summary\n    - uptime\n    - socket_summary\n    - core\n    - diskio\n    #- filesystem\n    - fsstat\n    #- raid\n    #- socket\n    - service\n  enabled: true\n  period: 10s\n  processes: ['.*']\n  cpu.metrics:  [\"percentages\",\"normalized_percentages\"]\n  core.metrics: [\"percentages\"]\n- module: consul\n  metricsets:\n  - agent\n  enabled: true\n  period: 10s\n  hosts: [\"localhost:8500\"]\n- module: docker\n  metricsets:\n    - \"container\"\n    - \"cpu\"\n    - \"diskio\"\n    - \"event\"\n    - \"healthcheck\"\n    - \"info\"\n    - \"image\"\n    - \"memory\"\n    - \"network\"\n  hosts: [\"unix:///var/run/docker.sock\"]\n  period: 10s\n  enabled: true\noutput.elasticsearch:\n  enabled: true\n  hosts: [\"elastic-rest.service.consul:9200\"]\n  compression_level: 0\n  escape_html: true\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  worker: 1\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nsetup.kibana:\n  host: \"elastic-kibana.service.consul:5601\"\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  #path: \"\"\n  #space.id: \"\"\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nEOF\n        destination  = \"local/metricbeat.yml\"\n      }\n    }\n\n    task \"prod-task1-packetbeat\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver         = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"docker.elastic.co/beats/packetbeat:7.10.1\"\n        dns_servers  = [ \"${attr.unique.network.ip-address}\" ]\n        privileged   = true\n\n        volumes      = [\n          \"local/packetbeat.yml:/usr/share/packetbeat/packetbeat.yml\"\n        ]\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\npacketbeat.interfaces.device: any\npacketbeat.flows:\n  enabled: true\n  timeout: 30s\n  period: 10s\npacketbeat.protocols:\n- type: icmp\n  enabled: true\n- type: dhcpv4\n  enabled: true\n  ports: [67, 68]\n- type: dns\n  enabled: true\n  ports: [53]\n  include_authorities: true\n  include_additionals: true\n- type: http\n  enabled: true\n  ports: [80, 8080, 8000, 5000, 8002]\n- type: tls\n  enabled: true\n  ports: [443, 8443, 9243]\npacketbeat.procs.enabled: false\npacketbeat.ignore_outgoing: false\noutput.elasticsearch:\n  enabled: true\n  hosts: [\"elastic-rest.service.consul:9200\"]\n  compression_level: 0\n  escape_html: true\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  worker: 1\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nsetup.kibana:\n  host: \"kibana.service.consul:5601\"\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  #path: \"\"\n  #space.id: \"\"\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nEOF\n        destination  = \"local/packetbeat.yml\"\n      }\n    }\n  }\n}",
            "template": "job \"${job_name}\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"${datacenters}\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. For a full list of job types and their differences,\n  # please see the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"system\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 1\n\n    health_check      = \"checks\"\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n%{ if use_canary }\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 1\n\n    # Specifies if the job should auto-promote to the canary version when all\n    # canaries become healthy during a deployment. Defaults to false which means\n    # canaries must be manually updated with the nomad deployment promote\n    # command.\n    auto_promote      = true\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = true\n%{ endif }\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-beats\" {\n\n    restart {\n      interval        = \"1m\"\n      attempts        = 3\n      delay           = \"15s\"\n      mode            = \"delay\"\n    }\n\n    task \"prod-task1-filebeat\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver          = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"docker.elastic.co/beats/filebeat:${version}\"\n        dns_servers  = [ \"$${attr.unique.network.ip-address}\" ]\n        privileged   = true\n        volumes      = [\n          \"/var/lib/docker/containers:/var/lib/docker/containers\",\n          \"/var/run/docker.sock:/var/run/docker.sock\",\n          \"local/filebeat.yml:/usr/share/filebeat/filebeat.yml\"\n        ]\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data          = \u003c\u003cEOF\nfilebeat.inputs:\n- type: log\n  enabled: true\n  paths:\n    - /var/log/syslog\n    - /var/log/*.log\n- type: container\n  enabled: false\n  paths:\n    - /var/lib/docker/containers/*/*.log\n  stream: all\noutput.elasticsearch:\n  enabled: true\n  hosts: [\"elastic-rest.service.consul:9200\"]\n  compression_level: 0\n  escape_html: true\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  worker: 1\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nsetup.kibana:\n  host: \"kibana.service.consul:5601\"\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  #path: \"\"\n  #space.id: \"\"\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nEOF\n        destination  = \"local/filebeat.yml\"\n      }\n    }\n\n    task \"prod-task1-metricbeat\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver         = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"docker.elastic.co/beats/metricbeat:${version}\"\n        dns_servers  = [ \"$${attr.unique.network.ip-address}\" ]\n        privileged   = true\n#        command      = \"metricbeat\"\n        args = [\n          \"-e\", \"-system.hostfs=/hostfs\"\n        ]\n        volumes      = [\n          \"/:/hostfs:ro\",\n          \"local/metricbeat.yml:/usr/share/metricbeat/metricbeat.yml\"\n        ]\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\nmetricbeat.config.modules:\n  path: $${path.config}/modules.d/*.yml\n  reload.period: 10s\n  reload.enabled: false\nmetricbeat.max_start_delay: 10s\nmetricbeat.modules:\n- module: system\n  metricsets:\n    - cpu\n    - load\n    - memory\n    - network\n    - process\n    - process_summary\n    - uptime\n    - socket_summary\n    - core\n    - diskio\n    #- filesystem\n    - fsstat\n    #- raid\n    #- socket\n    - service\n  enabled: true\n  period: 10s\n  processes: ['.*']\n  cpu.metrics:  [\"percentages\",\"normalized_percentages\"]\n  core.metrics: [\"percentages\"]\n- module: consul\n  metricsets:\n  - agent\n  enabled: true\n  period: 10s\n  hosts: [\"localhost:8500\"]\n- module: docker\n  metricsets:\n    - \"container\"\n    - \"cpu\"\n    - \"diskio\"\n    - \"event\"\n    - \"healthcheck\"\n    - \"info\"\n    - \"image\"\n    - \"memory\"\n    - \"network\"\n  hosts: [\"unix:///var/run/docker.sock\"]\n  period: 10s\n  enabled: true\noutput.elasticsearch:\n  enabled: true\n  hosts: [\"elastic-rest.service.consul:9200\"]\n  compression_level: 0\n  escape_html: true\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  worker: 1\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nsetup.kibana:\n  host: \"elastic-kibana.service.consul:5601\"\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  #path: \"\"\n  #space.id: \"\"\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nEOF\n        destination  = \"local/metricbeat.yml\"\n      }\n    }\n\n    task \"prod-task1-packetbeat\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver         = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"docker.elastic.co/beats/packetbeat:${version}\"\n        dns_servers  = [ \"$${attr.unique.network.ip-address}\" ]\n        privileged   = true\n\n        volumes      = [\n          \"local/packetbeat.yml:/usr/share/packetbeat/packetbeat.yml\"\n        ]\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\npacketbeat.interfaces.device: any\npacketbeat.flows:\n  enabled: true\n  timeout: 30s\n  period: 10s\npacketbeat.protocols:\n- type: icmp\n  enabled: true\n- type: dhcpv4\n  enabled: true\n  ports: [67, 68]\n- type: dns\n  enabled: true\n  ports: [53]\n  include_authorities: true\n  include_additionals: true\n- type: http\n  enabled: true\n  ports: [80, 8080, 8000, 5000, 8002]\n- type: tls\n  enabled: true\n  ports: [443, 8443, 9243]\npacketbeat.procs.enabled: false\npacketbeat.ignore_outgoing: false\noutput.elasticsearch:\n  enabled: true\n  hosts: [\"elastic-rest.service.consul:9200\"]\n  compression_level: 0\n  escape_html: true\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  worker: 1\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nsetup.kibana:\n  host: \"kibana.service.consul:5601\"\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  #path: \"\"\n  #space.id: \"\"\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nEOF\n        destination  = \"local/packetbeat.yml\"\n      }\n    }\n  }\n}",
            "vars": {
              "datacenters": "yul1",
              "group_count": "1",
              "job_name": "prod-beats",
              "use_canary": "true",
              "version": "7.10.1"
            }
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.elastic",
      "mode": "data",
      "type": "template_file",
      "name": "nomad_job_elastic",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "1f521a95abea61205e23203fe8c89b9b534cb0fb49a4aaeae6b93df1296dd7c3",
            "rendered": "job \"prod-elastic\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 1\n\n    health_check      = \"checks\"\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 1\n\n    # Specifies if the job should auto-promote to the canary version when all\n    # canaries become healthy during a deployment. Defaults to false which means\n    # canaries must be manually updated with the nomad deployment promote\n    # command.\n    auto_promote      = true\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = true\n\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-elastic-cluster\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count             = 1\n\n    constraint {\n      attribute       = \"${node.unique.name}\"\n      value           = \"s41-nomad-x86_64\"\n    }\n\n    ephemeral_disk {\n      size            = \"50000\"\n      sticky          = true\n      migrate         = false\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-elastic-cluster\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver          = \"docker\"\n\n      kill_timeout    = \"600s\"\n      kill_signal     = \"SIGTERM\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image         = \"docker.elastic.co/elasticsearch/elasticsearch:7.10.1\"\n        dns_servers   = [ \"${attr.unique.network.ip-address}\" ]\n        command       = \"elasticsearch\"\n        args          = [\n          \"-Enode.name=elastic${NOMAD_ALLOC_INDEX}\",\n          \"-Enetwork.host=0.0.0.0\",\n          \"-Ecluster.name=elastic\",\n          \"-Ehttp.port=${NOMAD_PORT_rest}\",\n          \"-Ehttp.publish_port=${NOMAD_HOST_PORT_rest}\",\n          \"-Ebootstrap.memory_lock=true\",\n          \"-Epath.logs=/alloc/logs/\",\n          \"-Ediscovery.type=single-node\",\n          \"-Etransport.publish_port=${NOMAD_HOST_PORT_transport}\",\n          \"-Etransport.port=${NOMAD_PORT_transport}\",\n          \"-Expack.license.self_generated.type=basic\",\n          \"-Expack.security.enabled=true\",\n          \"-Expack.security.http.ssl.enabled=true\",\n          \"-Expack.security.http.ssl.key=certs/elastic${NOMAD_ALLOC_INDEX}.key\",\n          \"-Expack.security.http.ssl.certificate=certs/elastic${NOMAD_ALLOC_INDEX}.crt\",\n          \"-Expack.security.http.ssl.certificate_authorities=certs/ca.crt\",\n          \"-Expack.security.transport.ssl.enabled=true\",\n          \"-Expack.security.transport.ssl.key=certs/elastic${NOMAD_ALLOC_INDEX}.key\",\n          \"-Expack.security.transport.ssl.certificate=certs/elastic${NOMAD_ALLOC_INDEX}.crt\",\n          \"-Expack.security.transport.ssl.certificate_authorities=certs/ca.crt\",\n          \"-Expack.security.transport.ssl.verification_mode=certificate\"\n        ]\n        volumes       = [\n          \"secrets/elastic${NOMAD_ALLOC_INDEX}.crt:/usr/share/elasticsearch/config/certs/elastic${NOMAD_ALLOC_INDEX}.crt\",\n          \"secrets/elastic${NOMAD_ALLOC_INDEX}.key:/usr/share/elasticsearch/config/certs/elastic${NOMAD_ALLOC_INDEX}.key\",\n          \"secrets/ca.crt:/usr/share/elasticsearch/config/certs/ca.crt\",\n          \"secrets/ca.key:/usr/share/elasticsearch/config/certs/ca.key\",\n          \"secrets/password:/usr/share/elasticsearch/config/password\"\n        ]\n        ulimit {\n          memlock     = \"-1\"\n          nofile      = \"65536\"\n          nproc       = \"8192\"\n        }\n      }\n\n      # The env stanza configures a list of environment variables to populate\n      # the task's environment before starting.\n      env {\n        ELASTIC_PASSWORD       = \"Elastic1234\"\n#        ELASTIC_PASSWORD_FILE  = \"/usr/share/elasticsearch/config/password\"\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\nElastic1234\nEOF\n        destination  = \"secrets/password\"\n        perms        = \"600\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDRjCCAi6gAwIBAgIUKHjku5RBb4UD9nm+Fg2RxR3z5mIwDQYJKoZIhvcNAQEL\nBQAwNDEyMDAGA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5l\ncmF0ZWQgQ0EwHhcNMjEwMTEzMDg0MDQ2WhcNMjQwMTEzMDg0MDQ2WjATMREwDwYD\nVQQDEwhlbGFzdGljMTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMtu\nBC5mFsyuq8RsaRpd8KyRaOPdRxqLiqLUONcc76XtvyGQT/87av2l5cuzniT/QZ0s\nVRpMeZWoRLF1SWS6YCCi+ohnaSun0tWXBdE5fdLv6yuNosAxgD7moIRSjSQ1loKm\nZEM6m0csVG/sh5J4J2APcJJjKPdOCf2szl/4Ct4VbutCaBex7E3ZmB2T1/adHh9F\nOkGZYj5FntsDL5eycYKmWOVDf/j24AMlCWzuucdZ/H0XZQ8OKou/Ut8v7hh5w2FT\n11/5uw/1C2Q4iVJnBWL6BADtxef0f6kCg3epXo35IIsmKpKecnW377kaO348SFmd\nBsIlPcCUUgXDQs/pJ+UCAwEAAaNxMG8wHQYDVR0OBBYEFCDsjcPITceLLpr2U8Xj\ncYLkKh6vMB8GA1UdIwQYMBaAFJkrC2lnfFdDaGNb7KPRhGy8tvw1MCIGA1UdEQQb\nMBmCF2VsYXN0aWMxLnNlcnZpY2UuY29uc3VsMAkGA1UdEwQCMAAwDQYJKoZIhvcN\nAQELBQADggEBADkT03usfJNn5G2Ldq9UBb2D5WTzcZ+OXdqcH1iTT8+0SwnqVvro\nDO2aE98XLqK/v2mlmU9ohrEsxhJAAHzc0wmL+kHk98z5EKAkXjj7hLtEyaTWLFGH\nV+Z1Oi+kTjukEDbHt2IP8/8HibjEWXqqbhFVQqmdON9axTtyO6syEonjBeLEO/89\n6Ki9GFmHKJp6WCpzRg0i5a1stPTo5cE+QJUiJjx5WmCXocakAAdORS3ES6gRYKW5\nKdD5AOlR6NNaewCZh0lNjZ3SmUUzSZ+HDJXi+BE4k1PJbyQ8Lav7rq5MXHtnV4kX\n4SC56vJ0z2g7/SonGTOJGWAbPGm6oK6Cu64=\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/elastic${NOMAD_ALLOC_INDEX}.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEpAIBAAKCAQEAy24ELmYWzK6rxGxpGl3wrJFo491HGouKotQ41xzvpe2/IZBP\n/ztq/aXly7OeJP9BnSxVGkx5lahEsXVJZLpgIKL6iGdpK6fS1ZcF0Tl90u/rK42i\nwDGAPuaghFKNJDWWgqZkQzqbRyxUb+yHkngnYA9wkmMo904J/azOX/gK3hVu60Jo\nF7HsTdmYHZPX9p0eH0U6QZliPkWe2wMvl7JxgqZY5UN/+PbgAyUJbO65x1n8fRdl\nDw4qi79S3y/uGHnDYVPXX/m7D/ULZDiJUmcFYvoEAO3F5/R/qQKDd6lejfkgiyYq\nkp5ydbfvuRo7fjxIWZ0GwiU9wJRSBcNCz+kn5QIDAQABAoIBAQCmz7QGCBiyBpk7\nHFqjEF0GZMZJ820Wy04Hb1acrlGlEmskLp4qgKKfE6Z3fvYzCEzZgTzXr9YTbkPF\n8JMaUen5WStvJr0K2zb7hjdy9V3D1pBUynOmffDXo24Ek1zBUF/3ClI0/p3NowAq\nNx6EcJp5HrAEmeNBx3BR353q/A6NRDd3/hbnZi+SfySI6eT4IfelKzhDSbjeuJwY\nysIukB6EBpJmhY7Zh9aK5QfPLA4hNyiLu04IhnwRkRh2s8G1Yckfdr/O3JmL+W1E\nrbNYF1Nxbr3aU22JFzoe1H2uSvaeQYzk2SNMCGhrF84hmyPXctp34Ao+H/dVBZZc\n4heI+G2BAoGBAO3MMzJmami4keA3Lo4N7i1jYQun+heTM0m43M/QKqICy/Rp/g6M\nChedNyRTUHWIsnt8QUz9L1HLHKuYmyfiKfdkt06uVDsQK4lbzbjsoCe0DYnxkXhE\no+mW5nUzRz2DL0orM7VZT64KVtTrTFtoxjm0pyGm0ayASOSPY9wHl5cZAoGBANsA\nWtfXSc4yt11QSkd6tGQdRCw+kcMXhiI2aRbtTha0yComXRo8SNXzvIfE5tIZ2oFG\ne2Kc1YHOPCEhq884ODbW8N+iBQnRst1yhM34Fc/TsFzTMuEGDXqyyRbPNn1MmkMI\nSSf4cmW9LhvE3Iph6jSgYB1nMRcptd6/+pS/NeytAoGAR8cpdP8hA3ci4TEG5m4i\nBKVIt8H+ZXtTMd+RF1FYbQq3EZGk1DNFIJed+2MCmFeouElrVJff3qqWft1TiBhm\nXnySMDfCyQk6ev2w/S6/sPxSUd8O7+SYLXwVGC9gQ5sDfTnJI+ZPfNM2HpLfu3/G\nxchX4np+M7mNRyBZHiNUiJECgYEAhjDAeTMMoVFIM+BHs2bHc/TO2gF41T7rzLjk\nSc0cpSMe51zcfX/k7VxM8DBBcwmubroeTn1lAgW5qF92ZCHBqDCqJY2kYrDgVXqf\nT4ms68x9a1NqAKHxznYQa26Kp9oxR9Oi59//UMHLp+5HaG+4z4hZfIrHdLb1Hskp\npM1JIH0CgYAwoIef0P/STykVI+YqK+06sEsEMjFPSg72fLimydSIAjMLc1xvnJYm\n4YnyNVtIqP4LmgQ3vKnz5yzHpsTWEeUAZ2d7UHE1gC5ZpV4DZvZIEzfPLU+cuQ+C\npFg9P7CZoeBBAHzVURVcinBkME7b+7RD5YkgJUr3+9KrSlisUMdvFg==\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/elastic${NOMAD_ALLOC_INDEX}.key\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDSjCCAjKgAwIBAgIVANfV55rdCci2edv4n6+zvIQ+TxWoMA0GCSqGSIb3DQEB\nCwUAMDQxMjAwBgNVBAMTKUVsYXN0aWMgQ2VydGlmaWNhdGUgVG9vbCBBdXRvZ2Vu\nZXJhdGVkIENBMB4XDTIxMDExMzA4NDA0NloXDTI0MDExMzA4NDA0NlowNDEyMDAG\nA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5lcmF0ZWQgQ0Ew\nggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCKgXUkoslTKKVeDMsfR9wW\n/jcT1ap576/BXrXK4LHgD8Cd2yGoQkS7aswln5KGM2X6k7qRGN6zMJuZq7yN88dP\nFVmU8/r8fKfhbhpwpus0Y3NyziSTIcZRoendx3I1jktEDcFFcB1yoVdbClh6S4Z+\nMkxHhCwBATvCCmNrwISbLLGyfYkmO10PW8zD/ly1UkSZJ//xfLBtL1u34Q5Aw38r\neBzxBIZRP5hBwnMXUiGo8Kz6ugJ+Qu7YbVL8z1Ie7icIsgjH8mulR23FZjEb5SXm\n0sVchA2vDLPkzdJCI8oDgrbT2KBSkr4u+wuo6VDDQsLWaI+uPhULHwdGQUVinliT\nAgMBAAGjUzBRMB0GA1UdDgQWBBSZKwtpZ3xXQ2hjW+yj0YRsvLb8NTAfBgNVHSME\nGDAWgBSZKwtpZ3xXQ2hjW+yj0YRsvLb8NTAPBgNVHRMBAf8EBTADAQH/MA0GCSqG\nSIb3DQEBCwUAA4IBAQBI9jkC3/B2SYc0v/uIS4AZ/7zvLi/bH+D2PsI9+Rhm7ae5\nrqXlN2eG8xdTKi4WTKLHOESJWeY5NSK6wAT5CPr1gfByzDAfFbnrwTtz4ELnAiw9\nZ4yTU/0ticQ4/d4vJXUPWZ8vqKCi6oTemPhdX0QoO65TKWYZRaaIPgKox+Q4VUP9\nLIUT7NBJn+Z5j+Bbyse0dQwzbKhUEOuQOfMcj7pyKFGeqVUEpRdbpEQ70hlHkwZ7\nHon4+ZsQ95BgCpPCm1rHSy/sJgiJ971CmwVJO8TjBgd4iwCit4x+EcFWsVUra/g0\n+FKz2L0enirRpzXpG4aNI/bSpiR3yLxAMENFkpo0\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/ca.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEpQIBAAKCAQEAioF1JKLJUyilXgzLH0fcFv43E9Wqee+vwV61yuCx4A/Andsh\nqEJEu2rMJZ+ShjNl+pO6kRjeszCbmau8jfPHTxVZlPP6/Hyn4W4acKbrNGNzcs4k\nkyHGUaHp3cdyNY5LRA3BRXAdcqFXWwpYekuGfjJMR4QsAQE7wgpja8CEmyyxsn2J\nJjtdD1vMw/5ctVJEmSf/8XywbS9bt+EOQMN/K3gc8QSGUT+YQcJzF1IhqPCs+roC\nfkLu2G1S/M9SHu4nCLIIx/JrpUdtxWYxG+Ul5tLFXIQNrwyz5M3SQiPKA4K209ig\nUpK+LvsLqOlQw0LC1miPrj4VCx8HRkFFYp5YkwIDAQABAoIBAQCKDA3dvgI7SD/K\nRaYOP2k14Zqzwjpv3l2mtecrllizof+xVj9tnN80jXV76lf4OjJiVeuVwtv0bXYo\n6+q68Uato/HtbF+0V+pb3YmszjGPva/LtXruyrMHmgGmcqt6haCu66a+tsgjAHw4\n2U7mVXBvR2KPxUS2m6wb8o61TuTcY3ATcYb05DN03nB42P6pDt/GVd1QrbKLylOT\nHopsr1ApgxvhPnxC90hgz4wniDI2RYhUgQbasNeC6CuSgjlQVCTl44JUIy/gQf0A\nrLX1lEySapHbLr8oxKpkvmfCh519vc9RnFjBIwzO3SAmNRUduZj+XFYJPgeavOjD\n5qYI88oBAoGBAMHjNew5mwqxAIjekKoJadv+Qz/xwwU+KA63GrDHqPfrs/4WJHJ7\nO0z5xFE5ANMYtC1EIxDfnZ4i3ZlPN3LpdzwPz6veAjUCHYNcf4USvErvawBluHbj\nnegkbjm7FyzVsyYNfkRk8pUrV4Pbw/eb2+mMeAOBCnuFBL8wWxYDzuUhAoGBALbg\nXGwU/zoYzq/xTkChojSJwKXjJovTFNkRgl6OyxdgjIKKogH1e0xYzGLuIk0olUwI\n8xP/77VGqnnE/dFnQ3pX0g7wgDwkXRBONnC0z21cNa5uB5cUKFUi4en61u/eORzF\nDZjLf4+QQD0CtU9MRz/okQ82LxzkLSR/D4BLfVMzAoGBAKcocrbku0yuaZ2W9PYE\nA6ZNQkGA9/gvLG3zYymCGaUVKysmf+nLYMbul1jHYnSc2cok8m57u/I4cQDaER4b\nNlcr8olkcFavKi60sqRSENAyNfgzuqOVffBEaFuRd1uKKlfmTjQ9K/97TIo8EGoL\nj799AYNT32u6tOr4j68dPWTBAoGBAK1iJF4Yvi6fzH5FYzKlzDrRi8P7i9UvqGlx\nT1BFQ8oDMNSniZgf3Olymz0El6Ld4ka3iXchxWvx9rkCir7Zj8FTuAWQAZSDyXQn\nIzhSRQNjVEXvbeTQKLknHFeRCe1bnHxpW03NSkCbvDvb8HihUkAGSFnKvno+34nl\nqZWyfLy/AoGAaonDtHmKHmhNZF35M6LQ81VmWRLw62LeYIdOCE+9PBMQgM8EtJ1Z\n5+F32dIDaP8QCkgYV7AyE1sxQvjYISu5OK2EEyl4aLYm9K0P1eeK6HEXz1HYkZpt\nNdZ6+3q7IGzxoqkmJmoR0bjrAZXH3mTEIsWZJTIK7qM+TM02GL8CHSg=\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/ca.key\"\n\n#        mounts        = [\n#          {\n#            type     = \"volume\"\n#            target   = \"/usr/share/elasticsearch/data/\"\n#            source   = \"es-cluster-cluster-vol\"\n#            readonly = false\n#          }\n#        ]\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name              = \"elastic\"\n        port              = \"rest\"\n        tags              = [ \"elastic${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Cluster REST Check Live\"\n          port            = \"rest\"\n          type            = \"tcp\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n        check {\n          name            = \"Elastic Cluster HTTP Check Live\"\n          type            = \"http\"\n          port            = \"rest\"\n          protocol        = \"https\"\n          method          = \"GET\"\n          header {\n            Authorization = [\"Basic ZWxhc3RpYzpFbGFzdGljMTIzNA==\"]\n          }\n          tls_skip_verify = true\n          path            = \"/_cluster/health?pretty\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n      service {\n        name              = \"elastic-transport\"\n        port              = \"transport\"\n        tags              = [ \"elastic-transport${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Cluster Transport Check Live\"\n          type            = \"tcp\"\n          port            = \"transport\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu        = 40000\n        memory     = 40000\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        network {\n          port \"rest\" {\n            static = 9200\n          }\n          port \"transport\" {\n            static = 9300\n          }\n        }\n      }\n    }\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-elastic-kibana\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count              = 1\n\n    constraint {\n      attribute        = \"${node.unique.name}\"\n      value            = \"s41-nomad-x86_64\"\n    }\n\n    update {\n      max_parallel     = 1\n      health_check     = \"checks\"\n      min_healthy_time = \"10s\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-elastic-task\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver           = \"docker\"\n\n      kill_timeout     = \"60s\"\n      kill_signal      = \"SIGTERM\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image         = \"docker.elastic.co/kibana/kibana:7.10.1\"\n        dns_servers   = [ \"${attr.unique.network.ip-address}\" ]\n        command       = \"kibana\"\n        args          = [\n          \"--server.name=kibana\",\n          \"--server.host=0.0.0.0\",\n          \"--server.port=${NOMAD_PORT_http}\",\n          \"--server.ssl.enabled=true\",\n          \"--server.ssl.certificate=/etc/kibana/config/certs/kibana.crt\",\n          \"--server.ssl.key=/etc/kibana/config/certs/kibana.key\",\n          \"--elasticsearch.hosts=http://elastic.service.consul:9200\",\n          \"--elasticsearch.username=kibanauser\",\n          \"--elasticsearch.password=Kibana1234\",\n          \"--elasticsearch.ssl.certificateAuthorities=/etc/kibana/config/certs/ca.crt\",\n          \"--xpack.apm.ui.enabled=false\",\n          \"--xpack.graph.enabled=false\",\n          \"--xpack.grokdebugger.enabled=false\",\n          \"--xpack.maps.enabled=false\",\n          \"--xpack.ml.enabled=false\",\n          \"--xpack.searchprofiler.enabled=false\"\n        ]\n        volumes       = [\n          \"secrets/kibana.crt:/etc/kibana/config/certs/kibana.crt\",\n          \"secrets/kibana.key:/etc/kibana/config/certs/kibana.key\",\n          \"secrets/ca.crt:/etc/kibana/config/certs/ca.crt\",\n          \"secrets/ca.key:/etc/kibana/config/certs/ca.key\"\n        ]\n        ulimit {\n          memlock     = \"-1\"\n          nofile      = \"65536\"\n          nproc       = \"8192\"\n        }\n      }\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDQjCCAiqgAwIBAgIULmIAn3JK6TSHotfL1HexLzHBz60wDQYJKoZIhvcNAQEL\nBQAwNDEyMDAGA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5l\ncmF0ZWQgQ0EwHhcNMjEwMTEzMDg0MDQ2WhcNMjQwMTEzMDg0MDQ2WjARMQ8wDQYD\nVQQDEwZraWJhbmEwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCGwE7u\nMO7RfEjfcvtnthZvLfBxEW/CbMfaHa6P2VdmSOi6AYIMZBkYfMEzGJDK8p/lDahm\naXyHc9/Lc+cA0H2XLVgBGZERJirdz3zKkqHLa+BcQxighDJJ3a9zsABoXALsxNgK\niCuFyrOQEEKkM775ZyrdQ6RE39KZLB3GMzkbCRQ/A3HTox8uzgoiXLLvNK60TIgb\n7Sg9v/Zt2ZrFkQu0TsSGouYtYPgvdf9osRznP4QX511HjXV+/P/0Q1abmJU5/WZd\nDk0Sd4TmfscuOiizmO8mrOG2afN4Gc+VYkNzLMVn92DGz/WxP3dKhIhfMyMCqj5M\nJNlCCvVaNm/wujR7AgMBAAGjbzBtMB0GA1UdDgQWBBQKzxXjr7QlQfEdLDo2dNup\noJQKYzAfBgNVHSMEGDAWgBSZKwtpZ3xXQ2hjW+yj0YRsvLb8NTAgBgNVHREEGTAX\nghVraWJhbmEuc2VydmljZS5jb25zdWwwCQYDVR0TBAIwADANBgkqhkiG9w0BAQsF\nAAOCAQEAW1h8U2s4RQoJnbmQOIJYcn8b9+glmOVq09ch5knc22C6VOPaSTUyAMkM\n3glrPnfbvFmSYGNTzXRkZ0m3GI0QSaiVZ8jHQq0unk904+zEqaxT1gMFKc3iv1lP\nDGxMWemP3T9FsEN6Ll9N5YSXP+IonwMEW8mh3PDWDkNZ4haYtbzFyDSNIawljU7G\nn/oqIX0bk7gTaqW789L4GDYWhP1vDldkLBhZOiBIrByaIfHvdYFmhfrXiDMKhkXJ\n6UJ4tFnU8c0xjVk9/uGCuwaUvaweBp9yJQGIknWlH8O/59JjcX7n1/GkGSAtwkH0\n/M+PDfjDTL3FtqY8MYHnkjVItwzftQ==\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/kibana.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEAhsBO7jDu0XxI33L7Z7YWby3wcRFvwmzH2h2uj9lXZkjougGC\nDGQZGHzBMxiQyvKf5Q2oZml8h3Pfy3PnANB9ly1YARmRESYq3c98ypKhy2vgXEMY\noIQySd2vc7AAaFwC7MTYCogrhcqzkBBCpDO++Wcq3UOkRN/SmSwdxjM5GwkUPwNx\n06MfLs4KIlyy7zSutEyIG+0oPb/2bdmaxZELtE7EhqLmLWD4L3X/aLEc5z+EF+dd\nR411fvz/9ENWm5iVOf1mXQ5NEneE5n7HLjoos5jvJqzhtmnzeBnPlWJDcyzFZ/dg\nxs/1sT93SoSIXzMjAqo+TCTZQgr1WjZv8Lo0ewIDAQABAoIBADTV+Nz6gNnREr3S\n1vLuedN0PuAGxzyD7MUAeG7c+KEZm287oiN7qD9qw1JmoneBNOLaPRqS6AowjCK5\nOm2eUnBRjj04KiKARbSdY8AGSLx7ewiSInjl/NXrv5zr+Ozyjw8Ji/BtPiuCtG+b\ngJXj2FDwe+UwXZvH60q1+qK5eP25OYmxTS+KROUDLzKSJwfiyIRwfzTV634KMae7\n9x8FTpR4/xnzPh5A9HaixtrZNZye02TvBvTavzRAABF3B7kFOqxhfJn55aJ8zLnF\nYZ4SDOuIQtt0hZUgQoZ8on4848H2yjW0FRef2A3VCqTRu3J0AQRJS8JPJ31yX+YK\ntyGjYcECgYEA1bFy+uJJ/6g60I9Lu+CQ2Ay/iQo/2TzEYjUa28MPG4KZkkihkfjB\n7cgvZwKvWDEUOGAz2GFDwz6H+1oao+hAB+VidOMkytGazQz6/V9cYqKmZk4kJrw7\n1EHLgkpFfHeG//WBGwKIINj0RLvfXx8ioq8ktrfAil48ozS3toLUHlsCgYEAoW3d\ntANQxTwoAlD5I6rzGBqxwCoA6JegBNsIsob4rB/R3zKJA52utEECI8i449NN6hmQ\n84L6JaodXe3XRlIa1saucIe5jezgRca0etzYviuYOQ0PsEqRMKYKeBo72X6LzEa8\nGZjV+d4rpvo71mmK892V2OpY1WoEzQ58bmj2XGECgYAQYaoO0YoaryrTEikcHfr8\nlP2Z489BOAdV//wvHKTr1vcu36KDLi6vq8j2fJ40hI6oQ7e1vr8TGJgUDLQ+HG/M\nKymBDGilo6vaTERxZ/4NEarv7M2YqpVrkB+pvUfWYtNWi9t51pfY7MjM/BoDkL92\n+TY3S57W/KJpYIE03JKmQQKBgQCQ/1AuSvQX1SrSucyunvRvaDrUsmXSha7z7ZHo\nWZevc31dj9TF7LJpsiKr5bU83iWT6pbqQ3FQt3ZdUi8VONZmqFszNJYUxvnDcvHV\nkd0VI689P2AiJzg2jE3HBzlO6H3FZJu8Gi3InCh1eTqaIn7vAM+B4S0dtHbPgP1/\nZsQywQKBgG4XNt+YyqdDvD406sWzQ2m+C+JQMeINn6QdDdW8rg1xLiTFd7vYYPYD\nj6ohBSQRgnVmMc0Q4efYP0TI978Mf1f5H/BCU+6azR/L6CapqhrDTtYY13iscyuV\nR7AQK4iPNeR6ls92AY2W+PJGIYiByk+7JbOXX/gvPTX7F+/6NsRq\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/kibana.key\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDSjCCAjKgAwIBAgIVANfV55rdCci2edv4n6+zvIQ+TxWoMA0GCSqGSIb3DQEB\nCwUAMDQxMjAwBgNVBAMTKUVsYXN0aWMgQ2VydGlmaWNhdGUgVG9vbCBBdXRvZ2Vu\nZXJhdGVkIENBMB4XDTIxMDExMzA4NDA0NloXDTI0MDExMzA4NDA0NlowNDEyMDAG\nA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5lcmF0ZWQgQ0Ew\nggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCKgXUkoslTKKVeDMsfR9wW\n/jcT1ap576/BXrXK4LHgD8Cd2yGoQkS7aswln5KGM2X6k7qRGN6zMJuZq7yN88dP\nFVmU8/r8fKfhbhpwpus0Y3NyziSTIcZRoendx3I1jktEDcFFcB1yoVdbClh6S4Z+\nMkxHhCwBATvCCmNrwISbLLGyfYkmO10PW8zD/ly1UkSZJ//xfLBtL1u34Q5Aw38r\neBzxBIZRP5hBwnMXUiGo8Kz6ugJ+Qu7YbVL8z1Ie7icIsgjH8mulR23FZjEb5SXm\n0sVchA2vDLPkzdJCI8oDgrbT2KBSkr4u+wuo6VDDQsLWaI+uPhULHwdGQUVinliT\nAgMBAAGjUzBRMB0GA1UdDgQWBBSZKwtpZ3xXQ2hjW+yj0YRsvLb8NTAfBgNVHSME\nGDAWgBSZKwtpZ3xXQ2hjW+yj0YRsvLb8NTAPBgNVHRMBAf8EBTADAQH/MA0GCSqG\nSIb3DQEBCwUAA4IBAQBI9jkC3/B2SYc0v/uIS4AZ/7zvLi/bH+D2PsI9+Rhm7ae5\nrqXlN2eG8xdTKi4WTKLHOESJWeY5NSK6wAT5CPr1gfByzDAfFbnrwTtz4ELnAiw9\nZ4yTU/0ticQ4/d4vJXUPWZ8vqKCi6oTemPhdX0QoO65TKWYZRaaIPgKox+Q4VUP9\nLIUT7NBJn+Z5j+Bbyse0dQwzbKhUEOuQOfMcj7pyKFGeqVUEpRdbpEQ70hlHkwZ7\nHon4+ZsQ95BgCpPCm1rHSy/sJgiJ971CmwVJO8TjBgd4iwCit4x+EcFWsVUra/g0\n+FKz2L0enirRpzXpG4aNI/bSpiR3yLxAMENFkpo0\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/ca.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEpQIBAAKCAQEAioF1JKLJUyilXgzLH0fcFv43E9Wqee+vwV61yuCx4A/Andsh\nqEJEu2rMJZ+ShjNl+pO6kRjeszCbmau8jfPHTxVZlPP6/Hyn4W4acKbrNGNzcs4k\nkyHGUaHp3cdyNY5LRA3BRXAdcqFXWwpYekuGfjJMR4QsAQE7wgpja8CEmyyxsn2J\nJjtdD1vMw/5ctVJEmSf/8XywbS9bt+EOQMN/K3gc8QSGUT+YQcJzF1IhqPCs+roC\nfkLu2G1S/M9SHu4nCLIIx/JrpUdtxWYxG+Ul5tLFXIQNrwyz5M3SQiPKA4K209ig\nUpK+LvsLqOlQw0LC1miPrj4VCx8HRkFFYp5YkwIDAQABAoIBAQCKDA3dvgI7SD/K\nRaYOP2k14Zqzwjpv3l2mtecrllizof+xVj9tnN80jXV76lf4OjJiVeuVwtv0bXYo\n6+q68Uato/HtbF+0V+pb3YmszjGPva/LtXruyrMHmgGmcqt6haCu66a+tsgjAHw4\n2U7mVXBvR2KPxUS2m6wb8o61TuTcY3ATcYb05DN03nB42P6pDt/GVd1QrbKLylOT\nHopsr1ApgxvhPnxC90hgz4wniDI2RYhUgQbasNeC6CuSgjlQVCTl44JUIy/gQf0A\nrLX1lEySapHbLr8oxKpkvmfCh519vc9RnFjBIwzO3SAmNRUduZj+XFYJPgeavOjD\n5qYI88oBAoGBAMHjNew5mwqxAIjekKoJadv+Qz/xwwU+KA63GrDHqPfrs/4WJHJ7\nO0z5xFE5ANMYtC1EIxDfnZ4i3ZlPN3LpdzwPz6veAjUCHYNcf4USvErvawBluHbj\nnegkbjm7FyzVsyYNfkRk8pUrV4Pbw/eb2+mMeAOBCnuFBL8wWxYDzuUhAoGBALbg\nXGwU/zoYzq/xTkChojSJwKXjJovTFNkRgl6OyxdgjIKKogH1e0xYzGLuIk0olUwI\n8xP/77VGqnnE/dFnQ3pX0g7wgDwkXRBONnC0z21cNa5uB5cUKFUi4en61u/eORzF\nDZjLf4+QQD0CtU9MRz/okQ82LxzkLSR/D4BLfVMzAoGBAKcocrbku0yuaZ2W9PYE\nA6ZNQkGA9/gvLG3zYymCGaUVKysmf+nLYMbul1jHYnSc2cok8m57u/I4cQDaER4b\nNlcr8olkcFavKi60sqRSENAyNfgzuqOVffBEaFuRd1uKKlfmTjQ9K/97TIo8EGoL\nj799AYNT32u6tOr4j68dPWTBAoGBAK1iJF4Yvi6fzH5FYzKlzDrRi8P7i9UvqGlx\nT1BFQ8oDMNSniZgf3Olymz0El6Ld4ka3iXchxWvx9rkCir7Zj8FTuAWQAZSDyXQn\nIzhSRQNjVEXvbeTQKLknHFeRCe1bnHxpW03NSkCbvDvb8HihUkAGSFnKvno+34nl\nqZWyfLy/AoGAaonDtHmKHmhNZF35M6LQ81VmWRLw62LeYIdOCE+9PBMQgM8EtJ1Z\n5+F32dIDaP8QCkgYV7AyE1sxQvjYISu5OK2EEyl4aLYm9K0P1eeK6HEXz1HYkZpt\nNdZ6+3q7IGzxoqkmJmoR0bjrAZXH3mTEIsWZJTIK7qM+TM02GL8CHSg=\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/ca.key\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name              = \"kibana\"\n        port              = \"http\"\n        tags              = [ \"kibana${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Kibana Transport Check Live\"\n          port            = \"http\"\n          type            = \"tcp\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n        check {\n          name            = \"Elastic Kibana HTTP Check Live\"\n          type            = \"http\"\n          port            = \"http\"\n          protocol        = \"https\"\n          method          = \"GET\"\n          tls_skip_verify = true\n          path            = \"/\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu           = 1000\n        memory        = 8192\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        network {\n          port \"http\" {\n            static    = 5601\n          }\n        }\n      }\n    }\n  }\n}\n",
            "template": "job \"${job_name}\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"${datacenters}\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 1\n\n    health_check      = \"checks\"\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n%{ if use_canary }\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 1\n\n    # Specifies if the job should auto-promote to the canary version when all\n    # canaries become healthy during a deployment. Defaults to false which means\n    # canaries must be manually updated with the nomad deployment promote\n    # command.\n    auto_promote      = true\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = true\n%{ endif }\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-elastic-cluster\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count             = ${group_count}\n\n    constraint {\n      attribute       = \"$${node.unique.name}\"\n      value           = \"s41-nomad-x86_64\"\n    }\n\n    ephemeral_disk {\n      size            = \"50000\"\n      sticky          = true\n      migrate         = false\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-elastic-cluster\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver          = \"docker\"\n\n      kill_timeout    = \"600s\"\n      kill_signal     = \"SIGTERM\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image         = \"docker.elastic.co/elasticsearch/elasticsearch:${version}\"\n        dns_servers   = [ \"$${attr.unique.network.ip-address}\" ]\n        command       = \"elasticsearch\"\n        args          = [\n          \"-Enode.name=${cluster_service_name}$${NOMAD_ALLOC_INDEX}\",\n          \"-Enetwork.host=0.0.0.0\",\n          \"-Ecluster.name=${cluster_service_name}\",\n          \"-Ehttp.port=$${NOMAD_PORT_rest}\",\n          \"-Ehttp.publish_port=$${NOMAD_HOST_PORT_rest}\",\n          \"-Ebootstrap.memory_lock=true\",\n          \"-Epath.logs=/alloc/logs/\",\n          \"-Ediscovery.type=single-node\",\n          \"-Etransport.publish_port=$${NOMAD_HOST_PORT_transport}\",\n          \"-Etransport.port=$${NOMAD_PORT_transport}\",\n          \"-Expack.license.self_generated.type=basic\",\n          \"-Expack.security.enabled=true\",\n          \"-Expack.security.http.ssl.enabled=true\",\n          \"-Expack.security.http.ssl.key=certs/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.key\",\n          \"-Expack.security.http.ssl.certificate=certs/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.crt\",\n          \"-Expack.security.http.ssl.certificate_authorities=certs/ca.crt\",\n          \"-Expack.security.transport.ssl.enabled=true\",\n          \"-Expack.security.transport.ssl.key=certs/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.key\",\n          \"-Expack.security.transport.ssl.certificate=certs/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.crt\",\n          \"-Expack.security.transport.ssl.certificate_authorities=certs/ca.crt\",\n          \"-Expack.security.transport.ssl.verification_mode=certificate\"\n        ]\n        volumes       = [\n          \"secrets/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.crt:/usr/share/elasticsearch/config/certs/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.crt\",\n          \"secrets/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.key:/usr/share/elasticsearch/config/certs/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.key\",\n          \"secrets/ca.crt:/usr/share/elasticsearch/config/certs/ca.crt\",\n          \"secrets/ca.key:/usr/share/elasticsearch/config/certs/ca.key\",\n          \"secrets/password:/usr/share/elasticsearch/config/password\"\n        ]\n        ulimit {\n          memlock     = \"-1\"\n          nofile      = \"65536\"\n          nproc       = \"8192\"\n        }\n      }\n\n      # The env stanza configures a list of environment variables to populate\n      # the task's environment before starting.\n      env {\n        ELASTIC_PASSWORD       = \"${cluster_password}\"\n#        ELASTIC_PASSWORD_FILE  = \"/usr/share/elasticsearch/config/password\"\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\n${cluster_password}\nEOF\n        destination  = \"secrets/password\"\n        perms        = \"600\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDRjCCAi6gAwIBAgIUKHjku5RBb4UD9nm+Fg2RxR3z5mIwDQYJKoZIhvcNAQEL\nBQAwNDEyMDAGA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5l\ncmF0ZWQgQ0EwHhcNMjEwMTEzMDg0MDQ2WhcNMjQwMTEzMDg0MDQ2WjATMREwDwYD\nVQQDEwhlbGFzdGljMTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMtu\nBC5mFsyuq8RsaRpd8KyRaOPdRxqLiqLUONcc76XtvyGQT/87av2l5cuzniT/QZ0s\nVRpMeZWoRLF1SWS6YCCi+ohnaSun0tWXBdE5fdLv6yuNosAxgD7moIRSjSQ1loKm\nZEM6m0csVG/sh5J4J2APcJJjKPdOCf2szl/4Ct4VbutCaBex7E3ZmB2T1/adHh9F\nOkGZYj5FntsDL5eycYKmWOVDf/j24AMlCWzuucdZ/H0XZQ8OKou/Ut8v7hh5w2FT\n11/5uw/1C2Q4iVJnBWL6BADtxef0f6kCg3epXo35IIsmKpKecnW377kaO348SFmd\nBsIlPcCUUgXDQs/pJ+UCAwEAAaNxMG8wHQYDVR0OBBYEFCDsjcPITceLLpr2U8Xj\ncYLkKh6vMB8GA1UdIwQYMBaAFJkrC2lnfFdDaGNb7KPRhGy8tvw1MCIGA1UdEQQb\nMBmCF2VsYXN0aWMxLnNlcnZpY2UuY29uc3VsMAkGA1UdEwQCMAAwDQYJKoZIhvcN\nAQELBQADggEBADkT03usfJNn5G2Ldq9UBb2D5WTzcZ+OXdqcH1iTT8+0SwnqVvro\nDO2aE98XLqK/v2mlmU9ohrEsxhJAAHzc0wmL+kHk98z5EKAkXjj7hLtEyaTWLFGH\nV+Z1Oi+kTjukEDbHt2IP8/8HibjEWXqqbhFVQqmdON9axTtyO6syEonjBeLEO/89\n6Ki9GFmHKJp6WCpzRg0i5a1stPTo5cE+QJUiJjx5WmCXocakAAdORS3ES6gRYKW5\nKdD5AOlR6NNaewCZh0lNjZ3SmUUzSZ+HDJXi+BE4k1PJbyQ8Lav7rq5MXHtnV4kX\n4SC56vJ0z2g7/SonGTOJGWAbPGm6oK6Cu64=\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEpAIBAAKCAQEAy24ELmYWzK6rxGxpGl3wrJFo491HGouKotQ41xzvpe2/IZBP\n/ztq/aXly7OeJP9BnSxVGkx5lahEsXVJZLpgIKL6iGdpK6fS1ZcF0Tl90u/rK42i\nwDGAPuaghFKNJDWWgqZkQzqbRyxUb+yHkngnYA9wkmMo904J/azOX/gK3hVu60Jo\nF7HsTdmYHZPX9p0eH0U6QZliPkWe2wMvl7JxgqZY5UN/+PbgAyUJbO65x1n8fRdl\nDw4qi79S3y/uGHnDYVPXX/m7D/ULZDiJUmcFYvoEAO3F5/R/qQKDd6lejfkgiyYq\nkp5ydbfvuRo7fjxIWZ0GwiU9wJRSBcNCz+kn5QIDAQABAoIBAQCmz7QGCBiyBpk7\nHFqjEF0GZMZJ820Wy04Hb1acrlGlEmskLp4qgKKfE6Z3fvYzCEzZgTzXr9YTbkPF\n8JMaUen5WStvJr0K2zb7hjdy9V3D1pBUynOmffDXo24Ek1zBUF/3ClI0/p3NowAq\nNx6EcJp5HrAEmeNBx3BR353q/A6NRDd3/hbnZi+SfySI6eT4IfelKzhDSbjeuJwY\nysIukB6EBpJmhY7Zh9aK5QfPLA4hNyiLu04IhnwRkRh2s8G1Yckfdr/O3JmL+W1E\nrbNYF1Nxbr3aU22JFzoe1H2uSvaeQYzk2SNMCGhrF84hmyPXctp34Ao+H/dVBZZc\n4heI+G2BAoGBAO3MMzJmami4keA3Lo4N7i1jYQun+heTM0m43M/QKqICy/Rp/g6M\nChedNyRTUHWIsnt8QUz9L1HLHKuYmyfiKfdkt06uVDsQK4lbzbjsoCe0DYnxkXhE\no+mW5nUzRz2DL0orM7VZT64KVtTrTFtoxjm0pyGm0ayASOSPY9wHl5cZAoGBANsA\nWtfXSc4yt11QSkd6tGQdRCw+kcMXhiI2aRbtTha0yComXRo8SNXzvIfE5tIZ2oFG\ne2Kc1YHOPCEhq884ODbW8N+iBQnRst1yhM34Fc/TsFzTMuEGDXqyyRbPNn1MmkMI\nSSf4cmW9LhvE3Iph6jSgYB1nMRcptd6/+pS/NeytAoGAR8cpdP8hA3ci4TEG5m4i\nBKVIt8H+ZXtTMd+RF1FYbQq3EZGk1DNFIJed+2MCmFeouElrVJff3qqWft1TiBhm\nXnySMDfCyQk6ev2w/S6/sPxSUd8O7+SYLXwVGC9gQ5sDfTnJI+ZPfNM2HpLfu3/G\nxchX4np+M7mNRyBZHiNUiJECgYEAhjDAeTMMoVFIM+BHs2bHc/TO2gF41T7rzLjk\nSc0cpSMe51zcfX/k7VxM8DBBcwmubroeTn1lAgW5qF92ZCHBqDCqJY2kYrDgVXqf\nT4ms68x9a1NqAKHxznYQa26Kp9oxR9Oi59//UMHLp+5HaG+4z4hZfIrHdLb1Hskp\npM1JIH0CgYAwoIef0P/STykVI+YqK+06sEsEMjFPSg72fLimydSIAjMLc1xvnJYm\n4YnyNVtIqP4LmgQ3vKnz5yzHpsTWEeUAZ2d7UHE1gC5ZpV4DZvZIEzfPLU+cuQ+C\npFg9P7CZoeBBAHzVURVcinBkME7b+7RD5YkgJUr3+9KrSlisUMdvFg==\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/${cluster_service_name}$${NOMAD_ALLOC_INDEX}.key\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDSjCCAjKgAwIBAgIVANfV55rdCci2edv4n6+zvIQ+TxWoMA0GCSqGSIb3DQEB\nCwUAMDQxMjAwBgNVBAMTKUVsYXN0aWMgQ2VydGlmaWNhdGUgVG9vbCBBdXRvZ2Vu\nZXJhdGVkIENBMB4XDTIxMDExMzA4NDA0NloXDTI0MDExMzA4NDA0NlowNDEyMDAG\nA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5lcmF0ZWQgQ0Ew\nggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCKgXUkoslTKKVeDMsfR9wW\n/jcT1ap576/BXrXK4LHgD8Cd2yGoQkS7aswln5KGM2X6k7qRGN6zMJuZq7yN88dP\nFVmU8/r8fKfhbhpwpus0Y3NyziSTIcZRoendx3I1jktEDcFFcB1yoVdbClh6S4Z+\nMkxHhCwBATvCCmNrwISbLLGyfYkmO10PW8zD/ly1UkSZJ//xfLBtL1u34Q5Aw38r\neBzxBIZRP5hBwnMXUiGo8Kz6ugJ+Qu7YbVL8z1Ie7icIsgjH8mulR23FZjEb5SXm\n0sVchA2vDLPkzdJCI8oDgrbT2KBSkr4u+wuo6VDDQsLWaI+uPhULHwdGQUVinliT\nAgMBAAGjUzBRMB0GA1UdDgQWBBSZKwtpZ3xXQ2hjW+yj0YRsvLb8NTAfBgNVHSME\nGDAWgBSZKwtpZ3xXQ2hjW+yj0YRsvLb8NTAPBgNVHRMBAf8EBTADAQH/MA0GCSqG\nSIb3DQEBCwUAA4IBAQBI9jkC3/B2SYc0v/uIS4AZ/7zvLi/bH+D2PsI9+Rhm7ae5\nrqXlN2eG8xdTKi4WTKLHOESJWeY5NSK6wAT5CPr1gfByzDAfFbnrwTtz4ELnAiw9\nZ4yTU/0ticQ4/d4vJXUPWZ8vqKCi6oTemPhdX0QoO65TKWYZRaaIPgKox+Q4VUP9\nLIUT7NBJn+Z5j+Bbyse0dQwzbKhUEOuQOfMcj7pyKFGeqVUEpRdbpEQ70hlHkwZ7\nHon4+ZsQ95BgCpPCm1rHSy/sJgiJ971CmwVJO8TjBgd4iwCit4x+EcFWsVUra/g0\n+FKz2L0enirRpzXpG4aNI/bSpiR3yLxAMENFkpo0\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/ca.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEpQIBAAKCAQEAioF1JKLJUyilXgzLH0fcFv43E9Wqee+vwV61yuCx4A/Andsh\nqEJEu2rMJZ+ShjNl+pO6kRjeszCbmau8jfPHTxVZlPP6/Hyn4W4acKbrNGNzcs4k\nkyHGUaHp3cdyNY5LRA3BRXAdcqFXWwpYekuGfjJMR4QsAQE7wgpja8CEmyyxsn2J\nJjtdD1vMw/5ctVJEmSf/8XywbS9bt+EOQMN/K3gc8QSGUT+YQcJzF1IhqPCs+roC\nfkLu2G1S/M9SHu4nCLIIx/JrpUdtxWYxG+Ul5tLFXIQNrwyz5M3SQiPKA4K209ig\nUpK+LvsLqOlQw0LC1miPrj4VCx8HRkFFYp5YkwIDAQABAoIBAQCKDA3dvgI7SD/K\nRaYOP2k14Zqzwjpv3l2mtecrllizof+xVj9tnN80jXV76lf4OjJiVeuVwtv0bXYo\n6+q68Uato/HtbF+0V+pb3YmszjGPva/LtXruyrMHmgGmcqt6haCu66a+tsgjAHw4\n2U7mVXBvR2KPxUS2m6wb8o61TuTcY3ATcYb05DN03nB42P6pDt/GVd1QrbKLylOT\nHopsr1ApgxvhPnxC90hgz4wniDI2RYhUgQbasNeC6CuSgjlQVCTl44JUIy/gQf0A\nrLX1lEySapHbLr8oxKpkvmfCh519vc9RnFjBIwzO3SAmNRUduZj+XFYJPgeavOjD\n5qYI88oBAoGBAMHjNew5mwqxAIjekKoJadv+Qz/xwwU+KA63GrDHqPfrs/4WJHJ7\nO0z5xFE5ANMYtC1EIxDfnZ4i3ZlPN3LpdzwPz6veAjUCHYNcf4USvErvawBluHbj\nnegkbjm7FyzVsyYNfkRk8pUrV4Pbw/eb2+mMeAOBCnuFBL8wWxYDzuUhAoGBALbg\nXGwU/zoYzq/xTkChojSJwKXjJovTFNkRgl6OyxdgjIKKogH1e0xYzGLuIk0olUwI\n8xP/77VGqnnE/dFnQ3pX0g7wgDwkXRBONnC0z21cNa5uB5cUKFUi4en61u/eORzF\nDZjLf4+QQD0CtU9MRz/okQ82LxzkLSR/D4BLfVMzAoGBAKcocrbku0yuaZ2W9PYE\nA6ZNQkGA9/gvLG3zYymCGaUVKysmf+nLYMbul1jHYnSc2cok8m57u/I4cQDaER4b\nNlcr8olkcFavKi60sqRSENAyNfgzuqOVffBEaFuRd1uKKlfmTjQ9K/97TIo8EGoL\nj799AYNT32u6tOr4j68dPWTBAoGBAK1iJF4Yvi6fzH5FYzKlzDrRi8P7i9UvqGlx\nT1BFQ8oDMNSniZgf3Olymz0El6Ld4ka3iXchxWvx9rkCir7Zj8FTuAWQAZSDyXQn\nIzhSRQNjVEXvbeTQKLknHFeRCe1bnHxpW03NSkCbvDvb8HihUkAGSFnKvno+34nl\nqZWyfLy/AoGAaonDtHmKHmhNZF35M6LQ81VmWRLw62LeYIdOCE+9PBMQgM8EtJ1Z\n5+F32dIDaP8QCkgYV7AyE1sxQvjYISu5OK2EEyl4aLYm9K0P1eeK6HEXz1HYkZpt\nNdZ6+3q7IGzxoqkmJmoR0bjrAZXH3mTEIsWZJTIK7qM+TM02GL8CHSg=\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/ca.key\"\n\n#        mounts        = [\n#          {\n#            type     = \"volume\"\n#            target   = \"/usr/share/elasticsearch/data/\"\n#            source   = \"es-cluster-cluster-vol\"\n#            readonly = false\n#          }\n#        ]\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name              = \"${cluster_service_name}\"\n        port              = \"rest\"\n        tags              = [ \"${cluster_service_name}$${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Cluster REST Check Live\"\n          port            = \"rest\"\n          type            = \"tcp\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n        check {\n          name            = \"Elastic Cluster HTTP Check Live\"\n          type            = \"http\"\n          port            = \"rest\"\n          protocol        = \"https\"\n          method          = \"GET\"\n          header {\n            Authorization = [\"Basic ZWxhc3RpYzpFbGFzdGljMTIzNA==\"]\n          }\n          tls_skip_verify = true\n          path            = \"/_cluster/health?pretty\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n      service {\n        name              = \"${cluster_service_name}-transport\"\n        port              = \"transport\"\n        tags              = [ \"${cluster_service_name}-transport$${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Cluster Transport Check Live\"\n          type            = \"tcp\"\n          port            = \"transport\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu        = ${cluster_cpu}\n        memory     = ${cluster_memory}\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        network {\n          port \"rest\" {\n            static = ${cluster_rest_port}\n          }\n          port \"transport\" {\n            static = ${cluster_transport_port}\n          }\n        }\n      }\n    }\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-elastic-kibana\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count              = 1\n\n    constraint {\n      attribute        = \"$${node.unique.name}\"\n      value            = \"s41-nomad-x86_64\"\n    }\n\n    update {\n      max_parallel     = 1\n      health_check     = \"checks\"\n      min_healthy_time = \"10s\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-elastic-task\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver           = \"docker\"\n\n      kill_timeout     = \"60s\"\n      kill_signal      = \"SIGTERM\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image         = \"docker.elastic.co/kibana/kibana:${version}\"\n        dns_servers   = [ \"$${attr.unique.network.ip-address}\" ]\n        command       = \"kibana\"\n        args          = [\n          \"--server.name=${kibana_service_name}\",\n          \"--server.host=0.0.0.0\",\n          \"--server.port=$${NOMAD_PORT_http}\",\n          \"--server.ssl.enabled=true\",\n          \"--server.ssl.certificate=/etc/kibana/config/certs/${kibana_service_name}.crt\",\n          \"--server.ssl.key=/etc/kibana/config/certs/${kibana_service_name}.key\",\n          \"--elasticsearch.hosts=http://${cluster_service_name}.service.consul:9200\",\n          \"--elasticsearch.username=kibanauser\",\n          \"--elasticsearch.password=Kibana1234\",\n          \"--elasticsearch.ssl.certificateAuthorities=/etc/kibana/config/certs/ca.crt\",\n          \"--xpack.apm.ui.enabled=false\",\n          \"--xpack.graph.enabled=false\",\n          \"--xpack.grokdebugger.enabled=false\",\n          \"--xpack.maps.enabled=false\",\n          \"--xpack.ml.enabled=false\",\n          \"--xpack.searchprofiler.enabled=false\"\n        ]\n        volumes       = [\n          \"secrets/${kibana_service_name}.crt:/etc/kibana/config/certs/${kibana_service_name}.crt\",\n          \"secrets/${kibana_service_name}.key:/etc/kibana/config/certs/${kibana_service_name}.key\",\n          \"secrets/ca.crt:/etc/kibana/config/certs/ca.crt\",\n          \"secrets/ca.key:/etc/kibana/config/certs/ca.key\"\n        ]\n        ulimit {\n          memlock     = \"-1\"\n          nofile      = \"65536\"\n          nproc       = \"8192\"\n        }\n      }\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDQjCCAiqgAwIBAgIULmIAn3JK6TSHotfL1HexLzHBz60wDQYJKoZIhvcNAQEL\nBQAwNDEyMDAGA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5l\ncmF0ZWQgQ0EwHhcNMjEwMTEzMDg0MDQ2WhcNMjQwMTEzMDg0MDQ2WjARMQ8wDQYD\nVQQDEwZraWJhbmEwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCGwE7u\nMO7RfEjfcvtnthZvLfBxEW/CbMfaHa6P2VdmSOi6AYIMZBkYfMEzGJDK8p/lDahm\naXyHc9/Lc+cA0H2XLVgBGZERJirdz3zKkqHLa+BcQxighDJJ3a9zsABoXALsxNgK\niCuFyrOQEEKkM775ZyrdQ6RE39KZLB3GMzkbCRQ/A3HTox8uzgoiXLLvNK60TIgb\n7Sg9v/Zt2ZrFkQu0TsSGouYtYPgvdf9osRznP4QX511HjXV+/P/0Q1abmJU5/WZd\nDk0Sd4TmfscuOiizmO8mrOG2afN4Gc+VYkNzLMVn92DGz/WxP3dKhIhfMyMCqj5M\nJNlCCvVaNm/wujR7AgMBAAGjbzBtMB0GA1UdDgQWBBQKzxXjr7QlQfEdLDo2dNup\noJQKYzAfBgNVHSMEGDAWgBSZKwtpZ3xXQ2hjW+yj0YRsvLb8NTAgBgNVHREEGTAX\nghVraWJhbmEuc2VydmljZS5jb25zdWwwCQYDVR0TBAIwADANBgkqhkiG9w0BAQsF\nAAOCAQEAW1h8U2s4RQoJnbmQOIJYcn8b9+glmOVq09ch5knc22C6VOPaSTUyAMkM\n3glrPnfbvFmSYGNTzXRkZ0m3GI0QSaiVZ8jHQq0unk904+zEqaxT1gMFKc3iv1lP\nDGxMWemP3T9FsEN6Ll9N5YSXP+IonwMEW8mh3PDWDkNZ4haYtbzFyDSNIawljU7G\nn/oqIX0bk7gTaqW789L4GDYWhP1vDldkLBhZOiBIrByaIfHvdYFmhfrXiDMKhkXJ\n6UJ4tFnU8c0xjVk9/uGCuwaUvaweBp9yJQGIknWlH8O/59JjcX7n1/GkGSAtwkH0\n/M+PDfjDTL3FtqY8MYHnkjVItwzftQ==\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/${kibana_service_name}.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEAhsBO7jDu0XxI33L7Z7YWby3wcRFvwmzH2h2uj9lXZkjougGC\nDGQZGHzBMxiQyvKf5Q2oZml8h3Pfy3PnANB9ly1YARmRESYq3c98ypKhy2vgXEMY\noIQySd2vc7AAaFwC7MTYCogrhcqzkBBCpDO++Wcq3UOkRN/SmSwdxjM5GwkUPwNx\n06MfLs4KIlyy7zSutEyIG+0oPb/2bdmaxZELtE7EhqLmLWD4L3X/aLEc5z+EF+dd\nR411fvz/9ENWm5iVOf1mXQ5NEneE5n7HLjoos5jvJqzhtmnzeBnPlWJDcyzFZ/dg\nxs/1sT93SoSIXzMjAqo+TCTZQgr1WjZv8Lo0ewIDAQABAoIBADTV+Nz6gNnREr3S\n1vLuedN0PuAGxzyD7MUAeG7c+KEZm287oiN7qD9qw1JmoneBNOLaPRqS6AowjCK5\nOm2eUnBRjj04KiKARbSdY8AGSLx7ewiSInjl/NXrv5zr+Ozyjw8Ji/BtPiuCtG+b\ngJXj2FDwe+UwXZvH60q1+qK5eP25OYmxTS+KROUDLzKSJwfiyIRwfzTV634KMae7\n9x8FTpR4/xnzPh5A9HaixtrZNZye02TvBvTavzRAABF3B7kFOqxhfJn55aJ8zLnF\nYZ4SDOuIQtt0hZUgQoZ8on4848H2yjW0FRef2A3VCqTRu3J0AQRJS8JPJ31yX+YK\ntyGjYcECgYEA1bFy+uJJ/6g60I9Lu+CQ2Ay/iQo/2TzEYjUa28MPG4KZkkihkfjB\n7cgvZwKvWDEUOGAz2GFDwz6H+1oao+hAB+VidOMkytGazQz6/V9cYqKmZk4kJrw7\n1EHLgkpFfHeG//WBGwKIINj0RLvfXx8ioq8ktrfAil48ozS3toLUHlsCgYEAoW3d\ntANQxTwoAlD5I6rzGBqxwCoA6JegBNsIsob4rB/R3zKJA52utEECI8i449NN6hmQ\n84L6JaodXe3XRlIa1saucIe5jezgRca0etzYviuYOQ0PsEqRMKYKeBo72X6LzEa8\nGZjV+d4rpvo71mmK892V2OpY1WoEzQ58bmj2XGECgYAQYaoO0YoaryrTEikcHfr8\nlP2Z489BOAdV//wvHKTr1vcu36KDLi6vq8j2fJ40hI6oQ7e1vr8TGJgUDLQ+HG/M\nKymBDGilo6vaTERxZ/4NEarv7M2YqpVrkB+pvUfWYtNWi9t51pfY7MjM/BoDkL92\n+TY3S57W/KJpYIE03JKmQQKBgQCQ/1AuSvQX1SrSucyunvRvaDrUsmXSha7z7ZHo\nWZevc31dj9TF7LJpsiKr5bU83iWT6pbqQ3FQt3ZdUi8VONZmqFszNJYUxvnDcvHV\nkd0VI689P2AiJzg2jE3HBzlO6H3FZJu8Gi3InCh1eTqaIn7vAM+B4S0dtHbPgP1/\nZsQywQKBgG4XNt+YyqdDvD406sWzQ2m+C+JQMeINn6QdDdW8rg1xLiTFd7vYYPYD\nj6ohBSQRgnVmMc0Q4efYP0TI978Mf1f5H/BCU+6azR/L6CapqhrDTtYY13iscyuV\nR7AQK4iPNeR6ls92AY2W+PJGIYiByk+7JbOXX/gvPTX7F+/6NsRq\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/${kibana_service_name}.key\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDSjCCAjKgAwIBAgIVANfV55rdCci2edv4n6+zvIQ+TxWoMA0GCSqGSIb3DQEB\nCwUAMDQxMjAwBgNVBAMTKUVsYXN0aWMgQ2VydGlmaWNhdGUgVG9vbCBBdXRvZ2Vu\nZXJhdGVkIENBMB4XDTIxMDExMzA4NDA0NloXDTI0MDExMzA4NDA0NlowNDEyMDAG\nA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5lcmF0ZWQgQ0Ew\nggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCKgXUkoslTKKVeDMsfR9wW\n/jcT1ap576/BXrXK4LHgD8Cd2yGoQkS7aswln5KGM2X6k7qRGN6zMJuZq7yN88dP\nFVmU8/r8fKfhbhpwpus0Y3NyziSTIcZRoendx3I1jktEDcFFcB1yoVdbClh6S4Z+\nMkxHhCwBATvCCmNrwISbLLGyfYkmO10PW8zD/ly1UkSZJ//xfLBtL1u34Q5Aw38r\neBzxBIZRP5hBwnMXUiGo8Kz6ugJ+Qu7YbVL8z1Ie7icIsgjH8mulR23FZjEb5SXm\n0sVchA2vDLPkzdJCI8oDgrbT2KBSkr4u+wuo6VDDQsLWaI+uPhULHwdGQUVinliT\nAgMBAAGjUzBRMB0GA1UdDgQWBBSZKwtpZ3xXQ2hjW+yj0YRsvLb8NTAfBgNVHSME\nGDAWgBSZKwtpZ3xXQ2hjW+yj0YRsvLb8NTAPBgNVHRMBAf8EBTADAQH/MA0GCSqG\nSIb3DQEBCwUAA4IBAQBI9jkC3/B2SYc0v/uIS4AZ/7zvLi/bH+D2PsI9+Rhm7ae5\nrqXlN2eG8xdTKi4WTKLHOESJWeY5NSK6wAT5CPr1gfByzDAfFbnrwTtz4ELnAiw9\nZ4yTU/0ticQ4/d4vJXUPWZ8vqKCi6oTemPhdX0QoO65TKWYZRaaIPgKox+Q4VUP9\nLIUT7NBJn+Z5j+Bbyse0dQwzbKhUEOuQOfMcj7pyKFGeqVUEpRdbpEQ70hlHkwZ7\nHon4+ZsQ95BgCpPCm1rHSy/sJgiJ971CmwVJO8TjBgd4iwCit4x+EcFWsVUra/g0\n+FKz2L0enirRpzXpG4aNI/bSpiR3yLxAMENFkpo0\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/ca.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEpQIBAAKCAQEAioF1JKLJUyilXgzLH0fcFv43E9Wqee+vwV61yuCx4A/Andsh\nqEJEu2rMJZ+ShjNl+pO6kRjeszCbmau8jfPHTxVZlPP6/Hyn4W4acKbrNGNzcs4k\nkyHGUaHp3cdyNY5LRA3BRXAdcqFXWwpYekuGfjJMR4QsAQE7wgpja8CEmyyxsn2J\nJjtdD1vMw/5ctVJEmSf/8XywbS9bt+EOQMN/K3gc8QSGUT+YQcJzF1IhqPCs+roC\nfkLu2G1S/M9SHu4nCLIIx/JrpUdtxWYxG+Ul5tLFXIQNrwyz5M3SQiPKA4K209ig\nUpK+LvsLqOlQw0LC1miPrj4VCx8HRkFFYp5YkwIDAQABAoIBAQCKDA3dvgI7SD/K\nRaYOP2k14Zqzwjpv3l2mtecrllizof+xVj9tnN80jXV76lf4OjJiVeuVwtv0bXYo\n6+q68Uato/HtbF+0V+pb3YmszjGPva/LtXruyrMHmgGmcqt6haCu66a+tsgjAHw4\n2U7mVXBvR2KPxUS2m6wb8o61TuTcY3ATcYb05DN03nB42P6pDt/GVd1QrbKLylOT\nHopsr1ApgxvhPnxC90hgz4wniDI2RYhUgQbasNeC6CuSgjlQVCTl44JUIy/gQf0A\nrLX1lEySapHbLr8oxKpkvmfCh519vc9RnFjBIwzO3SAmNRUduZj+XFYJPgeavOjD\n5qYI88oBAoGBAMHjNew5mwqxAIjekKoJadv+Qz/xwwU+KA63GrDHqPfrs/4WJHJ7\nO0z5xFE5ANMYtC1EIxDfnZ4i3ZlPN3LpdzwPz6veAjUCHYNcf4USvErvawBluHbj\nnegkbjm7FyzVsyYNfkRk8pUrV4Pbw/eb2+mMeAOBCnuFBL8wWxYDzuUhAoGBALbg\nXGwU/zoYzq/xTkChojSJwKXjJovTFNkRgl6OyxdgjIKKogH1e0xYzGLuIk0olUwI\n8xP/77VGqnnE/dFnQ3pX0g7wgDwkXRBONnC0z21cNa5uB5cUKFUi4en61u/eORzF\nDZjLf4+QQD0CtU9MRz/okQ82LxzkLSR/D4BLfVMzAoGBAKcocrbku0yuaZ2W9PYE\nA6ZNQkGA9/gvLG3zYymCGaUVKysmf+nLYMbul1jHYnSc2cok8m57u/I4cQDaER4b\nNlcr8olkcFavKi60sqRSENAyNfgzuqOVffBEaFuRd1uKKlfmTjQ9K/97TIo8EGoL\nj799AYNT32u6tOr4j68dPWTBAoGBAK1iJF4Yvi6fzH5FYzKlzDrRi8P7i9UvqGlx\nT1BFQ8oDMNSniZgf3Olymz0El6Ld4ka3iXchxWvx9rkCir7Zj8FTuAWQAZSDyXQn\nIzhSRQNjVEXvbeTQKLknHFeRCe1bnHxpW03NSkCbvDvb8HihUkAGSFnKvno+34nl\nqZWyfLy/AoGAaonDtHmKHmhNZF35M6LQ81VmWRLw62LeYIdOCE+9PBMQgM8EtJ1Z\n5+F32dIDaP8QCkgYV7AyE1sxQvjYISu5OK2EEyl4aLYm9K0P1eeK6HEXz1HYkZpt\nNdZ6+3q7IGzxoqkmJmoR0bjrAZXH3mTEIsWZJTIK7qM+TM02GL8CHSg=\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/ca.key\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name              = \"${kibana_service_name}\"\n        port              = \"http\"\n        tags              = [ \"${kibana_service_name}$${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Kibana Transport Check Live\"\n          port            = \"http\"\n          type            = \"tcp\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n        check {\n          name            = \"Elastic Kibana HTTP Check Live\"\n          type            = \"http\"\n          port            = \"http\"\n          protocol        = \"https\"\n          method          = \"GET\"\n          tls_skip_verify = true\n          path            = \"/\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu           = ${kibana_cpu}\n        memory        = ${kibana_memory}\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        network {\n          port \"http\" {\n            static    = ${kibana_port}\n          }\n        }\n      }\n    }\n  }\n}\n",
            "vars": {
              "cluster_cpu": "40000",
              "cluster_memory": "40000",
              "cluster_password": "Elastic1234",
              "cluster_rest_port": "9200",
              "cluster_service_name": "elastic",
              "cluster_transport_port": "9300",
              "datacenters": "yul1",
              "group_count": "1",
              "job_name": "prod-elastic",
              "kibana_cpu": "1000",
              "kibana_memory": "8192",
              "kibana_port": "5601",
              "kibana_service_name": "kibana",
              "use_canary": "true",
              "version": "7.10.1"
            }
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.elastic",
      "mode": "managed",
      "type": "nomad_job",
      "name": "nomad_job_beats",
      "provider": "provider[\"registry.terraform.io/hashicorp/nomad\"].yul1",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "allocation_ids": [
              "ee1275de-5d4a-a84b-f196-c4365f0f2d4b",
              "c75cf968-88e9-2a1a-9d53-da34f0e03063",
              "9afca8fd-8753-8d61-f3f0-b50e4097c919",
              "e7d26d7d-a395-c8e3-b9ac-54263db28c74",
              "6670ae27-0f6c-7c1f-ff82-599e16c67753",
              "bbba9f3c-76b9-7f13-566a-6b0a58ccf268",
              "0ba9f3e2-d367-6383-12d9-0cb05a01b01c",
              "8dc55ed6-f682-60d0-9dc1-c9e14eb7c9a0",
              "6f1d8919-fe08-16f4-73d8-46412f6bd7c4",
              "a34b536f-19eb-6878-b21e-041026b93bf3",
              "65e19c01-e74f-f15a-9f0f-bac2fdfe3c7f",
              "c21e7fff-d830-5930-aa4b-95008d6b2e81",
              "5e2ebbd0-102c-ca71-d1ff-ef3edcbb7963",
              "d91acb1b-e803-e2c6-e9e4-2e0e90fc2f4d",
              "e47088e1-8fd1-68ec-6fb4-3dce3e1e2e81",
              "3f1bef8c-bfdf-5c6b-4589-a769cd2fdf37",
              "e89bc585-394b-efb9-c313-797d996d70a6",
              "eb51278e-a846-6866-d942-035b002a44d3",
              "3ce073a6-ecee-910d-cd6c-f40b7002de44",
              "efc27857-8440-a7c9-7058-65b32fede19e"
            ],
            "datacenters": [
              "yul1"
            ],
            "deployment_id": "",
            "deployment_status": "",
            "deregister_on_destroy": true,
            "deregister_on_id_change": true,
            "detach": false,
            "id": "prod-beats",
            "jobspec": "job \"prod-beats\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. For a full list of job types and their differences,\n  # please see the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"system\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 1\n\n    health_check      = \"checks\"\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 1\n\n    # Specifies if the job should auto-promote to the canary version when all\n    # canaries become healthy during a deployment. Defaults to false which means\n    # canaries must be manually updated with the nomad deployment promote\n    # command.\n    auto_promote      = true\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = true\n\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-beats\" {\n\n    restart {\n      interval        = \"1m\"\n      attempts        = 3\n      delay           = \"15s\"\n      mode            = \"delay\"\n    }\n\n    task \"prod-task1-filebeat\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver          = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"docker.elastic.co/beats/filebeat:7.10.1\"\n        dns_servers  = [ \"${attr.unique.network.ip-address}\" ]\n        privileged   = true\n        volumes      = [\n          \"/var/lib/docker/containers:/var/lib/docker/containers\",\n          \"/var/run/docker.sock:/var/run/docker.sock\",\n          \"local/filebeat.yml:/usr/share/filebeat/filebeat.yml\"\n        ]\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data          = \u003c\u003cEOF\nfilebeat.inputs:\n- type: log\n  enabled: true\n  paths:\n    - /var/log/syslog\n    - /var/log/*.log\n- type: container\n  enabled: false\n  paths:\n    - /var/lib/docker/containers/*/*.log\n  stream: all\noutput.elasticsearch:\n  enabled: true\n  hosts: [\"elastic-rest.service.consul:9200\"]\n  compression_level: 0\n  escape_html: true\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  worker: 1\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nsetup.kibana:\n  host: \"kibana.service.consul:5601\"\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  #path: \"\"\n  #space.id: \"\"\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nEOF\n        destination  = \"local/filebeat.yml\"\n      }\n    }\n\n    task \"prod-task1-metricbeat\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver         = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"docker.elastic.co/beats/metricbeat:7.10.1\"\n        dns_servers  = [ \"${attr.unique.network.ip-address}\" ]\n        privileged   = true\n#        command      = \"metricbeat\"\n        args = [\n          \"-e\", \"-system.hostfs=/hostfs\"\n        ]\n        volumes      = [\n          \"/:/hostfs:ro\",\n          \"local/metricbeat.yml:/usr/share/metricbeat/metricbeat.yml\"\n        ]\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\nmetricbeat.config.modules:\n  path: ${path.config}/modules.d/*.yml\n  reload.period: 10s\n  reload.enabled: false\nmetricbeat.max_start_delay: 10s\nmetricbeat.modules:\n- module: system\n  metricsets:\n    - cpu\n    - load\n    - memory\n    - network\n    - process\n    - process_summary\n    - uptime\n    - socket_summary\n    - core\n    - diskio\n    #- filesystem\n    - fsstat\n    #- raid\n    #- socket\n    - service\n  enabled: true\n  period: 10s\n  processes: ['.*']\n  cpu.metrics:  [\"percentages\",\"normalized_percentages\"]\n  core.metrics: [\"percentages\"]\n- module: consul\n  metricsets:\n  - agent\n  enabled: true\n  period: 10s\n  hosts: [\"localhost:8500\"]\n- module: docker\n  metricsets:\n    - \"container\"\n    - \"cpu\"\n    - \"diskio\"\n    - \"event\"\n    - \"healthcheck\"\n    - \"info\"\n    - \"image\"\n    - \"memory\"\n    - \"network\"\n  hosts: [\"unix:///var/run/docker.sock\"]\n  period: 10s\n  enabled: true\noutput.elasticsearch:\n  enabled: true\n  hosts: [\"elastic-rest.service.consul:9200\"]\n  compression_level: 0\n  escape_html: true\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  worker: 1\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nsetup.kibana:\n  host: \"elastic-kibana.service.consul:5601\"\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  #path: \"\"\n  #space.id: \"\"\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nEOF\n        destination  = \"local/metricbeat.yml\"\n      }\n    }\n\n    task \"prod-task1-packetbeat\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver         = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"docker.elastic.co/beats/packetbeat:7.10.1\"\n        dns_servers  = [ \"${attr.unique.network.ip-address}\" ]\n        privileged   = true\n\n        volumes      = [\n          \"local/packetbeat.yml:/usr/share/packetbeat/packetbeat.yml\"\n        ]\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\npacketbeat.interfaces.device: any\npacketbeat.flows:\n  enabled: true\n  timeout: 30s\n  period: 10s\npacketbeat.protocols:\n- type: icmp\n  enabled: true\n- type: dhcpv4\n  enabled: true\n  ports: [67, 68]\n- type: dns\n  enabled: true\n  ports: [53]\n  include_authorities: true\n  include_additionals: true\n- type: http\n  enabled: true\n  ports: [80, 8080, 8000, 5000, 8002]\n- type: tls\n  enabled: true\n  ports: [443, 8443, 9243]\npacketbeat.procs.enabled: false\npacketbeat.ignore_outgoing: false\noutput.elasticsearch:\n  enabled: true\n  hosts: [\"elastic-rest.service.consul:9200\"]\n  compression_level: 0\n  escape_html: true\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  worker: 1\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nsetup.kibana:\n  host: \"kibana.service.consul:5601\"\n  protocol: \"http\"\n  #username: \"elastic\"\n  #password: \"changeme\"\n  #path: \"\"\n  #space.id: \"\"\n  #ssl.enabled: true\n  #ssl.verification_mode: none\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\n  #ssl.key: \"/etc/pki/client/cert.key\"\nEOF\n        destination  = \"local/packetbeat.yml\"\n      }\n    }\n  }\n}",
            "json": null,
            "modify_index": "6226242",
            "name": "prod-beats",
            "namespace": "default",
            "policy_override": null,
            "purge_on_destroy": null,
            "region": "global",
            "task_groups": [
              {
                "count": 1,
                "meta": {},
                "name": "prod-group1-beats",
                "task": [
                  {
                    "driver": "docker",
                    "meta": {},
                    "name": "prod-task1-filebeat",
                    "volume_mounts": []
                  },
                  {
                    "driver": "docker",
                    "meta": {},
                    "name": "prod-task1-metricbeat",
                    "volume_mounts": []
                  },
                  {
                    "driver": "docker",
                    "meta": {},
                    "name": "prod-task1-packetbeat",
                    "volume_mounts": []
                  }
                ],
                "volumes": []
              }
            ],
            "type": "system"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.elastic.data.template_file.nomad_job_beats"
          ]
        }
      ]
    },
    {
      "module": "module.elastic",
      "mode": "managed",
      "type": "nomad_job",
      "name": "nomad_job_elastic",
      "provider": "provider[\"registry.terraform.io/hashicorp/nomad\"].yul1",
      "instances": [
        {
          "status": "tainted",
          "schema_version": 0,
          "attributes": {
            "allocation_ids": null,
            "datacenters": [
              "yul1"
            ],
            "deployment_id": null,
            "deployment_status": null,
            "deregister_on_destroy": true,
            "deregister_on_id_change": true,
            "detach": false,
            "id": "prod-elastic",
            "jobspec": "job \"prod-elastic\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 1\n\n    health_check      = \"checks\"\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 1\n\n    # Specifies if the job should auto-promote to the canary version when all\n    # canaries become healthy during a deployment. Defaults to false which means\n    # canaries must be manually updated with the nomad deployment promote\n    # command.\n    auto_promote      = true\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = true\n\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-elastic-cluster\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count             = 1\n\n    constraint {\n      attribute       = \"${node.unique.name}\"\n      value           = \"s41-nomad-x86_64\"\n    }\n\n    ephemeral_disk {\n      size            = \"50000\"\n      sticky          = true\n      migrate         = false\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-elastic-cluster\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver          = \"docker\"\n\n      kill_timeout    = \"600s\"\n      kill_signal     = \"SIGTERM\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image         = \"docker.elastic.co/elasticsearch/elasticsearch:7.10.1\"\n        dns_servers   = [ \"${attr.unique.network.ip-address}\" ]\n        command       = \"elasticsearch\"\n        args          = [\n          \"-Enode.name=elastic${NOMAD_ALLOC_INDEX}\",\n          \"-Enetwork.host=0.0.0.0\",\n          \"-Ecluster.name=elastic\",\n          \"-Ehttp.port=${NOMAD_PORT_rest}\",\n          \"-Ehttp.publish_port=${NOMAD_HOST_PORT_rest}\",\n          \"-Ebootstrap.memory_lock=true\",\n          \"-Epath.logs=/alloc/logs/\",\n          \"-Ediscovery.type=single-node\",\n          \"-Etransport.publish_port=${NOMAD_HOST_PORT_transport}\",\n          \"-Etransport.port=${NOMAD_PORT_transport}\",\n          \"-Expack.license.self_generated.type=basic\",\n          \"-Expack.security.enabled=true\",\n          \"-Expack.security.http.ssl.enabled=true\",\n          \"-Expack.security.http.ssl.key=certs/elastic${NOMAD_ALLOC_INDEX}.key\",\n          \"-Expack.security.http.ssl.certificate=certs/elastic${NOMAD_ALLOC_INDEX}.crt\",\n          \"-Expack.security.http.ssl.certificate_authorities=certs/ca.crt\",\n          \"-Expack.security.transport.ssl.enabled=true\",\n          \"-Expack.security.transport.ssl.key=certs/elastic${NOMAD_ALLOC_INDEX}.key\",\n          \"-Expack.security.transport.ssl.certificate=certs/elastic${NOMAD_ALLOC_INDEX}.crt\",\n          \"-Expack.security.transport.ssl.certificate_authorities=certs/ca.crt\",\n          \"-Expack.security.transport.ssl.verification_mode=certificate\"\n        ]\n        volumes       = [\n          \"secrets/elastic${NOMAD_ALLOC_INDEX}.crt:/usr/share/elasticsearch/config/certs/elastic${NOMAD_ALLOC_INDEX}.crt\",\n          \"secrets/elastic${NOMAD_ALLOC_INDEX}.key:/usr/share/elasticsearch/config/certs/elastic${NOMAD_ALLOC_INDEX}.key\",\n          \"secrets/ca.crt:/usr/share/elasticsearch/config/certs/ca.crt\",\n          \"secrets/ca.key:/usr/share/elasticsearch/config/certs/ca.key\",\n          \"secrets/password:/usr/share/elasticsearch/config/password\"\n        ]\n        ulimit {\n          memlock     = \"-1\"\n          nofile      = \"65536\"\n          nproc       = \"8192\"\n        }\n      }\n\n      # The env stanza configures a list of environment variables to populate\n      # the task's environment before starting.\n      env {\n        ELASTIC_PASSWORD       = \"Elastic1234\"\n#        ELASTIC_PASSWORD_FILE  = \"/usr/share/elasticsearch/config/password\"\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\nElastic1234\nEOF\n        destination  = \"secrets/password\"\n        perms        = \"600\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDRjCCAi6gAwIBAgIUKHjku5RBb4UD9nm+Fg2RxR3z5mIwDQYJKoZIhvcNAQEL\nBQAwNDEyMDAGA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5l\ncmF0ZWQgQ0EwHhcNMjEwMTEzMDg0MDQ2WhcNMjQwMTEzMDg0MDQ2WjATMREwDwYD\nVQQDEwhlbGFzdGljMTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMtu\nBC5mFsyuq8RsaRpd8KyRaOPdRxqLiqLUONcc76XtvyGQT/87av2l5cuzniT/QZ0s\nVRpMeZWoRLF1SWS6YCCi+ohnaSun0tWXBdE5fdLv6yuNosAxgD7moIRSjSQ1loKm\nZEM6m0csVG/sh5J4J2APcJJjKPdOCf2szl/4Ct4VbutCaBex7E3ZmB2T1/adHh9F\nOkGZYj5FntsDL5eycYKmWOVDf/j24AMlCWzuucdZ/H0XZQ8OKou/Ut8v7hh5w2FT\n11/5uw/1C2Q4iVJnBWL6BADtxef0f6kCg3epXo35IIsmKpKecnW377kaO348SFmd\nBsIlPcCUUgXDQs/pJ+UCAwEAAaNxMG8wHQYDVR0OBBYEFCDsjcPITceLLpr2U8Xj\ncYLkKh6vMB8GA1UdIwQYMBaAFJkrC2lnfFdDaGNb7KPRhGy8tvw1MCIGA1UdEQQb\nMBmCF2VsYXN0aWMxLnNlcnZpY2UuY29uc3VsMAkGA1UdEwQCMAAwDQYJKoZIhvcN\nAQELBQADggEBADkT03usfJNn5G2Ldq9UBb2D5WTzcZ+OXdqcH1iTT8+0SwnqVvro\nDO2aE98XLqK/v2mlmU9ohrEsxhJAAHzc0wmL+kHk98z5EKAkXjj7hLtEyaTWLFGH\nV+Z1Oi+kTjukEDbHt2IP8/8HibjEWXqqbhFVQqmdON9axTtyO6syEonjBeLEO/89\n6Ki9GFmHKJp6WCpzRg0i5a1stPTo5cE+QJUiJjx5WmCXocakAAdORS3ES6gRYKW5\nKdD5AOlR6NNaewCZh0lNjZ3SmUUzSZ+HDJXi+BE4k1PJbyQ8Lav7rq5MXHtnV4kX\n4SC56vJ0z2g7/SonGTOJGWAbPGm6oK6Cu64=\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/elastic${NOMAD_ALLOC_INDEX}.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEpAIBAAKCAQEAy24ELmYWzK6rxGxpGl3wrJFo491HGouKotQ41xzvpe2/IZBP\n/ztq/aXly7OeJP9BnSxVGkx5lahEsXVJZLpgIKL6iGdpK6fS1ZcF0Tl90u/rK42i\nwDGAPuaghFKNJDWWgqZkQzqbRyxUb+yHkngnYA9wkmMo904J/azOX/gK3hVu60Jo\nF7HsTdmYHZPX9p0eH0U6QZliPkWe2wMvl7JxgqZY5UN/+PbgAyUJbO65x1n8fRdl\nDw4qi79S3y/uGHnDYVPXX/m7D/ULZDiJUmcFYvoEAO3F5/R/qQKDd6lejfkgiyYq\nkp5ydbfvuRo7fjxIWZ0GwiU9wJRSBcNCz+kn5QIDAQABAoIBAQCmz7QGCBiyBpk7\nHFqjEF0GZMZJ820Wy04Hb1acrlGlEmskLp4qgKKfE6Z3fvYzCEzZgTzXr9YTbkPF\n8JMaUen5WStvJr0K2zb7hjdy9V3D1pBUynOmffDXo24Ek1zBUF/3ClI0/p3NowAq\nNx6EcJp5HrAEmeNBx3BR353q/A6NRDd3/hbnZi+SfySI6eT4IfelKzhDSbjeuJwY\nysIukB6EBpJmhY7Zh9aK5QfPLA4hNyiLu04IhnwRkRh2s8G1Yckfdr/O3JmL+W1E\nrbNYF1Nxbr3aU22JFzoe1H2uSvaeQYzk2SNMCGhrF84hmyPXctp34Ao+H/dVBZZc\n4heI+G2BAoGBAO3MMzJmami4keA3Lo4N7i1jYQun+heTM0m43M/QKqICy/Rp/g6M\nChedNyRTUHWIsnt8QUz9L1HLHKuYmyfiKfdkt06uVDsQK4lbzbjsoCe0DYnxkXhE\no+mW5nUzRz2DL0orM7VZT64KVtTrTFtoxjm0pyGm0ayASOSPY9wHl5cZAoGBANsA\nWtfXSc4yt11QSkd6tGQdRCw+kcMXhiI2aRbtTha0yComXRo8SNXzvIfE5tIZ2oFG\ne2Kc1YHOPCEhq884ODbW8N+iBQnRst1yhM34Fc/TsFzTMuEGDXqyyRbPNn1MmkMI\nSSf4cmW9LhvE3Iph6jSgYB1nMRcptd6/+pS/NeytAoGAR8cpdP8hA3ci4TEG5m4i\nBKVIt8H+ZXtTMd+RF1FYbQq3EZGk1DNFIJed+2MCmFeouElrVJff3qqWft1TiBhm\nXnySMDfCyQk6ev2w/S6/sPxSUd8O7+SYLXwVGC9gQ5sDfTnJI+ZPfNM2HpLfu3/G\nxchX4np+M7mNRyBZHiNUiJECgYEAhjDAeTMMoVFIM+BHs2bHc/TO2gF41T7rzLjk\nSc0cpSMe51zcfX/k7VxM8DBBcwmubroeTn1lAgW5qF92ZCHBqDCqJY2kYrDgVXqf\nT4ms68x9a1NqAKHxznYQa26Kp9oxR9Oi59//UMHLp+5HaG+4z4hZfIrHdLb1Hskp\npM1JIH0CgYAwoIef0P/STykVI+YqK+06sEsEMjFPSg72fLimydSIAjMLc1xvnJYm\n4YnyNVtIqP4LmgQ3vKnz5yzHpsTWEeUAZ2d7UHE1gC5ZpV4DZvZIEzfPLU+cuQ+C\npFg9P7CZoeBBAHzVURVcinBkME7b+7RD5YkgJUr3+9KrSlisUMdvFg==\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/elastic${NOMAD_ALLOC_INDEX}.key\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDSjCCAjKgAwIBAgIVANfV55rdCci2edv4n6+zvIQ+TxWoMA0GCSqGSIb3DQEB\nCwUAMDQxMjAwBgNVBAMTKUVsYXN0aWMgQ2VydGlmaWNhdGUgVG9vbCBBdXRvZ2Vu\nZXJhdGVkIENBMB4XDTIxMDExMzA4NDA0NloXDTI0MDExMzA4NDA0NlowNDEyMDAG\nA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5lcmF0ZWQgQ0Ew\nggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCKgXUkoslTKKVeDMsfR9wW\n/jcT1ap576/BXrXK4LHgD8Cd2yGoQkS7aswln5KGM2X6k7qRGN6zMJuZq7yN88dP\nFVmU8/r8fKfhbhpwpus0Y3NyziSTIcZRoendx3I1jktEDcFFcB1yoVdbClh6S4Z+\nMkxHhCwBATvCCmNrwISbLLGyfYkmO10PW8zD/ly1UkSZJ//xfLBtL1u34Q5Aw38r\neBzxBIZRP5hBwnMXUiGo8Kz6ugJ+Qu7YbVL8z1Ie7icIsgjH8mulR23FZjEb5SXm\n0sVchA2vDLPkzdJCI8oDgrbT2KBSkr4u+wuo6VDDQsLWaI+uPhULHwdGQUVinliT\nAgMBAAGjUzBRMB0GA1UdDgQWBBSZKwtpZ3xXQ2hjW+yj0YRsvLb8NTAfBgNVHSME\nGDAWgBSZKwtpZ3xXQ2hjW+yj0YRsvLb8NTAPBgNVHRMBAf8EBTADAQH/MA0GCSqG\nSIb3DQEBCwUAA4IBAQBI9jkC3/B2SYc0v/uIS4AZ/7zvLi/bH+D2PsI9+Rhm7ae5\nrqXlN2eG8xdTKi4WTKLHOESJWeY5NSK6wAT5CPr1gfByzDAfFbnrwTtz4ELnAiw9\nZ4yTU/0ticQ4/d4vJXUPWZ8vqKCi6oTemPhdX0QoO65TKWYZRaaIPgKox+Q4VUP9\nLIUT7NBJn+Z5j+Bbyse0dQwzbKhUEOuQOfMcj7pyKFGeqVUEpRdbpEQ70hlHkwZ7\nHon4+ZsQ95BgCpPCm1rHSy/sJgiJ971CmwVJO8TjBgd4iwCit4x+EcFWsVUra/g0\n+FKz2L0enirRpzXpG4aNI/bSpiR3yLxAMENFkpo0\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/ca.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEpQIBAAKCAQEAioF1JKLJUyilXgzLH0fcFv43E9Wqee+vwV61yuCx4A/Andsh\nqEJEu2rMJZ+ShjNl+pO6kRjeszCbmau8jfPHTxVZlPP6/Hyn4W4acKbrNGNzcs4k\nkyHGUaHp3cdyNY5LRA3BRXAdcqFXWwpYekuGfjJMR4QsAQE7wgpja8CEmyyxsn2J\nJjtdD1vMw/5ctVJEmSf/8XywbS9bt+EOQMN/K3gc8QSGUT+YQcJzF1IhqPCs+roC\nfkLu2G1S/M9SHu4nCLIIx/JrpUdtxWYxG+Ul5tLFXIQNrwyz5M3SQiPKA4K209ig\nUpK+LvsLqOlQw0LC1miPrj4VCx8HRkFFYp5YkwIDAQABAoIBAQCKDA3dvgI7SD/K\nRaYOP2k14Zqzwjpv3l2mtecrllizof+xVj9tnN80jXV76lf4OjJiVeuVwtv0bXYo\n6+q68Uato/HtbF+0V+pb3YmszjGPva/LtXruyrMHmgGmcqt6haCu66a+tsgjAHw4\n2U7mVXBvR2KPxUS2m6wb8o61TuTcY3ATcYb05DN03nB42P6pDt/GVd1QrbKLylOT\nHopsr1ApgxvhPnxC90hgz4wniDI2RYhUgQbasNeC6CuSgjlQVCTl44JUIy/gQf0A\nrLX1lEySapHbLr8oxKpkvmfCh519vc9RnFjBIwzO3SAmNRUduZj+XFYJPgeavOjD\n5qYI88oBAoGBAMHjNew5mwqxAIjekKoJadv+Qz/xwwU+KA63GrDHqPfrs/4WJHJ7\nO0z5xFE5ANMYtC1EIxDfnZ4i3ZlPN3LpdzwPz6veAjUCHYNcf4USvErvawBluHbj\nnegkbjm7FyzVsyYNfkRk8pUrV4Pbw/eb2+mMeAOBCnuFBL8wWxYDzuUhAoGBALbg\nXGwU/zoYzq/xTkChojSJwKXjJovTFNkRgl6OyxdgjIKKogH1e0xYzGLuIk0olUwI\n8xP/77VGqnnE/dFnQ3pX0g7wgDwkXRBONnC0z21cNa5uB5cUKFUi4en61u/eORzF\nDZjLf4+QQD0CtU9MRz/okQ82LxzkLSR/D4BLfVMzAoGBAKcocrbku0yuaZ2W9PYE\nA6ZNQkGA9/gvLG3zYymCGaUVKysmf+nLYMbul1jHYnSc2cok8m57u/I4cQDaER4b\nNlcr8olkcFavKi60sqRSENAyNfgzuqOVffBEaFuRd1uKKlfmTjQ9K/97TIo8EGoL\nj799AYNT32u6tOr4j68dPWTBAoGBAK1iJF4Yvi6fzH5FYzKlzDrRi8P7i9UvqGlx\nT1BFQ8oDMNSniZgf3Olymz0El6Ld4ka3iXchxWvx9rkCir7Zj8FTuAWQAZSDyXQn\nIzhSRQNjVEXvbeTQKLknHFeRCe1bnHxpW03NSkCbvDvb8HihUkAGSFnKvno+34nl\nqZWyfLy/AoGAaonDtHmKHmhNZF35M6LQ81VmWRLw62LeYIdOCE+9PBMQgM8EtJ1Z\n5+F32dIDaP8QCkgYV7AyE1sxQvjYISu5OK2EEyl4aLYm9K0P1eeK6HEXz1HYkZpt\nNdZ6+3q7IGzxoqkmJmoR0bjrAZXH3mTEIsWZJTIK7qM+TM02GL8CHSg=\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/ca.key\"\n\n#        mounts        = [\n#          {\n#            type     = \"volume\"\n#            target   = \"/usr/share/elasticsearch/data/\"\n#            source   = \"es-cluster-cluster-vol\"\n#            readonly = false\n#          }\n#        ]\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name              = \"elastic\"\n        port              = \"rest\"\n        tags              = [ \"elastic${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Cluster REST Check Live\"\n          port            = \"rest\"\n          type            = \"tcp\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n        check {\n          name            = \"Elastic Cluster HTTP Check Live\"\n          type            = \"http\"\n          port            = \"rest\"\n          protocol        = \"https\"\n          method          = \"GET\"\n          header {\n            Authorization = [\"Basic ZWxhc3RpYzpFbGFzdGljMTIzNA==\"]\n          }\n          tls_skip_verify = true\n          path            = \"/_cluster/health?pretty\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n      service {\n        name              = \"elastic-transport\"\n        port              = \"transport\"\n        tags              = [ \"elastic-transport${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Cluster Transport Check Live\"\n          type            = \"tcp\"\n          port            = \"transport\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu        = 40000\n        memory     = 40000\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        network {\n          port \"rest\" {\n            static = 9200\n          }\n          port \"transport\" {\n            static = 9300\n          }\n        }\n      }\n    }\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-elastic-kibana\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count              = 1\n\n    constraint {\n      attribute        = \"${node.unique.name}\"\n      value            = \"s41-nomad-x86_64\"\n    }\n\n    update {\n      max_parallel     = 1\n      health_check     = \"checks\"\n      min_healthy_time = \"10s\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-elastic-task\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver           = \"docker\"\n\n      kill_timeout     = \"60s\"\n      kill_signal      = \"SIGTERM\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image         = \"docker.elastic.co/kibana/kibana:7.10.1\"\n        dns_servers   = [ \"${attr.unique.network.ip-address}\" ]\n        command       = \"kibana\"\n        args          = [\n          \"--server.name=kibana\",\n          \"--server.host=0.0.0.0\",\n          \"--server.port=${NOMAD_PORT_http}\",\n          \"--server.ssl.enabled=true\",\n          \"--server.ssl.certificate=/etc/kibana/config/certs/kibana.crt\",\n          \"--server.ssl.key=/etc/kibana/config/certs/kibana.key\",\n          \"--elasticsearch.hosts=http://elastic.service.consul:9200\",\n          \"--elasticsearch.username=kibanauser\",\n          \"--elasticsearch.password=Kibana1234\",\n          \"--elasticsearch.ssl.certificateAuthorities=/etc/kibana/config/certs/ca.crt\",\n          \"--xpack.apm.ui.enabled=false\",\n          \"--xpack.graph.enabled=false\",\n          \"--xpack.grokdebugger.enabled=false\",\n          \"--xpack.maps.enabled=false\",\n          \"--xpack.ml.enabled=false\",\n          \"--xpack.searchprofiler.enabled=false\"\n        ]\n        volumes       = [\n          \"secrets/kibana.crt:/etc/kibana/config/certs/kibana.crt\",\n          \"secrets/kibana.key:/etc/kibana/config/certs/kibana.key\",\n          \"secrets/ca.crt:/etc/kibana/config/certs/ca.crt\",\n          \"secrets/ca.key:/etc/kibana/config/certs/ca.key\"\n        ]\n        ulimit {\n          memlock     = \"-1\"\n          nofile      = \"65536\"\n          nproc       = \"8192\"\n        }\n      }\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDQjCCAiqgAwIBAgIULmIAn3JK6TSHotfL1HexLzHBz60wDQYJKoZIhvcNAQEL\nBQAwNDEyMDAGA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5l\ncmF0ZWQgQ0EwHhcNMjEwMTEzMDg0MDQ2WhcNMjQwMTEzMDg0MDQ2WjARMQ8wDQYD\nVQQDEwZraWJhbmEwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCGwE7u\nMO7RfEjfcvtnthZvLfBxEW/CbMfaHa6P2VdmSOi6AYIMZBkYfMEzGJDK8p/lDahm\naXyHc9/Lc+cA0H2XLVgBGZERJirdz3zKkqHLa+BcQxighDJJ3a9zsABoXALsxNgK\niCuFyrOQEEKkM775ZyrdQ6RE39KZLB3GMzkbCRQ/A3HTox8uzgoiXLLvNK60TIgb\n7Sg9v/Zt2ZrFkQu0TsSGouYtYPgvdf9osRznP4QX511HjXV+/P/0Q1abmJU5/WZd\nDk0Sd4TmfscuOiizmO8mrOG2afN4Gc+VYkNzLMVn92DGz/WxP3dKhIhfMyMCqj5M\nJNlCCvVaNm/wujR7AgMBAAGjbzBtMB0GA1UdDgQWBBQKzxXjr7QlQfEdLDo2dNup\noJQKYzAfBgNVHSMEGDAWgBSZKwtpZ3xXQ2hjW+yj0YRsvLb8NTAgBgNVHREEGTAX\nghVraWJhbmEuc2VydmljZS5jb25zdWwwCQYDVR0TBAIwADANBgkqhkiG9w0BAQsF\nAAOCAQEAW1h8U2s4RQoJnbmQOIJYcn8b9+glmOVq09ch5knc22C6VOPaSTUyAMkM\n3glrPnfbvFmSYGNTzXRkZ0m3GI0QSaiVZ8jHQq0unk904+zEqaxT1gMFKc3iv1lP\nDGxMWemP3T9FsEN6Ll9N5YSXP+IonwMEW8mh3PDWDkNZ4haYtbzFyDSNIawljU7G\nn/oqIX0bk7gTaqW789L4GDYWhP1vDldkLBhZOiBIrByaIfHvdYFmhfrXiDMKhkXJ\n6UJ4tFnU8c0xjVk9/uGCuwaUvaweBp9yJQGIknWlH8O/59JjcX7n1/GkGSAtwkH0\n/M+PDfjDTL3FtqY8MYHnkjVItwzftQ==\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/kibana.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEAhsBO7jDu0XxI33L7Z7YWby3wcRFvwmzH2h2uj9lXZkjougGC\nDGQZGHzBMxiQyvKf5Q2oZml8h3Pfy3PnANB9ly1YARmRESYq3c98ypKhy2vgXEMY\noIQySd2vc7AAaFwC7MTYCogrhcqzkBBCpDO++Wcq3UOkRN/SmSwdxjM5GwkUPwNx\n06MfLs4KIlyy7zSutEyIG+0oPb/2bdmaxZELtE7EhqLmLWD4L3X/aLEc5z+EF+dd\nR411fvz/9ENWm5iVOf1mXQ5NEneE5n7HLjoos5jvJqzhtmnzeBnPlWJDcyzFZ/dg\nxs/1sT93SoSIXzMjAqo+TCTZQgr1WjZv8Lo0ewIDAQABAoIBADTV+Nz6gNnREr3S\n1vLuedN0PuAGxzyD7MUAeG7c+KEZm287oiN7qD9qw1JmoneBNOLaPRqS6AowjCK5\nOm2eUnBRjj04KiKARbSdY8AGSLx7ewiSInjl/NXrv5zr+Ozyjw8Ji/BtPiuCtG+b\ngJXj2FDwe+UwXZvH60q1+qK5eP25OYmxTS+KROUDLzKSJwfiyIRwfzTV634KMae7\n9x8FTpR4/xnzPh5A9HaixtrZNZye02TvBvTavzRAABF3B7kFOqxhfJn55aJ8zLnF\nYZ4SDOuIQtt0hZUgQoZ8on4848H2yjW0FRef2A3VCqTRu3J0AQRJS8JPJ31yX+YK\ntyGjYcECgYEA1bFy+uJJ/6g60I9Lu+CQ2Ay/iQo/2TzEYjUa28MPG4KZkkihkfjB\n7cgvZwKvWDEUOGAz2GFDwz6H+1oao+hAB+VidOMkytGazQz6/V9cYqKmZk4kJrw7\n1EHLgkpFfHeG//WBGwKIINj0RLvfXx8ioq8ktrfAil48ozS3toLUHlsCgYEAoW3d\ntANQxTwoAlD5I6rzGBqxwCoA6JegBNsIsob4rB/R3zKJA52utEECI8i449NN6hmQ\n84L6JaodXe3XRlIa1saucIe5jezgRca0etzYviuYOQ0PsEqRMKYKeBo72X6LzEa8\nGZjV+d4rpvo71mmK892V2OpY1WoEzQ58bmj2XGECgYAQYaoO0YoaryrTEikcHfr8\nlP2Z489BOAdV//wvHKTr1vcu36KDLi6vq8j2fJ40hI6oQ7e1vr8TGJgUDLQ+HG/M\nKymBDGilo6vaTERxZ/4NEarv7M2YqpVrkB+pvUfWYtNWi9t51pfY7MjM/BoDkL92\n+TY3S57W/KJpYIE03JKmQQKBgQCQ/1AuSvQX1SrSucyunvRvaDrUsmXSha7z7ZHo\nWZevc31dj9TF7LJpsiKr5bU83iWT6pbqQ3FQt3ZdUi8VONZmqFszNJYUxvnDcvHV\nkd0VI689P2AiJzg2jE3HBzlO6H3FZJu8Gi3InCh1eTqaIn7vAM+B4S0dtHbPgP1/\nZsQywQKBgG4XNt+YyqdDvD406sWzQ2m+C+JQMeINn6QdDdW8rg1xLiTFd7vYYPYD\nj6ohBSQRgnVmMc0Q4efYP0TI978Mf1f5H/BCU+6azR/L6CapqhrDTtYY13iscyuV\nR7AQK4iPNeR6ls92AY2W+PJGIYiByk+7JbOXX/gvPTX7F+/6NsRq\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/kibana.key\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN CERTIFICATE-----\nMIIDSjCCAjKgAwIBAgIVANfV55rdCci2edv4n6+zvIQ+TxWoMA0GCSqGSIb3DQEB\nCwUAMDQxMjAwBgNVBAMTKUVsYXN0aWMgQ2VydGlmaWNhdGUgVG9vbCBBdXRvZ2Vu\nZXJhdGVkIENBMB4XDTIxMDExMzA4NDA0NloXDTI0MDExMzA4NDA0NlowNDEyMDAG\nA1UEAxMpRWxhc3RpYyBDZXJ0aWZpY2F0ZSBUb29sIEF1dG9nZW5lcmF0ZWQgQ0Ew\nggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCKgXUkoslTKKVeDMsfR9wW\n/jcT1ap576/BXrXK4LHgD8Cd2yGoQkS7aswln5KGM2X6k7qRGN6zMJuZq7yN88dP\nFVmU8/r8fKfhbhpwpus0Y3NyziSTIcZRoendx3I1jktEDcFFcB1yoVdbClh6S4Z+\nMkxHhCwBATvCCmNrwISbLLGyfYkmO10PW8zD/ly1UkSZJ//xfLBtL1u34Q5Aw38r\neBzxBIZRP5hBwnMXUiGo8Kz6ugJ+Qu7YbVL8z1Ie7icIsgjH8mulR23FZjEb5SXm\n0sVchA2vDLPkzdJCI8oDgrbT2KBSkr4u+wuo6VDDQsLWaI+uPhULHwdGQUVinliT\nAgMBAAGjUzBRMB0GA1UdDgQWBBSZKwtpZ3xXQ2hjW+yj0YRsvLb8NTAfBgNVHSME\nGDAWgBSZKwtpZ3xXQ2hjW+yj0YRsvLb8NTAPBgNVHRMBAf8EBTADAQH/MA0GCSqG\nSIb3DQEBCwUAA4IBAQBI9jkC3/B2SYc0v/uIS4AZ/7zvLi/bH+D2PsI9+Rhm7ae5\nrqXlN2eG8xdTKi4WTKLHOESJWeY5NSK6wAT5CPr1gfByzDAfFbnrwTtz4ELnAiw9\nZ4yTU/0ticQ4/d4vJXUPWZ8vqKCi6oTemPhdX0QoO65TKWYZRaaIPgKox+Q4VUP9\nLIUT7NBJn+Z5j+Bbyse0dQwzbKhUEOuQOfMcj7pyKFGeqVUEpRdbpEQ70hlHkwZ7\nHon4+ZsQ95BgCpPCm1rHSy/sJgiJ971CmwVJO8TjBgd4iwCit4x+EcFWsVUra/g0\n+FKz2L0enirRpzXpG4aNI/bSpiR3yLxAMENFkpo0\n-----END CERTIFICATE-----\nEOF\n        destination  = \"secrets/ca.crt\"\n      }\n      template {\n        data         = \u003c\u003cEOF\n-----BEGIN RSA PRIVATE KEY-----\nMIIEpQIBAAKCAQEAioF1JKLJUyilXgzLH0fcFv43E9Wqee+vwV61yuCx4A/Andsh\nqEJEu2rMJZ+ShjNl+pO6kRjeszCbmau8jfPHTxVZlPP6/Hyn4W4acKbrNGNzcs4k\nkyHGUaHp3cdyNY5LRA3BRXAdcqFXWwpYekuGfjJMR4QsAQE7wgpja8CEmyyxsn2J\nJjtdD1vMw/5ctVJEmSf/8XywbS9bt+EOQMN/K3gc8QSGUT+YQcJzF1IhqPCs+roC\nfkLu2G1S/M9SHu4nCLIIx/JrpUdtxWYxG+Ul5tLFXIQNrwyz5M3SQiPKA4K209ig\nUpK+LvsLqOlQw0LC1miPrj4VCx8HRkFFYp5YkwIDAQABAoIBAQCKDA3dvgI7SD/K\nRaYOP2k14Zqzwjpv3l2mtecrllizof+xVj9tnN80jXV76lf4OjJiVeuVwtv0bXYo\n6+q68Uato/HtbF+0V+pb3YmszjGPva/LtXruyrMHmgGmcqt6haCu66a+tsgjAHw4\n2U7mVXBvR2KPxUS2m6wb8o61TuTcY3ATcYb05DN03nB42P6pDt/GVd1QrbKLylOT\nHopsr1ApgxvhPnxC90hgz4wniDI2RYhUgQbasNeC6CuSgjlQVCTl44JUIy/gQf0A\nrLX1lEySapHbLr8oxKpkvmfCh519vc9RnFjBIwzO3SAmNRUduZj+XFYJPgeavOjD\n5qYI88oBAoGBAMHjNew5mwqxAIjekKoJadv+Qz/xwwU+KA63GrDHqPfrs/4WJHJ7\nO0z5xFE5ANMYtC1EIxDfnZ4i3ZlPN3LpdzwPz6veAjUCHYNcf4USvErvawBluHbj\nnegkbjm7FyzVsyYNfkRk8pUrV4Pbw/eb2+mMeAOBCnuFBL8wWxYDzuUhAoGBALbg\nXGwU/zoYzq/xTkChojSJwKXjJovTFNkRgl6OyxdgjIKKogH1e0xYzGLuIk0olUwI\n8xP/77VGqnnE/dFnQ3pX0g7wgDwkXRBONnC0z21cNa5uB5cUKFUi4en61u/eORzF\nDZjLf4+QQD0CtU9MRz/okQ82LxzkLSR/D4BLfVMzAoGBAKcocrbku0yuaZ2W9PYE\nA6ZNQkGA9/gvLG3zYymCGaUVKysmf+nLYMbul1jHYnSc2cok8m57u/I4cQDaER4b\nNlcr8olkcFavKi60sqRSENAyNfgzuqOVffBEaFuRd1uKKlfmTjQ9K/97TIo8EGoL\nj799AYNT32u6tOr4j68dPWTBAoGBAK1iJF4Yvi6fzH5FYzKlzDrRi8P7i9UvqGlx\nT1BFQ8oDMNSniZgf3Olymz0El6Ld4ka3iXchxWvx9rkCir7Zj8FTuAWQAZSDyXQn\nIzhSRQNjVEXvbeTQKLknHFeRCe1bnHxpW03NSkCbvDvb8HihUkAGSFnKvno+34nl\nqZWyfLy/AoGAaonDtHmKHmhNZF35M6LQ81VmWRLw62LeYIdOCE+9PBMQgM8EtJ1Z\n5+F32dIDaP8QCkgYV7AyE1sxQvjYISu5OK2EEyl4aLYm9K0P1eeK6HEXz1HYkZpt\nNdZ6+3q7IGzxoqkmJmoR0bjrAZXH3mTEIsWZJTIK7qM+TM02GL8CHSg=\n-----END RSA PRIVATE KEY-----\nEOF\n        destination  = \"secrets/ca.key\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name              = \"kibana\"\n        port              = \"http\"\n        tags              = [ \"kibana${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name            = \"Elastic Kibana Transport Check Live\"\n          port            = \"http\"\n          type            = \"tcp\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n        check {\n          name            = \"Elastic Kibana HTTP Check Live\"\n          type            = \"http\"\n          port            = \"http\"\n          protocol        = \"https\"\n          method          = \"GET\"\n          tls_skip_verify = true\n          path            = \"/\"\n          interval        = \"5s\"\n          timeout         = \"4s\"\n        }\n      }\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu           = 1000\n        memory        = 8192\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        network {\n          port \"http\" {\n            static    = 5601\n          }\n        }\n      }\n    }\n  }\n}\n",
            "json": null,
            "modify_index": "6243879",
            "name": "prod-elastic",
            "namespace": "default",
            "policy_override": null,
            "purge_on_destroy": null,
            "region": null,
            "task_groups": [
              {
                "count": 1,
                "meta": {},
                "name": "prod-group1-elastic-cluster",
                "task": [
                  {
                    "driver": "docker",
                    "meta": {},
                    "name": "prod-task1-elastic-cluster",
                    "volume_mounts": null
                  }
                ],
                "volumes": null
              },
              {
                "count": 1,
                "meta": {},
                "name": "prod-group1-elastic-kibana",
                "task": [
                  {
                    "driver": "docker",
                    "meta": {},
                    "name": "prod-task1-elastic-task",
                    "volume_mounts": null
                  }
                ],
                "volumes": null
              }
            ],
            "type": "service"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.elastic.data.template_file.nomad_job_elastic"
          ]
        }
      ]
    },
    {
      "module": "module.minio",
      "mode": "data",
      "type": "template_file",
      "name": "nomad_job_mc",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "ca0c95bbe91c4ac393d5cd5bef8efb90fb5f176f266ba233542fd4338b51c6cc",
            "rendered": "job \"prod-mc\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"batch\"\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-mc\" {\n    task \"prod-task1-create-buckets\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver        = \"docker\"\n\n      \n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image       = \"minio/mc:RELEASE.2020-12-10T01-26-17Z\"\n        entrypoint  = [\n          \"/bin/sh\",\n          \"-c\",\n          \"mc config host add LOCALMINIO http://storage.service.consul:9000 $MINIO_ACCESS_KEY $MINIO_SECRET_KEY \u0026\u0026 mc mb -p LOCALMINIO/logs.fd.io LOCALMINIO/docs.fd.io ; mc policy set public LOCALMINIO/logs.fd.io mc policy set public LOCALMINIO/docs.fd.io mc ilm add --expiry-days '180' LOCALMINIO/logs.fd.io mc admin user add LOCALMINIO storage Storage1234 mc admin policy set LOCALMINIO writeonly user=storage\"\n        ]\n        dns_servers  = [ \"${attr.unique.network.ip-address}\" ]\n        privileged   = false\n      }\n\n      # The env stanza configures a list of environment variables to populate\n      # the task's environment before starting.\n      env {\n        \n        MINIO_ACCESS_KEY = \"minio\"\n        MINIO_SECRET_KEY = \"minio123\"\n        \n        \n      }\n    }\n  }\n}\n",
            "template": "job \"${job_name}\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"${datacenters}\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"batch\"\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-mc\" {\n    task \"prod-task1-create-buckets\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver        = \"docker\"\n\n      %{ if use_vault_provider }\n      vault {\n        policies    = \"${vault_kv_policy_name}\"\n      }\n     %{ endif }\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image       = \"${image}\"\n        entrypoint  = [\n          \"/bin/sh\",\n          \"-c\",\n          \"${command}\"\n        ]\n        dns_servers  = [ \"$${attr.unique.network.ip-address}\" ]\n        privileged   = false\n      }\n\n      # The env stanza configures a list of environment variables to populate\n      # the task's environment before starting.\n      env {\n        %{ if use_vault_provider }\n        {{ with secret \"${vault_kv_path}\" }}\n        MINIO_ACCESS_KEY = \"{{ .Data.data.${vault_kv_field_access_key} }}\"\n        MINIO_SECRET_KEY = \"{{ .Data.data.${vault_kv_field_secret_key} }}\"\n        {{ end }}\n        %{ else }\n        MINIO_ACCESS_KEY = \"${access_key}\"\n        MINIO_SECRET_KEY = \"${secret_key}\"\n        %{ endif }\n        ${ envs }\n      }\n    }\n  }\n}\n",
            "vars": {
              "access_key": "minio",
              "command": "mc config host add LOCALMINIO http://storage.service.consul:9000 $MINIO_ACCESS_KEY $MINIO_SECRET_KEY \u0026\u0026 mc mb -p LOCALMINIO/logs.fd.io LOCALMINIO/docs.fd.io ; mc policy set public LOCALMINIO/logs.fd.io mc policy set public LOCALMINIO/docs.fd.io mc ilm add --expiry-days '180' LOCALMINIO/logs.fd.io mc admin user add LOCALMINIO storage Storage1234 mc admin policy set LOCALMINIO writeonly user=storage",
              "datacenters": "yul1",
              "envs": "",
              "image": "minio/mc:RELEASE.2020-12-10T01-26-17Z",
              "job_name": "prod-mc",
              "minio_port": "9000",
              "minio_service_name": "storage",
              "secret_key": "minio123",
              "service_name": "mc",
              "use_vault_provider": "false"
            }
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.minio",
      "mode": "data",
      "type": "template_file",
      "name": "nomad_job_minio",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "c38a0b7182d39c70ec07bdaa998f57a215d7193b264c1b1c1a299c84e7ca53de",
            "rendered": "job \"prod-minio\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 1\n\n    health_check      = \"checks\"\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 1\n\n    # Specifies if the job should auto-promote to the canary version when all\n    # canaries become healthy during a deployment. Defaults to false which means\n    # canaries must be manually updated with the nomad deployment promote\n    # command.\n    auto_promote      = true\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = true\n\n  }\n\n  # All groups in this job should be scheduled on different hosts.\n  constraint {\n    operator          = \"distinct_hosts\"\n    value             = \"true\"\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-minio\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count       = 4\n\n    # https://www.nomadproject.io/docs/job-specification/volume\n    \n    volume \"prod-volume1-minio\" {\n      type      = \"host\"\n      read_only = false\n      source    = \"prod-volume-data1-1\"\n    }\n    \n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-minio\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver             = \"docker\"\n\n    \n      volume_mount {\n        volume           = \"prod-volume1-minio\"\n        destination      = \"/data/\"\n        read_only        = false\n      }\n    \n\n    \n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image            = \"minio/minio:RELEASE.2020-12-03T05-49-24Z\"\n        dns_servers      = [ \"${attr.unique.network.ip-address}\" ]\n        network_mode     = \"host\"\n        command          = \"server\"\n        args             = [ \"http://10.32.8.1{4...7}:9000/data/\" ]\n        port_map {\n          http           = 9000\n        }\n        privileged       = false\n      }\n\n      # The env stanza configures a list of environment variables to populate\n      # the task's environment before starting.\n      env {\n\n        MINIO_ACCESS_KEY = \"minio\"\n        MINIO_SECRET_KEY = \"minio123\"\n\n        MINIO_BROWSER=\"off\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name       = \"storage\"\n        port       = \"http\"\n        tags       = [ \"storage${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name     = \"Min.io Server HTTP Check Live\"\n          type     = \"http\"\n          port     = \"http\"\n          protocol = \"http\"\n          method   = \"GET\"\n          path     = \"/minio/health/live\"\n          interval = \"10s\"\n          timeout  = \"2s\"\n        }\n        check {\n          name     = \"Min.io Server HTTP Check Ready\"\n          type     = \"http\"\n          port     = \"http\"\n          protocol = \"http\"\n          method   = \"GET\"\n          path     = \"/minio/health/ready\"\n          interval = \"10s\"\n          timeout  = \"2s\"\n        }\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu        = 40000\n        memory     = 40000\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        network {\n          port \"http\" {\n            static = 9000\n          }\n        }\n      }\n    }\n  }\n}\n",
            "template": "job \"${job_name}\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"${datacenters}\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 1\n\n    health_check      = \"checks\"\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n%{ if use_canary }\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 1\n\n    # Specifies if the job should auto-promote to the canary version when all\n    # canaries become healthy during a deployment. Defaults to false which means\n    # canaries must be manually updated with the nomad deployment promote\n    # command.\n    auto_promote      = true\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = true\n%{ endif }\n  }\n\n  # All groups in this job should be scheduled on different hosts.\n  constraint {\n    operator          = \"distinct_hosts\"\n    value             = \"true\"\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-minio\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count       = ${group_count}\n\n    # https://www.nomadproject.io/docs/job-specification/volume\n    %{ if use_host_volume }\n    volume \"prod-volume1-minio\" {\n      type      = \"host\"\n      read_only = false\n      source    = \"${host_volume}\"\n    }\n    %{ endif }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-minio\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver             = \"docker\"\n\n    %{ if use_host_volume }\n      volume_mount {\n        volume           = \"prod-volume1-minio\"\n        destination      = \"${data_dir}\"\n        read_only        = false\n      }\n    %{ endif }\n\n    %{ if use_vault_provider }\n      vault {\n        policies         = \"${vault_kv_policy_name}\"\n      }\n    %{ endif }\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image            = \"${image}\"\n        dns_servers      = [ \"$${attr.unique.network.ip-address}\" ]\n        network_mode     = \"host\"\n        command          = \"server\"\n        args             = [ \"${host}:${port}${data_dir}\" ]\n        port_map {\n          http           = ${port}\n        }\n        privileged       = false\n      }\n\n      # The env stanza configures a list of environment variables to populate\n      # the task's environment before starting.\n      env {\n%{ if use_vault_provider }\n{{ with secret \"${vault_kv_path}\" }}\n        MINIO_ACCESS_KEY = \"{{ .Data.data.${vault_kv_field_access_key} }}\"\n        MINIO_SECRET_KEY = \"{{ .Data.data.${vault_kv_field_secret_key} }}\"\n{{ end }}\n%{ else }\n        MINIO_ACCESS_KEY = \"${access_key}\"\n        MINIO_SECRET_KEY = \"${secret_key}\"\n%{ endif }\n        ${ envs }\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name       = \"${service_name}\"\n        port       = \"http\"\n        tags       = [ \"storage$${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name     = \"Min.io Server HTTP Check Live\"\n          type     = \"http\"\n          port     = \"http\"\n          protocol = \"http\"\n          method   = \"GET\"\n          path     = \"/minio/health/live\"\n          interval = \"10s\"\n          timeout  = \"2s\"\n        }\n        check {\n          name     = \"Min.io Server HTTP Check Ready\"\n          type     = \"http\"\n          port     = \"http\"\n          protocol = \"http\"\n          method   = \"GET\"\n          path     = \"/minio/health/ready\"\n          interval = \"10s\"\n          timeout  = \"2s\"\n        }\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu        = ${cpu}\n        memory     = ${memory}\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        network {\n          port \"http\" {\n            static = ${port}\n          }\n        }\n      }\n    }\n  }\n}\n",
            "vars": {
              "access_key": "minio",
              "cpu": "40000",
              "cpu_proxy": "200",
              "data_dir": "/data/",
              "datacenters": "yul1",
              "envs": "MINIO_BROWSER=\"off\"",
              "group_count": "4",
              "host": "http://10.32.8.1{4...7}",
              "host_volume": "prod-volume-data1-1",
              "image": "minio/minio:RELEASE.2020-12-03T05-49-24Z",
              "job_name": "prod-minio",
              "memory": "40000",
              "memory_proxy": "128",
              "port": "9000",
              "secret_key": "minio123",
              "service_name": "storage",
              "upstreams": "[]",
              "use_canary": "true",
              "use_host_volume": "true",
              "use_vault_provider": "false"
            }
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.minio",
      "mode": "managed",
      "type": "nomad_job",
      "name": "nomad_job_mc",
      "provider": "provider[\"registry.terraform.io/hashicorp/nomad\"].yul1",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "allocation_ids": [
              "dbb2e32b-3bbd-b700-4e12-d0a9fa7d4ad0"
            ],
            "datacenters": [
              "yul1"
            ],
            "deployment_id": "",
            "deployment_status": "",
            "deregister_on_destroy": true,
            "deregister_on_id_change": true,
            "detach": false,
            "id": "prod-mc",
            "jobspec": "job \"prod-mc\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"batch\"\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-mc\" {\n    task \"prod-task1-create-buckets\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver        = \"docker\"\n\n      \n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image       = \"minio/mc:RELEASE.2020-12-10T01-26-17Z\"\n        entrypoint  = [\n          \"/bin/sh\",\n          \"-c\",\n          \"mc config host add LOCALMINIO http://storage.service.consul:9000 $MINIO_ACCESS_KEY $MINIO_SECRET_KEY \u0026\u0026 mc mb -p LOCALMINIO/logs.fd.io LOCALMINIO/docs.fd.io ; mc policy set public LOCALMINIO/logs.fd.io mc policy set public LOCALMINIO/docs.fd.io mc ilm add --expiry-days '180' LOCALMINIO/logs.fd.io mc admin user add LOCALMINIO storage Storage1234 mc admin policy set LOCALMINIO writeonly user=storage\"\n        ]\n        dns_servers  = [ \"${attr.unique.network.ip-address}\" ]\n        privileged   = false\n      }\n\n      # The env stanza configures a list of environment variables to populate\n      # the task's environment before starting.\n      env {\n        \n        MINIO_ACCESS_KEY = \"minio\"\n        MINIO_SECRET_KEY = \"minio123\"\n        \n        \n      }\n    }\n  }\n}\n",
            "json": null,
            "modify_index": "6243930",
            "name": "prod-mc",
            "namespace": "default",
            "policy_override": null,
            "purge_on_destroy": null,
            "region": "global",
            "task_groups": [
              {
                "count": 1,
                "meta": {},
                "name": "prod-group1-mc",
                "task": [
                  {
                    "driver": "docker",
                    "meta": {},
                    "name": "prod-task1-create-buckets",
                    "volume_mounts": null
                  }
                ],
                "volumes": null
              }
            ],
            "type": "batch"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.minio.data.template_file.nomad_job_mc",
            "module.minio.data.template_file.nomad_job_minio",
            "module.minio.nomad_job.nomad_job_minio"
          ]
        }
      ]
    },
    {
      "module": "module.minio",
      "mode": "managed",
      "type": "nomad_job",
      "name": "nomad_job_minio",
      "provider": "provider[\"registry.terraform.io/hashicorp/nomad\"].yul1",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "allocation_ids": [
              "322e6ea5-acc7-2f37-3022-cd0a31c2f3db",
              "619833b9-dd70-46ae-f576-75c3cd058a13",
              "0510561a-c828-dd23-5910-b92d9f2c41ef",
              "504924c8-387b-abe2-c4d5-d6a7424a4689"
            ],
            "datacenters": [
              "yul1"
            ],
            "deployment_id": "cd6d79cf-6e6f-289d-a0bb-6b2bb8da769a",
            "deployment_status": "successful",
            "deregister_on_destroy": true,
            "deregister_on_id_change": true,
            "detach": false,
            "id": "prod-minio",
            "jobspec": "job \"prod-minio\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type        = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 1\n\n    health_check      = \"checks\"\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 1\n\n    # Specifies if the job should auto-promote to the canary version when all\n    # canaries become healthy during a deployment. Defaults to false which means\n    # canaries must be manually updated with the nomad deployment promote\n    # command.\n    auto_promote      = true\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = true\n\n  }\n\n  # All groups in this job should be scheduled on different hosts.\n  constraint {\n    operator          = \"distinct_hosts\"\n    value             = \"true\"\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-minio\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count       = 4\n\n    # https://www.nomadproject.io/docs/job-specification/volume\n    \n    volume \"prod-volume1-minio\" {\n      type      = \"host\"\n      read_only = false\n      source    = \"prod-volume-data1-1\"\n    }\n    \n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-minio\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver             = \"docker\"\n\n    \n      volume_mount {\n        volume           = \"prod-volume1-minio\"\n        destination      = \"/data/\"\n        read_only        = false\n      }\n    \n\n    \n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image            = \"minio/minio:RELEASE.2020-12-03T05-49-24Z\"\n        dns_servers      = [ \"${attr.unique.network.ip-address}\" ]\n        network_mode     = \"host\"\n        command          = \"server\"\n        args             = [ \"http://10.32.8.1{4...7}:9000/data/\" ]\n        port_map {\n          http           = 9000\n        }\n        privileged       = false\n      }\n\n      # The env stanza configures a list of environment variables to populate\n      # the task's environment before starting.\n      env {\n\n        MINIO_ACCESS_KEY = \"minio\"\n        MINIO_SECRET_KEY = \"minio123\"\n\n        MINIO_BROWSER=\"off\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name       = \"storage\"\n        port       = \"http\"\n        tags       = [ \"storage${NOMAD_ALLOC_INDEX}\" ]\n        check {\n          name     = \"Min.io Server HTTP Check Live\"\n          type     = \"http\"\n          port     = \"http\"\n          protocol = \"http\"\n          method   = \"GET\"\n          path     = \"/minio/health/live\"\n          interval = \"10s\"\n          timeout  = \"2s\"\n        }\n        check {\n          name     = \"Min.io Server HTTP Check Ready\"\n          type     = \"http\"\n          port     = \"http\"\n          protocol = \"http\"\n          method   = \"GET\"\n          path     = \"/minio/health/ready\"\n          interval = \"10s\"\n          timeout  = \"2s\"\n        }\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        # The network stanza specifies the networking requirements for the task\n        # group, including the network mode and port allocations. When scheduling\n        # jobs in Nomad they are provisioned across your fleet of machines along\n        # with other jobs and services. Because you don't know in advance what host\n        # your job will be provisioned on, Nomad will provide your tasks with\n        # network configuration when they start up.\n        #\n        # For more information and examples on the \"template\" stanza, please see\n        # the online documentation at:\n        #\n        #     https://www.nomadproject.io/docs/job-specification/network.html\n        #\n        cpu      = 40000\n        memory   = 40000\n        network {\n          port \"http\" {\n            static = 9000\n          }\n        }\n      }\n    }\n  }\n}\n",
            "json": null,
            "modify_index": "5933603",
            "name": "prod-minio",
            "namespace": "default",
            "policy_override": null,
            "purge_on_destroy": null,
            "region": "global",
            "task_groups": [
              {
                "count": 4,
                "meta": {},
                "name": "prod-group1-minio",
                "task": [
                  {
                    "driver": "docker",
                    "meta": {},
                    "name": "prod-task1-minio",
                    "volume_mounts": [
                      {
                        "destination": "/data/",
                        "read_only": false,
                        "volume": "prod-volume1-minio"
                      }
                    ]
                  }
                ],
                "volumes": [
                  {
                    "name": "prod-volume1-minio",
                    "read_only": false,
                    "source": "prod-volume-data1-1",
                    "type": "host"
                  }
                ]
              }
            ],
            "type": "service"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.minio.data.template_file.nomad_job_minio"
          ]
        }
      ]
    },
    {
      "module": "module.nginx",
      "mode": "data",
      "type": "template_file",
      "name": "nomad_job_nginx",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "2816603491214d9abe10981b9f9cc6a4cb806acbc6981b952653847fec86e9ff",
            "rendered": "job \"prod-nginx\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 0\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = false\n\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 0\n  }\n\n  # The reschedule stanza specifies the group's rescheduling strategy. If\n  # specified at the job level, the configuration will apply to all groups\n  # within the job. If the reschedule stanza is present on both the job and the\n  # group, they are merged with the group stanza taking the highest precedence\n  # and then the job.\n  reschedule {\n    delay             = \"30s\"\n    delay_function    = \"constant\"\n    unlimited         = true\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-nginx\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count = 1\n\n    # The restart stanza configures a tasks's behavior on task failure. Restarts\n    # happen on the client that is running the task.\n    restart {\n      interval  = \"10m\"\n      attempts  = 2\n      delay     = \"15s\"\n      mode      = \"fail\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-nginx\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"nginx:stable\"\n        dns_servers  = [ \"${attr.unique.network.ip-address}\" ]\n        port_map {\n          https      = 443\n        }\n        privileged   = false\n        volumes      = [\n          \"/etc/consul.d/ssl/consul.pem:/etc/ssl/certs/nginx-cert.pem\",\n          \"/etc/consul.d/ssl/consul-key.pem:/etc/ssl/private/nginx-key.pem\",\n          \"custom/upstream.conf:/etc/nginx/conf.d/upstream.conf\",\n          \"custom/logs.conf:/etc/nginx/conf.d/logs.conf\",\n          \"custom/docs.conf:/etc/nginx/conf.d/docs.conf\"\n        ]\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data = \u003c\u003cEOH\n          upstream storage {\n            server storage0.storage.service.consul:9000;\n            server storage1.storage.service.consul:9000;\n            server storage2.storage.service.consul:9000;\n            server storage3.storage.service.consul:9000;\n          }\n        EOH\n        destination = \"custom/upstream.conf\"\n      }\n      template {\n        data = \u003c\u003cEOH\n          server {\n            listen 443 ssl default_server;\n            server_name logs.nginx.service.consul;\n            keepalive_timeout 70;\n            ssl_session_cache shared:SSL:10m;\n            ssl_session_timeout 10m;\n            ssl_protocols TLSv1.2;\n            ssl_prefer_server_ciphers on;\n            ssl_ciphers \"ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384\";\n            ssl_certificate /etc/ssl/certs/nginx-cert.pem;\n            ssl_certificate_key /etc/ssl/private/nginx-key.pem;\n            location / {\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/;\n              server_name_in_redirect off;\n            }\n            location ~ (.*html.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type text/html;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n            location ~ (.*txt.gz|.*log.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type text/plain;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n            location ~ (.*xml.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type application/xml;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n        }\n        EOH\n        destination = \"custom/logs.conf\"\n      }\n      template {\n        data = \u003c\u003cEOH\n          server {\n            listen 443 ssl;\n            server_name docs.nginx.service.consul;\n            keepalive_timeout 70;\n            ssl_session_cache shared:SSL:10m;\n            ssl_session_timeout 10m;\n            ssl_protocols TLSv1.2;\n            ssl_prefer_server_ciphers on;\n            ssl_ciphers \"ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384\";\n            ssl_certificate /etc/ssl/certs/nginx-cert.pem;\n            ssl_certificate_key /etc/ssl/private/nginx-key.pem;\n            location / {\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/docs.fd.io/;\n              server_name_in_redirect off;\n            }\n          }\n        EOH\n        destination = \"custom/docs.conf\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name       = \"nginx\"\n        port       = \"https\"\n        tags       = [ \"docs\", \"logs\" ]\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu        = 1000\n        memory     = 1024\n        network {\n          mode     = \"bridge\"\n          port \"https\" {\n            static = 443\n          }\n        }\n      }\n    }\n  }\n}",
            "template": "job \"${job_name}\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"${datacenters}\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 0\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = false\n\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 0\n  }\n\n  # The reschedule stanza specifies the group's rescheduling strategy. If\n  # specified at the job level, the configuration will apply to all groups\n  # within the job. If the reschedule stanza is present on both the job and the\n  # group, they are merged with the group stanza taking the highest precedence\n  # and then the job.\n  reschedule {\n    delay             = \"30s\"\n    delay_function    = \"constant\"\n    unlimited         = true\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-nginx\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count = 1\n\n    # The restart stanza configures a tasks's behavior on task failure. Restarts\n    # happen on the client that is running the task.\n    restart {\n      interval  = \"10m\"\n      attempts  = 2\n      delay     = \"15s\"\n      mode      = \"fail\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-nginx\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"nginx:stable\"\n        dns_servers  = [ \"$${attr.unique.network.ip-address}\" ]\n        port_map {\n          https      = 443\n        }\n        privileged   = false\n        volumes      = [\n          \"/etc/consul.d/ssl/consul.pem:/etc/ssl/certs/nginx-cert.pem\",\n          \"/etc/consul.d/ssl/consul-key.pem:/etc/ssl/private/nginx-key.pem\",\n          \"custom/upstream.conf:/etc/nginx/conf.d/upstream.conf\",\n          \"custom/logs.conf:/etc/nginx/conf.d/logs.conf\",\n          \"custom/docs.conf:/etc/nginx/conf.d/docs.conf\"\n        ]\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data = \u003c\u003cEOH\n          upstream storage {\n            server storage0.storage.service.consul:9000;\n            server storage1.storage.service.consul:9000;\n            server storage2.storage.service.consul:9000;\n            server storage3.storage.service.consul:9000;\n          }\n        EOH\n        destination = \"custom/upstream.conf\"\n      }\n      template {\n        data = \u003c\u003cEOH\n          server {\n            listen 443 ssl default_server;\n            server_name logs.nginx.service.consul;\n            keepalive_timeout 70;\n            ssl_session_cache shared:SSL:10m;\n            ssl_session_timeout 10m;\n            ssl_protocols TLSv1.2;\n            ssl_prefer_server_ciphers on;\n            ssl_ciphers \"ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384\";\n            ssl_certificate /etc/ssl/certs/nginx-cert.pem;\n            ssl_certificate_key /etc/ssl/private/nginx-key.pem;\n            location / {\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/;\n              server_name_in_redirect off;\n            }\n            location ~ (.*html.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type text/html;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n            location ~ (.*txt.gz|.*log.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type text/plain;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n            location ~ (.*xml.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type application/xml;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n        }\n        EOH\n        destination = \"custom/logs.conf\"\n      }\n      template {\n        data = \u003c\u003cEOH\n          server {\n            listen 443 ssl;\n            server_name docs.nginx.service.consul;\n            keepalive_timeout 70;\n            ssl_session_cache shared:SSL:10m;\n            ssl_session_timeout 10m;\n            ssl_protocols TLSv1.2;\n            ssl_prefer_server_ciphers on;\n            ssl_ciphers \"ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384\";\n            ssl_certificate /etc/ssl/certs/nginx-cert.pem;\n            ssl_certificate_key /etc/ssl/private/nginx-key.pem;\n            location / {\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/docs.fd.io/;\n              server_name_in_redirect off;\n            }\n          }\n        EOH\n        destination = \"custom/docs.conf\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name       = \"nginx\"\n        port       = \"https\"\n        tags       = [ \"docs\", \"logs\" ]\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu        = 1000\n        memory     = 1024\n        network {\n          mode     = \"bridge\"\n          port \"https\" {\n            static = 443\n          }\n        }\n      }\n    }\n  }\n}",
            "vars": {
              "datacenters": "yul1",
              "job_name": "prod-nginx"
            }
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.nginx",
      "mode": "managed",
      "type": "nomad_job",
      "name": "nomad_job_nginx",
      "provider": "provider[\"registry.terraform.io/hashicorp/nomad\"].yul1",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "allocation_ids": [
              "de9af589-8f0b-6634-7199-c661f8f17df0"
            ],
            "datacenters": [
              "yul1"
            ],
            "deployment_id": "",
            "deployment_status": "",
            "deregister_on_destroy": true,
            "deregister_on_id_change": true,
            "detach": false,
            "id": "prod-nginx",
            "jobspec": "job \"prod-nginx\" {\n  # The \"region\" parameter specifies the region in which to execute the job.\n  # If omitted, this inherits the default region name of \"global\".\n  # region = \"global\"\n  #\n  # The \"datacenters\" parameter specifies the list of datacenters which should\n  # be considered when placing this task. This must be provided.\n  datacenters = \"yul1\"\n\n  # The \"type\" parameter controls the type of job, which impacts the scheduler's\n  # decision on placement. This configuration is optional and defaults to\n  # \"service\". For a full list of job types and their differences, please see\n  # the online documentation.\n  #\n  # For more information, please see the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/jobspec/schedulers.html\n  #\n  type = \"service\"\n\n  update {\n    # The \"max_parallel\" parameter specifies the maximum number of updates to\n    # perform in parallel. In this case, this specifies to update a single task\n    # at a time.\n    max_parallel      = 0\n\n    # The \"min_healthy_time\" parameter specifies the minimum time the allocation\n    # must be in the healthy state before it is marked as healthy and unblocks\n    # further allocations from being updated.\n    min_healthy_time  = \"10s\"\n\n    # The \"healthy_deadline\" parameter specifies the deadline in which the\n    # allocation must be marked as healthy after which the allocation is\n    # automatically transitioned to unhealthy. Transitioning to unhealthy will\n    # fail the deployment and potentially roll back the job if \"auto_revert\" is\n    # set to true.\n    healthy_deadline  = \"3m\"\n\n    # The \"progress_deadline\" parameter specifies the deadline in which an\n    # allocation must be marked as healthy. The deadline begins when the first\n    # allocation for the deployment is created and is reset whenever an allocation\n    # as part of the deployment transitions to a healthy state. If no allocation\n    # transitions to the healthy state before the progress deadline, the\n    # deployment is marked as failed.\n    progress_deadline = \"10m\"\n\n    # The \"auto_revert\" parameter specifies if the job should auto-revert to the\n    # last stable job on deployment failure. A job is marked as stable if all the\n    # allocations as part of its deployment were marked healthy.\n    auto_revert       = false\n\n    # The \"canary\" parameter specifies that changes to the job that would result\n    # in destructive updates should create the specified number of canaries\n    # without stopping any previous allocations. Once the operator determines the\n    # canaries are healthy, they can be promoted which unblocks a rolling update\n    # of the remaining allocations at a rate of \"max_parallel\".\n    #\n    # Further, setting \"canary\" equal to the count of the task group allows\n    # blue/green deployments. When the job is updated, a full set of the new\n    # version is deployed and upon promotion the old version is stopped.\n    canary            = 0\n  }\n\n  # The reschedule stanza specifies the group's rescheduling strategy. If\n  # specified at the job level, the configuration will apply to all groups\n  # within the job. If the reschedule stanza is present on both the job and the\n  # group, they are merged with the group stanza taking the highest precedence\n  # and then the job.\n  reschedule {\n    delay             = \"30s\"\n    delay_function    = \"constant\"\n    unlimited         = true\n  }\n\n  # The \"group\" stanza defines a series of tasks that should be co-located on\n  # the same Nomad client. Any task within a group will be placed on the same\n  # client.\n  #\n  # For more information and examples on the \"group\" stanza, please see\n  # the online documentation at:\n  #\n  #     https://www.nomadproject.io/docs/job-specification/group.html\n  #\n  group \"prod-group1-nginx\" {\n    # The \"count\" parameter specifies the number of the task groups that should\n    # be running under this group. This value must be non-negative and defaults\n    # to 1.\n    count = 1\n\n    # The restart stanza configures a tasks's behavior on task failure. Restarts\n    # happen on the client that is running the task.\n    restart {\n      interval  = \"10m\"\n      attempts  = 2\n      delay     = \"15s\"\n      mode      = \"fail\"\n    }\n\n    # The \"task\" stanza creates an individual unit of work, such as a Docker\n    # container, web application, or batch processing.\n    #\n    # For more information and examples on the \"task\" stanza, please see\n    # the online documentation at:\n    #\n    #     https://www.nomadproject.io/docs/job-specification/task.html\n    #\n    task \"prod-task1-nginx\" {\n      # The \"driver\" parameter specifies the task driver that should be used to\n      # run the task.\n      driver = \"docker\"\n\n      # The \"config\" stanza specifies the driver configuration, which is passed\n      # directly to the driver to start the task. The details of configurations\n      # are specific to each driver, so please see specific driver\n      # documentation for more information.\n      config {\n        image        = \"nginx:stable\"\n        dns_servers  = [ \"${attr.unique.network.ip-address}\" ]\n        port_map {\n          https      = 443\n        }\n        privileged   = false\n        volumes      = [\n          \"/etc/consul.d/ssl/consul.pem:/etc/ssl/certs/nginx-cert.pem\",\n          \"/etc/consul.d/ssl/consul-key.pem:/etc/ssl/private/nginx-key.pem\",\n          \"custom/upstream.conf:/etc/nginx/conf.d/upstream.conf\",\n          \"custom/logs.conf:/etc/nginx/conf.d/logs.conf\",\n          \"custom/docs.conf:/etc/nginx/conf.d/docs.conf\"\n        ]\n      }\n\n      # The \"template\" stanza instructs Nomad to manage a template, such as\n      # a configuration file or script. This template can optionally pull data\n      # from Consul or Vault to populate runtime configuration data.\n      #\n      # For more information and examples on the \"template\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/template.html\n      #\n      template {\n        data = \u003c\u003cEOH\n          upstream storage {\n            server storage0.storage.service.consul:9000;\n            server storage1.storage.service.consul:9000;\n            server storage2.storage.service.consul:9000;\n            server storage3.storage.service.consul:9000;\n          }\n        EOH\n        destination = \"custom/upstream.conf\"\n      }\n      template {\n        data = \u003c\u003cEOH\n          server {\n            listen 443 ssl default_server;\n            server_name logs.nginx.service.consul;\n            keepalive_timeout 70;\n            ssl_session_cache shared:SSL:10m;\n            ssl_session_timeout 10m;\n            ssl_protocols TLSv1.2;\n            ssl_prefer_server_ciphers on;\n            ssl_ciphers \"ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384\";\n            ssl_certificate /etc/ssl/certs/nginx-cert.pem;\n            ssl_certificate_key /etc/ssl/private/nginx-key.pem;\n            location / {\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/;\n              server_name_in_redirect off;\n            }\n            location ~ (.*html.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type text/html;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n            location ~ (.*txt.gz|.*log.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type text/plain;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n            location ~ (.*xml.gz)$ {\n              add_header Content-Encoding gzip;\n              add_header Content-Type application/xml;\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/logs.fd.io/$1;\n              server_name_in_redirect off;\n            }\n        }\n        EOH\n        destination = \"custom/logs.conf\"\n      }\n      template {\n        data = \u003c\u003cEOH\n          server {\n            listen 443 ssl;\n            server_name docs.nginx.service.consul;\n            keepalive_timeout 70;\n            ssl_session_cache shared:SSL:10m;\n            ssl_session_timeout 10m;\n            ssl_protocols TLSv1.2;\n            ssl_prefer_server_ciphers on;\n            ssl_ciphers \"ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384\";\n            ssl_certificate /etc/ssl/certs/nginx-cert.pem;\n            ssl_certificate_key /etc/ssl/private/nginx-key.pem;\n            location / {\n              chunked_transfer_encoding off;\n              proxy_connect_timeout 300;\n              proxy_http_version 1.1;\n              proxy_set_header Host $host:$server_port;\n              proxy_set_header Connection \"\";\n              proxy_pass http://storage/docs.fd.io/;\n              server_name_in_redirect off;\n            }\n          }\n        EOH\n        destination = \"custom/docs.conf\"\n      }\n\n      # The service stanza instructs Nomad to register a service with Consul.\n      #\n      # For more information and examples on the \"task\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/service.html\n      #\n      service {\n        name       = \"nginx\"\n        port       = \"https\"\n        tags       = [ \"docs\", \"logs\" ]\n      }\n\n      # The \"resources\" stanza describes the requirements a task needs to\n      # execute. Resource requirements include memory, network, cpu, and more.\n      # This ensures the task will execute on a machine that contains enough\n      # resource capacity.\n      #\n      # For more information and examples on the \"resources\" stanza, please see\n      # the online documentation at:\n      #\n      #     https://www.nomadproject.io/docs/job-specification/resources.html\n      #\n      resources {\n        cpu        = 1000\n        memory     = 1024\n        network {\n          mode     = \"bridge\"\n          port \"https\" {\n            static = 443\n          }\n        }\n      }\n    }\n  }\n}",
            "json": null,
            "modify_index": "5922474",
            "name": "prod-nginx",
            "namespace": "default",
            "policy_override": null,
            "purge_on_destroy": null,
            "region": "global",
            "task_groups": [
              {
                "count": 1,
                "meta": {},
                "name": "prod-group1-nginx",
                "task": [
                  {
                    "driver": "docker",
                    "meta": {},
                    "name": "prod-task1-nginx",
                    "volume_mounts": []
                  }
                ],
                "volumes": []
              }
            ],
            "type": "service"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.nginx.data.template_file.nomad_job_nginx"
          ]
        }
      ]
    }
  ]
}
